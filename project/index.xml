<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Li Wang</title>
    <link>https://Alibabade.github.io/project/</link>
      <atom:link href="https://Alibabade.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 17 May 2023 14:07:32 +0800</lastBuildDate>
    <image>
      <url>https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://Alibabade.github.io/project/</link>
    </image>
    
    <item>
      <title>Commnad_line_Program_Youtube_Music_Channel_Scraper</title>
      <link>https://Alibabade.github.io/project/commnad_line_program_youtube_music_channel_scraper/</link>
      <pubDate>Wed, 17 May 2023 14:07:32 +0800</pubDate>
      <guid>https://Alibabade.github.io/project/commnad_line_program_youtube_music_channel_scraper/</guid>
      <description>&lt;p&gt;This program scrapes the given Youtube music channels by fetching infomation of videos, and saving information and videos into local drives. The program is currently only avaiable for downloading artwork images (if applicable), thumbnails, audios (mp3 format) and videos.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Automatically Resume Scraping Process&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Update Channels Scraped&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Download Images From Various Image Websites (e.g., Artstation, Pixix, Deviantart, Pexels, Unsplash, Flickr)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Automatically Rename Video Titles (Optional)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;automatically-resume-scraping-and-downloading-processes&#34;&gt;Automatically Resume Scraping and Downloading Processes.&lt;/h3&gt;
&lt;p&gt;Normally, scraping a big music channel (e.g., MrSuicideSheep) could take hours (or even longer) as the channel contains over thousands of videos.
Thus the scraping and downloading process could be easily interrupted by anything, for example, power failure or poor internet connection, or coffee leak (even worse :) ).&lt;/p&gt;
&lt;p&gt;Here the information scraping and video downloading are seperately treated.&lt;/p&gt;
&lt;p&gt;To automatically resume the information scraping process, this program will deal with this problem by checking the number of videos (after stop_upload_time if applicable) of two files: FILE1. &amp;ldquo;channel_videos_id_list.txt&amp;rdquo; and FILE2.&amp;ldquo;channel_videos_info_list.json&amp;rdquo;
FILE1 will store all video_ids from one channel, which only takes minutes to scrape and save.
FILE2 will store all video_info (including URL, title, upload_date, chapters, thumbnail, artwork_url, etc), which will be used for downloading. Saving this file could take around mins or hours depends on how many videos that channel has.&lt;/p&gt;
&lt;p&gt;There are three senarios in resume scheme:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. both files are created, then check the number of recored videos,
   if   	the numbers are equal, then no more infomartion fetch process.
   else 	resume the information fetch process from the last video id in FILE2.
        	       
2. FILE2 not exists but FILE1 exists, then resume the infomation fetch process and create FILE2.
        	
3. both files are not created, then start the infomation fetch process straightforward.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To automatically resume the video downloading process, this program will deal with this problem by checking the number of files under one video folder.
For videos without chapters, the file number under a video folder should be the number of items downloaded. For example, usually, the files of a video folder should contains an artwork image (if recorded under the same video title in &amp;ldquo;channel_videos_info_list.json&amp;rdquo;), a thumbnail and a video file, thus if file number is lower than 3, then the downloading process will resume from downloading the files not exist. For videos with chapters, the file number should be 3 + number of chapters. If the number of files is lower than that, the downloading process will resume from downloading the files not exist.&lt;/p&gt;
&lt;h3 id=&#34;update-channels-already-scraped&#34;&gt;Update Channels Already Scraped&lt;/h3&gt;
&lt;p&gt;Setting up flag &amp;ldquo;&amp;ndash;update True&amp;rdquo; when running program will automatically update these two files: &amp;ldquo;channel_videos_id_list.txt&amp;rdquo; and &amp;ldquo;channel_videos_info_list.json&amp;rdquo;, and download new videos.&lt;/p&gt;
&lt;h3 id=&#34;image-downloader-for-various-image-websites&#34;&gt;Image Downloader for Various Image Websites&lt;/h3&gt;
&lt;p&gt;Several downloaders are implemented in &amp;ldquo;lib/Youtube_Scraper_API.py&amp;rdquo; file. Currently, this program is able to download images from Websites: Artstation, Pixix, Deviantart, Pexels, Unsplash, Flickr.&lt;/p&gt;
&lt;h3 id=&#34;automatically-rename-video-titles-optional&#34;&gt;Automatically Rename Video Titles (Optional)&lt;/h3&gt;
&lt;p&gt;During the development, an interesting problem is observed that the video titles could contain some weird words or symbols which are not able to be file titles saved in disk (especially for Linux OS). Further more, the titles are abitrary sometimes, which are difficult to extract useful information, for example,  artist names, features, music or song titles and types of video contents (e.g., mv or lyric video or visualizer, etc.). Thus, here presents a rename scheme that will automatically rename the video titles into a unified format:
&amp;ldquo;artist_usernames - song_name (feat./with/prod. by/cover by artist_usernames) + (artist_usernames remix/mix/flip/cover) + [ncs release]&amp;rdquo;
where the words in bracket &amp;ldquo;[]&amp;rdquo; are essential while the words in bracket &amp;ldquo;{}&amp;rdquo; are optional.&lt;/p&gt;
&lt;h2 id=&#34;setups&#34;&gt;Setups&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Please install youtube-dl for video (or audio) download.&lt;/li&gt;
&lt;li&gt;Please install all the other necessary libraries in &amp;ldquo;requirements.txt&amp;rdquo; before starting the program.&lt;/li&gt;
&lt;li&gt;(Optional) An adblock plugin could be installed via Firefox browser as this will disable the ads when scraping the video webpages (this could save lots of times).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;You shall find the code in &lt;a href=&#34;https://github.com/Alibabade/Commnad-line-Youtube-Music-Channel-Scraper/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;h3 id=&#34;example-1-scrape-youtube-channels&#34;&gt;Example 1: Scrape youtube channels&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;python youtube_scraper_by_given_artist_channels.py \
   --music_channel_list_filepath [path/to/channel_list.txt]\
   --saved_path [path/to/saved_path]\
   --download_file_format [file_format]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running this example, a folder will be created if saved_path not exists, then &amp;ldquo;channel_videos_id_list.txt&amp;rdquo; and &amp;ldquo;channel_videos_info_list.json&amp;rdquo; will be created under the same youtube channel folder, next a folder (named as the video_id) will be created for each downloaded video, in which all related images (e.g., artwork images and thumbnails) and videos (or audios) will be downloaded from Youtube.&lt;/p&gt;
&lt;h3 id=&#34;example-2-update-youtube-channels-which-are-already-scraped&#34;&gt;Example 2: Update youtube channels which are already scraped&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;python youtube_scraper_by_given_artist_channels.py \
   --music_channel_list_filepath [path/to/channel_list.txt]\
   --saved_path [path/to/saved_path]\
   --download_file_format [file_format]
   --update True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running this example, channels that already scraped and downloaded will be updated by re-scraping the information of channels, and downloading new videos.&lt;/p&gt;
&lt;h3 id=&#34;optional-flags&#34;&gt;Optional Flags&lt;/h3&gt;
&lt;p&gt;Set &amp;ldquo;&amp;ndash;rename_title&amp;rdquo; as True, then all video titles will be automatically reformed into the unified format described above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mesh_preprocessor_in_blender_with_python</title>
      <link>https://Alibabade.github.io/project/mesh_preprocessor_in_blender_with_python/</link>
      <pubDate>Tue, 16 May 2023 14:46:41 +0800</pubDate>
      <guid>https://Alibabade.github.io/project/mesh_preprocessor_in_blender_with_python/</guid>
      <description>&lt;p&gt;This project presents a mesh preprocessor tool which works with python and blender. It&amp;rsquo;s capable of processing mesh files in batch manner.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The main reason to create such a mesh preprocessor is that preparing raw mesh files (especially .obj file format) for datasets usually is time-consuming, and it kind of needs tons of manual work. To free hands, a mesh processor tool is presented to do the dirty work for people.&lt;/p&gt;
&lt;h2 id=&#34;functions&#34;&gt;Functions&lt;/h2&gt;
&lt;p&gt;A few functions are added in this tool, for example, subdivide mesh into (or closest to) desired vertex number, triangulate the mesh if needed, and extract vertex indices of arbitrary sub-surfaces from the mesh.&lt;/p&gt;
&lt;h2 id=&#34;time-consumption&#34;&gt;Time consumption&lt;/h2&gt;
&lt;p&gt;For a computer with i7-6700k cpu and 16GB memory, it will take about ~3 hours when processing over 100 meshes (low poly) plus subdivding them into 50K vertices plus extracting 1K sub-surfaces for each mesh. It will take about ~9 hours when processing over 100 meshes (low poly) plus subdivding them into 150K vertices plus extracting 1K sub-surfaces for each mesh. The time consumption varies on the cpu and memory.&lt;/p&gt;
&lt;h2 id=&#34;inputs-and-outputs&#34;&gt;Inputs and Outputs&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;inputs are mesh files ending with &amp;lsquo;.obj&amp;rsquo;, which are original mesh files you find or download online. In this repos, a folder named &amp;lsquo;data&amp;rsquo; contains two examples.&lt;/li&gt;
&lt;li&gt;outputs are mesh files ending with &amp;lsquo;_tri.obj&amp;rsquo; which are subdivided and triangulated mesh files, and data files ending with &amp;lsquo;.npy&amp;rsquo; under corresponding &amp;ldquo;vertex_indinces_of_arbitrary_surfaces&amp;rdquo; which contains a list of numpy arrays of vertex indinces of sub-surfaces from each mesh.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;You can find the code in &lt;a href=&#34;https://github.com/Alibabade/Mesh-preprocessor-in-blender-with-python&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;open blender software, if not installed, then download it from &lt;a href=&#34;https://www.blender.org/download/&#34;&gt;here&lt;/a&gt;. NO installation needed just unzip it.&lt;/li&gt;
&lt;li&gt;clone or download the zip file of this repos (then unzip it).&lt;/li&gt;
&lt;li&gt;click the &amp;ldquo;Scripting&amp;rdquo; tab on the above menu in blender, then click &amp;ldquo;Open&amp;rdquo; tab under &amp;ldquo;Scripting&amp;rdquo;, choose the work path to the mesh_preprocessor.py script in the unzipped repos.&lt;/li&gt;
&lt;li&gt;change the variable named &amp;ldquo;work_path&amp;rdquo; in the mesh_preprocesser.py.&lt;/li&gt;
&lt;li&gt;click the &amp;ldquo;Run Script&amp;rdquo; tab under &amp;ldquo;Scripting&amp;rdquo;, then you are ready to go.
You can follow the steps to run script in blender as shown in the following image.&lt;/li&gt;
&lt;/ol&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mesh_preprocesser_in_blender.png&#34; data-caption=&#34;Fig 1. Steps to run mesh preprocessor in blender.&#34;&gt;
&lt;img data-src=&#34;mesh_preprocesser_in_blender.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. Steps to run mesh preprocessor in blender.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Decomposition_for_normal_images</title>
      <link>https://Alibabade.github.io/project/decomposition_for_normal_images/</link>
      <pubDate>Thu, 11 May 2023 14:00:11 +0800</pubDate>
      <guid>https://Alibabade.github.io/project/decomposition_for_normal_images/</guid>
      <description>&lt;p&gt;This project presents a Matlab program implemented to decompose a normal image into a structure normal image and a detail normal image, where a structure normal image contains the main structure information from the original normal image (obtained by applying DTRF smoothing filter), and a detail normal image contains the detail information from the original normal image.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Normal Decomposition is mainly used for manipulating normal information stored in normal images, where normal images are more like a 2.5D data, a bridge that CONNECTs 3D information and 2D images. Thus, geometry processing applications especially 3D bas-relief modelling is naturally appealing to use such images to reconstruct 3D infromation. However, bas-relief modelling generally has to cope with compression cases, thus it is an essential capability for such application to preserve geometry details well under compression circumstances.&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example1.png&#34; data-caption=&#34;Fig 1. Comparison between value subtraction and vector subtraction.&#34;&gt;
&lt;img data-src=&#34;example1.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. Comparison between value subtraction and vector subtraction.
  &lt;/figcaption&gt;


&lt;/figure&gt;

People dealing with geometry detail preservation, in general, consider to decompose details from normal images first, then manipulate them before reconstruction. However, detail information obtained by value subtraction is broken (see zoom-in details of (e) in Fig 1.), where value subtraction is operated on pixel-level from original normal images and smoothed normal images. In fact, the detail information should be a vector subtraction as the RGB colour of each pixel in a normal image represents a vector. Therefore, to obtain more correct and intact detail information, we should use vector subtraction instead of value subtraction.&lt;/p&gt;
&lt;h2 id=&#34;script-in-matlab&#34;&gt;Script in Matlab&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Alibabade/Decomposition-for-Normal-Images&#34;&gt;Here&lt;/a&gt; is a script written in Matlab to decompose normal based on vector subtraction. Additionally, the script also includes a smoothing function that applies DTRF filter to obtain a structure normal image by smoothing the origial normal image.&lt;/p&gt;
&lt;p&gt;The equation for the vector subtraction is:
$$n1 - n2 = $quaternion(n2,n0).*N1.*quaternion(n2,n0)^{-1}$$
where $N1=(0, n1)$, $n0=(0,0,1)$ a constant vector, and $quaternion(n2,n0)$ and $quaternion(n2,n0)^{-1}$ represent the quaternion and inverse_quaternion calculated from the rotation matrix that rotates vector $n2$ to vector $n0$ via z-axis.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Here is a comparsion between detail information (see (b) and (d) in Fig 1.) and reconstruction results (see (c) and (f) in Fig 1.) between value subtraction and vector subtraction.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Wang M, Wang L, Jiang T, et al. Bas-relief modelling from enriched detail and geometry with deep normal transfer. Neurocomputing, 2021, 453: 825-838.&lt;/li&gt;
&lt;li&gt;Z. Ji, X. Sun, W. Ma, Normal image manipulation for bas-relief generation with hybrid styles, arXiv preprint arXiv:1804.06092.&lt;/li&gt;
&lt;li&gt;M. Wei, Y. Tian, W.-M. Pang, C. C. Wang, M.-Y. Pang, J. Wang, J. Qin, P.-A. Heng, Bas-relief modeling from normal layers, IEEE transactions on visualization and computer graphics 25 (4) (2018) 1651–1665.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Stable_video_style_transfer</title>
      <link>https://Alibabade.github.io/project/stable_video_style_transfer/</link>
      <pubDate>Thu, 06 Feb 2020 21:35:55 +0000</pubDate>
      <guid>https://Alibabade.github.io/project/stable_video_style_transfer/</guid>
      <description>&lt;p&gt;This project aims to deal with the &lt;strong&gt;flickering problem&lt;/strong&gt; caused by naively applying per-frame stylization methods (e.g., &lt;a href=&#34;http://svl.stanford.edu/assets/papers/JohnsonECCV16.pdf&#34;&gt;Fast-Neural-Style&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1703.06868.pdf&#34;&gt;AdaIN&lt;/a&gt;) on videos.&lt;/p&gt;
&lt;h2 id=&#34;1-background&#34;&gt;1. Background&lt;/h2&gt;
&lt;p&gt;In 2016, &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf&#34;&gt;Gatys et al.&lt;/a&gt; are the first to propose an image style transfer algorithm using deep neural networks, which is capable of transforming artistic style (e.g., colours, textures and brush strokes) from a given artistic image to arbitrary photos. The visual appealing results and elegant design of their approach motivate many researchers to dig in this field which is called Neural Artistic Style Transfer by followers. Along with the speedup (nearly real-time) of similar methods, researchers gradually turn their focus to video applications. However, &lt;strong&gt;naively applying these per-frame styling methods causes bad flickering problem which reflects on inconsistent textures among video adjacent frames.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To address the flickering problem, a few approaches made their attempts to achieve coherent video transfer results. In early stage, &lt;a href=&#34;https://arxiv.org/pdf/1605.08153.pdf&#34;&gt;Anderson et al.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1604.08610.pdf&#34;&gt;Ruder et al.&lt;/a&gt; are the very first to introduce temporal consistency by optical flow into video style transfer, and they achieve high coherent results but along with worse ghosting artefacts. Besides, their methods need 3 or 5 mins for each video frame which is less practical in video applications. &lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf&#34;&gt;Huang et al.&lt;/a&gt; and &lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2017/papers/Gupta_Characterizing_and_Improving_ICCV_2017_paper.pdf&#34;&gt;Gupta et al.&lt;/a&gt; propose real-time video style transfer by combining &lt;a href=&#34;http://svl.stanford.edu/assets/papers/JohnsonECCV16.pdf&#34;&gt;Fast-Neural-Style&lt;/a&gt; and temporal consistency. More recently, &lt;a href=&#34;https://arxiv.org/pdf/1703.09211.pdf&#34;&gt;Chen et al.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1708.04538.pdf&#34;&gt;Ruder et al.&lt;/a&gt; propose their methods to achieve more coherent results but sacrifice speed.&lt;/p&gt;
&lt;h2 id=&#34;2-motivation&#34;&gt;2. Motivation&lt;/h2&gt;
&lt;p&gt;We notice that all the methods aforementioned above are built upon feed-forward networks which are sensitive to small perturbations among adjacent frames, for example, lighting, noises and motions may cause large variations in stylised video frames. Thus &lt;strong&gt;there are still space to be improved&lt;/strong&gt;. Besides, their networks are all &lt;strong&gt;in a per-network-per-style pattern&lt;/strong&gt;, which means a training process is needed for each style and the training time may range from hours to days. In contrary, optimisation-based approaches are more stable for perturbations and naturally made for arbitrary styles. Thus we follow the optimisation-based routine.&lt;/p&gt;
&lt;p&gt;Now we need to deal with the problems such as slow runtime and ghosting artefacts. We dig into the reason behind these problems, and observe that there are two drawbacks of previous optimisation-based methods (e.g., &lt;a href=&#34;https://arxiv.org/pdf/1605.08153.pdf&#34;&gt;Anderson et al.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1604.08610.pdf&#34;&gt;Ruder et al.&lt;/a&gt;): &lt;strong&gt;1. their methods complete the entire style transformation for each video frame, which causes slow speed; 2. they have too much temporal consistency constraints between adjacent frames, which causes ghosting artefacts.&lt;/strong&gt; To avoid these drawbacks, we come up with a straightforward idea that we only constrain loose temporal consistency among already stylised frames. In this way, the optimisation process only completes a light style transformation for producing seamless frames, thus it runs much more faster (around 1.8 second including per-frame stylising process) than previous methods (e.g., &lt;a href=&#34;https://arxiv.org/pdf/1605.08153.pdf&#34;&gt;Anderson et al.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1604.08610.pdf&#34;&gt;Ruder et al.&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Following this idea, we need to handle another two problems: &lt;strong&gt;1. inconsistent textures between adjacent stylised frames due to flow errors (ghosting artefacts); 2. image degeneration after long-term running (blurriness artefacts).&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-methodology&#34;&gt;3. Methodology&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Prevent flow errors (ghosting artefacts) via multi-scale flow, incremental mask and multi-frame fusion.&lt;/li&gt;
&lt;li&gt;Prevent image degeneration (blurriness artefacts) via sharpness loss consists of perceptual losses and pixel loss.&lt;/li&gt;
&lt;li&gt;Enhance temporal consistency with loose constraints on both rgb-level and feature level.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;4-qualitative-evaluation&#34;&gt;4. Qualitative Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;41-ablation-study&#34;&gt;4.1 Ablation study&lt;/h3&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;11.png&#34; data-caption=&#34;Fig 1. Ablation study on proposed mask techniques.&#34;&gt;
&lt;img data-src=&#34;11.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. Ablation study on proposed mask techniques.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;12.png&#34; data-caption=&#34;Fig 2. Ablation study on proposed sharpness loss.&#34;&gt;
&lt;img data-src=&#34;12.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2. Ablation study on proposed sharpness loss.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;42-comparison-to-state-of-the-art-methods&#34;&gt;4.2 Comparison to state-of-the-art methods&lt;/h3&gt;
&lt;p&gt;We compare our approach with state-of-the-art methods, and these experiments demonstrate that our method produces more stable and diverse stylised video than them.












  


&lt;video controls &gt;
  &lt;source src=&#34;3_ijcv.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;1_ijcv.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;3_cvst.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;6_cvst.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;3_cvpr2017.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;3_flownet2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;h2 id=&#34;5-more-results&#34;&gt;5. More results&lt;/h2&gt;
&lt;p&gt;Here we show more video style transfer results by our approach with challenging style images.












  


&lt;video controls &gt;
  &lt;source src=&#34;10_Johnson.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;12_huang.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;20_huang.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;20_Johnson.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real_time_recording_sensor_rawdata</title>
      <link>https://Alibabade.github.io/project/real_time_recording_sensor_rawdata/</link>
      <pubDate>Wed, 15 Jan 2020 12:48:30 +0000</pubDate>
      <guid>https://Alibabade.github.io/project/real_time_recording_sensor_rawdata/</guid>
      <description>&lt;p&gt;This is a small prototype that I had been working on when I was a visiting scholar (more like an intern) in Italy. The main purpose is reprogram a sensor receiver software (in python) into an executable file (i.e., EXE) in Windows OS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; Two independent programs for data receiving via Bluetooth and visualization via browser respectively. In addition, two programs depend on various softwares and libraries (i.e., google chrome browser, tkinter, matplotlib etc.), which makes it difficult to install.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; integrate two programs into one executable file without any other software and library dependence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Method:&lt;/strong&gt; 1. Integrate two programs into one program with multiprocessing; 2. Visualize the sensor data and complete functions (i.e., extract data during usr specific time, export data into csv file, sample rate etc.) on a window created via tkinter and matplotlib; 3. Convert python code into a single executable file via pyinstaller.&lt;/p&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;prototype720p.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Fast_photographic_style_transfer</title>
      <link>https://Alibabade.github.io/project/fast_photographic_style_transfer/</link>
      <pubDate>Mon, 13 Jan 2020 21:34:39 +0000</pubDate>
      <guid>https://Alibabade.github.io/project/fast_photographic_style_transfer/</guid>
      <description>&lt;p&gt;This project aims to implement a torch version for fast photographic style transfer based on &lt;a href=&#34;https://cs.stanford.edu/people/jcjohns/eccv16/&#34;&gt;Fast-Neural-Style&lt;/a&gt;. The teaser image is a stylized result by the algorithm described in this project, which takes around 1.40 seconds for $852 \times 480$ resolution image on a single NVIDIA 1080Ti card.&lt;/p&gt;
&lt;p&gt;In this project, I also provide a torch implementation of the Domain Transform (Recursive Filter) which is described in the paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Domain Transform for Edge-Aware Image and Video Processing
Eduardo S. L. Gastal and Manuel M. Oliveira
ACM Transactions on Graphics. Volume 30 (2011), Number 4.
Proceedings of SIGGRAPH 2011, Article 69.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Photographic style transfer aims to transfer only the colour information from a given reference image to a source image without detail distortions. However, neural style transfer methods (i.e., &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf&#34;&gt;Neural-Style&lt;/a&gt; and &lt;a href=&#34;https://cs.stanford.edu/people/jcjohns/eccv16/&#34;&gt;Fast-Neural-Style&lt;/a&gt;) usually tend to distort the details of source image to complete artistic transformation (including colours and textures) for reference images. Thus preserving details or structures in source images without affecting colour transformation is the key to photographic style transfer.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;The idea behind this method is purely strightforward, which preserves the artistic style transformation but matches the colour distribution of reference images better to the source image. Fast-Neural-Style (or Neural-Style) tends to keep the structure details of source images on a single high-level conv layer, which distorts these details and transforms the textures (including colours) of reference images to unexpected regions in source images. In experiments, an interesting thing is found that simply &lt;strong&gt;restricting the structure details of source images in multiple conv layers (both low-level and high level) is able to suppress texture (of reference image) expression and match better colour distribution on generated images&lt;/strong&gt;. However, this still causes the detail loss of source images. To address this problem, &lt;strong&gt;a post-processing step is introduced to extract the detail information from original source image and transfer it to transformed images&lt;/strong&gt;. In image processing, an image is composed by its colour and detail information, in math, $I=C+D$ where $C$ and $D$ denotes the colour and detail information, respectively. Thus $D = I - C$ where $C$ is obtained from image smoothing technique like DTRF in this case.&lt;/p&gt;
&lt;p&gt;In total, the proposed method consists of two steps:1. Fast-Neural-Style with multiple conv layers restriction on detail preservation and a similarity loss; 2, Post-processing Refinement with detail extraction and exchange to transformed image from step 1. The training stage and testing stage are illustrated in below figures:&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;training.png&#34; data-caption=&#34;Fig 1. Training Stage.&#34;&gt;
&lt;img data-src=&#34;training.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. Training Stage.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;testing.png&#34; data-caption=&#34;Fig 2. Testing Stage.&#34;&gt;
&lt;img data-src=&#34;testing.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2. Testing Stage.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;more-results&#34;&gt;More results&lt;/h2&gt;
&lt;p&gt;Here are more stylized examples by this method:













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example1.png&#34; &gt;
&lt;img data-src=&#34;example1.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example2.png&#34; &gt;
&lt;img data-src=&#34;example2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example3.png&#34; &gt;
&lt;img data-src=&#34;example3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example4.png&#34; &gt;
&lt;img data-src=&#34;example4.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example5.png&#34; &gt;
&lt;img data-src=&#34;example5.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;example6.png&#34; &gt;
&lt;img data-src=&#34;example6.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The source image and reference image should share a similar semantic contents, otherwise the transformation will fail to generate faithful results as we do not apply semantic masks for input images.&lt;/li&gt;
&lt;li&gt;This method works well for photography images which basically have 1-3 colour tones. To extreme colourful images, this approach usually fails to achieve faithful results.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;one-more-thing&#34;&gt;One more thing&lt;/h2&gt;
&lt;p&gt;The github code is released in &lt;a href=&#34;https://github.com/Alibabade/Fast-photographic-style-transfer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
