<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Activation functions in DL | Li Wang</title>
    <link>https://Alibabade.github.io/tags/activation-functions-in-dl/</link>
      <atom:link href="https://Alibabade.github.io/tags/activation-functions-in-dl/index.xml" rel="self" type="application/rss+xml" />
    <description>Activation functions in DL</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 24 Dec 2019 21:15:41 +0000</lastBuildDate>
    <image>
      <url>https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Activation functions in DL</title>
      <link>https://Alibabade.github.io/tags/activation-functions-in-dl/</link>
    </image>
    
    <item>
      <title>Activation_functions_in_dl</title>
      <link>https://Alibabade.github.io/post/activation_functions_in_dl/</link>
      <pubDate>Tue, 24 Dec 2019 21:15:41 +0000</pubDate>
      <guid>https://Alibabade.github.io/post/activation_functions_in_dl/</guid>
      <description>&lt;h2 id=&#34;activation-functions&#34;&gt;Activation functions&lt;/h2&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Activation functions works like an on-off button that determines whether the output of a neuron or what information should be passed to next layer. In biology, it works like synaptic in brain which decides what information it passes from one neuron cell to next one. There are several activation functions widely used in neural networks.&lt;/p&gt;
&lt;h3 id=&#34;binary-function-step-function&#34;&gt;Binary function (step function)&lt;/h3&gt;
&lt;p&gt;In one word, the output of binary function is 1 or 0 which is based on whether the input is greater or lower than a threshold. In math, it looks like this:
f(x) = {1, if x &amp;gt; T; 0, otherwise}.&lt;/p&gt;
&lt;p&gt;Cons: it does not allow multiple outputs, and it can not support to classify inputs to one of categories.&lt;/p&gt;
&lt;h3 id=&#34;linear-function&#34;&gt;Linear function&lt;/h3&gt;
&lt;p&gt;f(x) = $cx$. &lt;strong&gt;Cons:&lt;/strong&gt; 1. the deviation of linear function is a constant, which does not help for backpropagation as the deviation is not correlated to its inputs, in another word, it can not distinguih what weights or parameters help to learn the task; 2. linear function makes the entire multiple neural network into one linear layer (as the combination of linear functions is still a linear function), which becomes a simple regression model. It can not handle complex tasks by varying parameters of inputs.&lt;/p&gt;
&lt;h3 id=&#34;non-linear-functions&#34;&gt;Non-linear functions&lt;/h3&gt;
&lt;p&gt;Non-linear functions address the problems by two aspects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The deviation of non-liear function is a function correlated to its inputs, which contributes the backpropagation to learn how to update weights for high accurancy.&lt;/li&gt;
&lt;li&gt;Non-linear functions form the layers with hidden neurons into a deep neural network which is capable of predicting for complicated tasks by learning from complex datasets.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are several popular activation functions used in modern deep neural networks.&lt;/p&gt;
&lt;h3 id=&#34;sigmoidlogistic-regression&#34;&gt;Sigmoid/Logistic Regression&lt;/h3&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://Alibabade.github.io/img/sigmoid.png&#34; data-caption=&#34;Fig 1. Sigmoid Visualization&#34;&gt;
&lt;img data-src=&#34;https://Alibabade.github.io/img/sigmoid.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. Sigmoid Visualization
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Equation: $$Sigmoid(x) = \frac{1}{1+e^{-x}}$$
Derivative (with respect to $x$): $$Sigmoid^{&#39;}(x) = Sigmoid(x)(1-Sigmoid(x))$$
&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;smooth gradient&lt;/strong&gt;, no jumping output values compared to binary function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;output value lies between 0 and 1&lt;/strong&gt;, normalizing output of each neuron.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;right choice for probability prediction&lt;/strong&gt;, the probability of anything exists only between 0 and 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;vanishing gradient&lt;/strong&gt;, the gradient barely changes when $x&amp;gt;2$ or $x&amp;lt;-2$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;computationally expensive.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;non zero centered outputs.&lt;/strong&gt; The outputs after applying sigmoid are always positive, during gradient descent, the gradients on weights in backpropagation will always be positive or negative, which means the gradient updates go too far in different directions, and makes the optimization harder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;softmax&#34;&gt;Softmax&lt;/h3&gt;
&lt;p&gt;$$Softmax(x_i)= \frac{x_i}{\Sigma_{j=1}^{n}{x_j}}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; capable of handling multiple classification and the sum of predicted probabilities is 1. &lt;strong&gt;Cons:&lt;/strong&gt; only used for output layer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Softmax is more suitable for multiple classification case when the predicted class must and only be one of categories. k-sigmoid/LR can be used to classify such multi-class problem that the predicted class could be multiple.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tanh&#34;&gt;Tanh&lt;/h3&gt;
&lt;p&gt;Equation: $$tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
Derivative (with respect to $x$): $$tanh^{&#39;}(x) = 1 -tanh(x)^2$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;zero centered. make it easier to model inputs that have strongly positive, strongly negative, and natural values.&lt;/li&gt;
&lt;li&gt;similar to sigmoid&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;vanishing gradient&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;computationally expensive&lt;/strong&gt; as it includes division and exponential operation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;vanishing-gradient&#34;&gt;Vanishing gradient&lt;/h3&gt;
&lt;p&gt;Vanishing gradient means that the values of weights and biases are barely change along with the training.&lt;/p&gt;
&lt;h3 id=&#34;exploding-gradient&#34;&gt;Exploding gradient&lt;/h3&gt;
&lt;p&gt;Gradient explosion means that the values of weights and biases are increasing rapidly along with the training.&lt;/p&gt;
&lt;h3 id=&#34;relu&#34;&gt;ReLU&lt;/h3&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://Alibabade.github.io/img/relu.png&#34; data-caption=&#34;Fig 2. ReLU Visualization&#34;&gt;
&lt;img data-src=&#34;https://Alibabade.github.io/img/relu.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2. ReLU Visualization
  &lt;/figcaption&gt;


&lt;/figure&gt;

Equation: $$ReLU(x) = max(0, x)$$
Derivative (with respect to $x$):
\begin{equation}
ReLU^{&#39;}(x) = \begin{cases}
0, &amp;amp;x \leqslant 0; \newline
1, &amp;amp; x &amp;gt; 0
\end{cases}
\end{equation}
&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;computationally efficient&lt;/li&gt;
&lt;li&gt;non-linear&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Why ReLU performs better in modern NNs?&lt;/strong&gt; The answer is not so sure right now, but its propeties like &lt;strong&gt;non-saturation gradient&lt;/strong&gt; and &lt;strong&gt;computionally efficient&lt;/strong&gt; indeed lead to fast convergence. Additionally, its property &lt;strong&gt;sparsing the network&lt;/strong&gt; also improves the modeling preformance. The &lt;strong&gt;non-zero centered issue&lt;/strong&gt; can be tackled by other regularization techniques like &lt;strong&gt;Batch Normalization&lt;/strong&gt; which produces a stable distribution for ReLU.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Dying ReLU problem. The backpropagation won&#39;t work when inputs approach zero or negative.
However, to some extent, dying ReLU problem makes input values sparse which is helpful for neural network to learn more important values and perform better.&lt;/li&gt;
&lt;li&gt;Non differentiable at zero.&lt;/li&gt;
&lt;li&gt;Non zero centered.&lt;/li&gt;
&lt;li&gt;Don&#39;t avoid gradient explode&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;elu&#34;&gt;ELU&lt;/h3&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://Alibabade.github.io/img/elu.png&#34; data-caption=&#34;Fig 3. ELU Visualization&#34;&gt;
&lt;img data-src=&#34;https://Alibabade.github.io/img/elu.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 3. ELU Visualization
  &lt;/figcaption&gt;


&lt;/figure&gt;

Equation:
\begin{equation}
ELU(x) = \begin{cases}
\alpha (e^x-1), &amp;amp; x \leqslant 0 \newline
x, &amp;amp;x &amp;gt; 0    &lt;br&gt;
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;Derivative:
\begin{equation}
ELU^{&#39;}(x) = \begin{cases}
ELU(x) + \alpha, &amp;amp; x \leqslant 0 \newline
1, &amp;amp; x &amp;gt; 0
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;prevent dying ReLU problem.&lt;/li&gt;
&lt;li&gt;gradient works when input values are negative.&lt;/li&gt;
&lt;li&gt;non-linear, gradient is not zero.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;don&#39;t avoid gradient explode.&lt;/li&gt;
&lt;li&gt;not computationally efficient.&lt;/li&gt;
&lt;li&gt;$\alpha$ is not learnt by neural networks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;leaky-relu&#34;&gt;Leaky ReLU&lt;/h3&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://Alibabade.github.io/img/lrelu.png&#34; data-caption=&#34;Fig 4. LReLU Visualization ($\alpha=0.1$)&#34;&gt;
&lt;img data-src=&#34;https://Alibabade.github.io/img/lrelu.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 4. LReLU Visualization ($\alpha=0.1$)
  &lt;/figcaption&gt;


&lt;/figure&gt;

Equation:
\begin{equation}
LReLU(x) = \begin{cases}
\alpha x,  &amp;amp;x \leqslant 0 \newline
x, &amp;amp;x &amp;gt; 0
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;Derviative:
\begin{equation}
LReLU^{&#39;}(x) = \begin{cases}
\alpha, &amp;amp;x \leqslant 0 \newline
1, &amp;amp;x &amp;gt; 0
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;prevent Dying ReLU problem&lt;/li&gt;
&lt;li&gt;computationally efficient&lt;/li&gt;
&lt;li&gt;non-linear&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;don&#39;t avoid gradient explode&lt;/li&gt;
&lt;li&gt;Non consistent results for negative input values.&lt;/li&gt;
&lt;li&gt;non-zero centered&lt;/li&gt;
&lt;li&gt;non differentiable at Zeros&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;selu&#34;&gt;SELU&lt;/h3&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://Alibabade.github.io/img/selu.png&#34; data-caption=&#34;Fig 5. SELU Visualization&#34;&gt;
&lt;img data-src=&#34;https://Alibabade.github.io/img/selu.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 5. SELU Visualization
  &lt;/figcaption&gt;


&lt;/figure&gt;

Equation:
\begin{equation}
SELU(x) = \lambda \begin{cases}
\alpha e^x-\alpha, &amp;amp; x \leqslant 0 \newline
x, &amp;amp; x &amp;gt; 0
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;Derivative:
\begin{equation}
SELU^{&#39;}(x) = \lambda \begin{cases}
\alpha e^x, &amp;amp; x \leqslant 0 \newline
1, &amp;amp; x &amp;gt; 0
\end{cases}
\end{equation}
where $\alpha \approx 1.6732632423543772848170429916717$ and $\lambda \approx 1.0507009873554804934193349852946$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Internal normalization, which means faster convergence.&lt;/li&gt;
&lt;li&gt;Preventing vanishing gradient and exploding gradient.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;
Need more applications to prove its performance on CNNs and RNNs.&lt;/p&gt;
&lt;h3 id=&#34;gelu&#34;&gt;GELU&lt;/h3&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://Alibabade.github.io/img/gelu.png&#34; data-caption=&#34;Fig 6. GELU Visualization&#34;&gt;
&lt;img data-src=&#34;https://Alibabade.github.io/img/gelu.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 6. GELU Visualization
  &lt;/figcaption&gt;


&lt;/figure&gt;

Equation:
\begin{equation}
GELU(x) = 0.5x(1 + tanh(\sqrt{\frac{2}{\pi}} (x + 0.044715x^3)))
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Best performance in NLP, especially BERT and GPT-2&lt;/li&gt;
&lt;li&gt;Avoid vanishing gradient&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;
Need more applications to prove its performance.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/&#34;&gt;https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/6db999961393&#34;&gt;https://www.jianshu.com/p/6db999961393&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/activation-functions-b63185778794&#34;&gt;https://towardsdatascience.com/activation-functions-b63185778794&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions&#34;&gt;https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
