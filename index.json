[{"authors":["admin"],"categories":null,"content":"I am a Machine Learning and Computer Vision Engineer with 6+ years experience (including 3+ years commercial experience). I have managed and completed 7+ projects, which applies cutting-edge machine learning and deep learning models to solve real-world and research cases. I now work as an Independent Computer Vision Engineer, Freelancer and Incubator as well. My work mainly focus on software/system development and image/video/geometry processing. I obtained my PhD degree on Computer Vision and Deep Learning at National Centre for Computer Animation, Bournemouth University, Top1 animation centre in UK, 2020. My research focuses on image, video and geometry using neural style transfer, in particular, photo style transfer, coherent video style transfer and geometry texture synthesis. I received my M.Eng. and B.Sci. degrees in Computer Science at Jilin University in 2016 and 2013, respectively.\nI like to get my hands dirty on programming and try new things on up-to-date applications. I love sharing ideas with people and learning from them as well. I post many things on this website, and hope you\u0026rsquo;ll find something interesting.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1683784932,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Alibabade.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Machine Learning and Computer Vision Engineer with 6+ years experience (including 3+ years commercial experience). I have managed and completed 7+ projects, which applies cutting-edge machine learning and deep learning models to solve real-world and research cases. I now work as an Independent Computer Vision Engineer, Freelancer and Incubator as well. My work mainly focus on software/system development and image/video/geometry processing. I obtained my PhD degree on Computer Vision and Deep Learning at National Centre for Computer Animation, Bournemouth University, Top1 animation centre in UK, 2020.","tags":null,"title":"Li Wang","type":"authors"},{"authors":[],"categories":[],"content":"This project presents a mesh preprocessor tool which works with python and blender. It\u0026rsquo;s capable of processing mesh files in batch manner.\nIntroduction The main reason to create such a mesh preprocessor is that preparing raw mesh files (especially .obj file format) for datasets usually is time-consuming, and it kind of needs tons of manual work. To free hands, a mesh processor tool is presented to do the dirty work for people.\nFunctions A few functions are added in this tool, for example, subdivide mesh into (or closest to) desired vertex number, triangulate the mesh if needed, and extract vertex indices of arbitrary sub-surfaces from the mesh.\nTime consumption For a computer with i7-6700k cpu and 16GB memory, it will take about ~3 hours when processing over 100 meshes (low poly) plus subdivding them into 50K vertices plus extracting 1K sub-surfaces for each mesh. It will take about ~9 hours when processing over 100 meshes (low poly) plus subdivding them into 150K vertices plus extracting 1K sub-surfaces for each mesh. The time consumption varies on the cpu and memory.\nInputs and Outputs  inputs are mesh files ending with \u0026lsquo;.obj\u0026rsquo;, which are original mesh files you find or download online. In this repos, a folder named \u0026lsquo;data\u0026rsquo; contains two examples. outputs are mesh files ending with \u0026lsquo;_tri.obj\u0026rsquo; which are subdivided and triangulated mesh files, and data files ending with \u0026lsquo;.npy\u0026rsquo; under corresponding \u0026ldquo;vertex_indinces_of_arbitrary_surfaces\u0026rdquo; which contains a list of numpy arrays of vertex indinces of sub-surfaces from each mesh.  Code You can find the code in here.\nUsage  open blender software, if not installed, then download it from here. NO installation needed just unzip it. clone or download the zip file of this repos (then unzip it). click the \u0026ldquo;Scripting\u0026rdquo; tab on the above menu in blender, then click \u0026ldquo;Open\u0026rdquo; tab under \u0026ldquo;Scripting\u0026rdquo;, choose the work path to the mesh_preprocessor.py script in the unzipped repos. change the variable named \u0026ldquo;work_path\u0026rdquo; in the mesh_preprocesser.py. click the \u0026ldquo;Run Script\u0026rdquo; tab under \u0026ldquo;Scripting\u0026rdquo;, then you are ready to go. You can follow the steps to run script in blender as shown in the following image.     Fig 1. Steps to run mesh preprocessor in blender.   ","date":1684219601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684238306,"objectID":"a813f938e888323f639fc0a97b5f7339","permalink":"https://Alibabade.github.io/project/mesh_preprocessor_in_blender_with_python/","publishdate":"2023-05-16T14:46:41+08:00","relpermalink":"/project/mesh_preprocessor_in_blender_with_python/","section":"project","summary":"This project presents a mesh preprocessor tool which works with python and blender. It\u0026rsquo;s capable of processing mesh files in batch manner.\nIntroduction The main reason to create such a mesh preprocessor is that preparing raw mesh files (especially .obj file format) for datasets usually is time-consuming, and it kind of needs tons of manual work. To free hands, a mesh processor tool is presented to do the dirty work for people.","tags":[],"title":"Mesh_preprocessor_in_blender_with_python","type":"project"},{"authors":[],"categories":[],"content":"This project presents a Matlab program implemented to decompose a normal image into a structure normal image and a detail normal image, where a structure normal image contains the main structure information from the original normal image (obtained by applying DTRF smoothing filter), and a detail normal image contains the detail information from the original normal image.\nIntroduction Normal Decomposition is mainly used for manipulating normal information stored in normal images, where normal images are more like a 2.5D data, a bridge that CONNECTs 3D information and 2D images. Thus, geometry processing applications especially 3D bas-relief modelling is naturally appealing to use such images to reconstruct 3D infromation. However, bas-relief modelling generally has to cope with compression cases, thus it is an essential capability for such application to preserve geometry details well under compression circumstances.\n    Fig 1. Comparison between value subtraction and vector subtraction.   People dealing with geometry detail preservation, in general, consider to decompose details from normal images first, then manipulate them before reconstruction. However, detail information obtained by value subtraction is broken (see zoom-in details of (e) in Fig 1.), where value subtraction is operated on pixel-level from original normal images and smoothed normal images. In fact, the detail information should be a vector subtraction as the RGB colour of each pixel in a normal image represents a vector. Therefore, to obtain more correct and intact detail information, we should use vector subtraction instead of value subtraction.\nScript in Matlab Here is a script written in Matlab to decompose normal based on vector subtraction. Additionally, the script also includes a smoothing function that applies DTRF filter to obtain a structure normal image by smoothing the origial normal image.\nThe equation for the vector subtraction is: $$n1 - n2 = $quaternion(n2,n0).*N1.*quaternion(n2,n0)^{-1}$$ where $N1=(0, n1)$, $n0=(0,0,1)$ a constant vector, and $quaternion(n2,n0)$ and $quaternion(n2,n0)^{-1}$ represent the quaternion and inverse_quaternion calculated from the rotation matrix that rotates vector $n2$ to vector $n0$ via z-axis.\nResults Here is a comparsion between detail information (see (b) and (d) in Fig 1.) and reconstruction results (see (c) and (f) in Fig 1.) between value subtraction and vector subtraction.\nReference  Wang M, Wang L, Jiang T, et al. Bas-relief modelling from enriched detail and geometry with deep normal transfer. Neurocomputing, 2021, 453: 825-838. Z. Ji, X. Sun, W. Ma, Normal image manipulation for bas-relief generation with hybrid styles, arXiv preprint arXiv:1804.06092. M. Wei, Y. Tian, W.-M. Pang, C. C. Wang, M.-Y. Pang, J. Wang, J. Qin, P.-A. Heng, Bas-relief modeling from normal layers, IEEE transactions on visualization and computer graphics 25 (4) (2018) 1651–1665.  ","date":1683784811,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683876433,"objectID":"1abe2765c2ec59e3a88e53a078d3d6de","permalink":"https://Alibabade.github.io/project/decomposition_for_normal_images/","publishdate":"2023-05-11T14:00:11+08:00","relpermalink":"/project/decomposition_for_normal_images/","section":"project","summary":"This project presents a Matlab program implemented to decompose a normal image into a structure normal image and a detail normal image, where a structure normal image contains the main structure information from the original normal image (obtained by applying DTRF smoothing filter), and a detail normal image contains the detail information from the original normal image.\nIntroduction Normal Decomposition is mainly used for manipulating normal information stored in normal images, where normal images are more like a 2.","tags":[],"title":"Decomposition_for_normal_images","type":"project"},{"authors":["Jaime Martín-Martín","Li Wang","Irene De-Torres","Adrian Escriche-Escuder","Manuel González-Sánchez","Antonio Muro-Culebras","Cristina Roldán-Jiménez","María Ruiz-Muñoz","Fermín Mayoral-Cleries","Attila Biró","Wen Tang","Borjanka Nikolova","Alfredo Salvatore","Antonio I Cuesta-Vargas"],"categories":[],"content":"","date":1682384609,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682497209,"objectID":"b7eff5fbfffb776c5f35baa57837ca8c","permalink":"https://Alibabade.github.io/publication/jmartin-2022-the/","publishdate":"2023-04-25T09:03:29+08:00","relpermalink":"/publication/jmartin-2022-the/","section":"publication","summary":"","tags":[],"title":"The Validity of the Energy Expenditure Criteria Based on Open Source Code through two Inertial Sensors","type":"publication"},{"authors":["Meili Wang*","Li Wang*","Tao Jiang","Nan Xiang","Juncong Lin","Mingqiang Wei","Xiaosong Yang","Taku Komura","Jianjun Zhang"],"categories":null,"content":"","date":1628363148,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682042025,"objectID":"79a6753b3d9eb990078860a2df040de2","permalink":"https://Alibabade.github.io/publication/wang-2021-bas/","publishdate":"2021-08-07T20:05:48+01:00","relpermalink":"/publication/wang-2021-bas/","section":"publication","summary":"","tags":null,"title":"Bas-relief modelling from enriched detail and geometry with deep normal transfer","type":"publication"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"This project aims to deal with the flickering problem caused by naively applying per-frame stylization methods (e.g., Fast-Neural-Style and AdaIN) on videos.\n1. Background In 2016, Gatys et al. are the first to propose an image style transfer algorithm using deep neural networks, which is capable of transforming artistic style (e.g., colours, textures and brush strokes) from a given artistic image to arbitrary photos. The visual appealing results and elegant design of their approach motivate many researchers to dig in this field which is called Neural Artistic Style Transfer by followers. Along with the speedup (nearly real-time) of similar methods, researchers gradually turn their focus to video applications. However, naively applying these per-frame styling methods causes bad flickering problem which reflects on inconsistent textures among video adjacent frames.\nTo address the flickering problem, a few approaches made their attempts to achieve coherent video transfer results. In early stage, Anderson et al. and Ruder et al. are the very first to introduce temporal consistency by optical flow into video style transfer, and they achieve high coherent results but along with worse ghosting artefacts. Besides, their methods need 3 or 5 mins for each video frame which is less practical in video applications. Huang et al. and Gupta et al. propose real-time video style transfer by combining Fast-Neural-Style and temporal consistency. More recently, Chen et al. and Ruder et al. propose their methods to achieve more coherent results but sacrifice speed.\n2. Motivation We notice that all the methods aforementioned above are built upon feed-forward networks which are sensitive to small perturbations among adjacent frames, for example, lighting, noises and motions may cause large variations in stylised video frames. Thus there are still space to be improved. Besides, their networks are all in a per-network-per-style pattern, which means a training process is needed for each style and the training time may range from hours to days. In contrary, optimisation-based approaches are more stable for perturbations and naturally made for arbitrary styles. Thus we follow the optimisation-based routine.\nNow we need to deal with the problems such as slow runtime and ghosting artefacts. We dig into the reason behind these problems, and observe that there are two drawbacks of previous optimisation-based methods (e.g., Anderson et al. and Ruder et al.): 1. their methods complete the entire style transformation for each video frame, which causes slow speed; 2. they have too much temporal consistency constraints between adjacent frames, which causes ghosting artefacts. To avoid these drawbacks, we come up with a straightforward idea that we only constrain loose temporal consistency among already stylised frames. In this way, the optimisation process only completes a light style transformation for producing seamless frames, thus it runs much more faster (around 1.8 second including per-frame stylising process) than previous methods (e.g., Anderson et al. and Ruder et al.).\nFollowing this idea, we need to handle another two problems: 1. inconsistent textures between adjacent stylised frames due to flow errors (ghosting artefacts); 2. image degeneration after long-term running (blurriness artefacts).\n3. Methodology  Prevent flow errors (ghosting artefacts) via multi-scale flow, incremental mask and multi-frame fusion. Prevent image degeneration (blurriness artefacts) via sharpness loss consists of perceptual losses and pixel loss. Enhance temporal consistency with loose constraints on both rgb-level and feature level.  4. Qualitative Evaluation 4.1 Ablation study     Fig 1. Ablation study on proposed mask techniques.      Fig 2. Ablation study on proposed sharpness loss.   4.2 Comparison to state-of-the-art methods We compare our approach with state-of-the-art methods, and these experiments demonstrate that our method produces more stable and diverse stylised video than them.      \n5. More results Here we show more video style transfer results by our approach with challenging style images.    \n","date":1581024955,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581110349,"objectID":"5c0a314ce4b5df01411855f92b1d2941","permalink":"https://Alibabade.github.io/project/stable_video_style_transfer/","publishdate":"2020-02-06T21:35:55Z","relpermalink":"/project/stable_video_style_transfer/","section":"project","summary":"This project aims to deal with the flickering problem caused by naively applying per-frame stylization methods (e.g., Fast-Neural-Style and AdaIN) on videos.\n1. Background In 2016, Gatys et al. are the first to propose an image style transfer algorithm using deep neural networks, which is capable of transforming artistic style (e.g., colours, textures and brush strokes) from a given artistic image to arbitrary photos. The visual appealing results and elegant design of their approach motivate many researchers to dig in this field which is called Neural Artistic Style Transfer by followers.","tags":["Academic"],"title":"Stable_video_style_transfer","type":"project"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"1. Background In this post, we use the following symbols for all sections:\n $W$: width, $H$: height $N$: input channel, $M$: output channel $K$: convolution kernel size.  1.1 Params for a neural network Params is related to the model size, the unit is Million in float32, thus the model size is approximately 4 times of params. For a standard convolution operation, the params = $(K^2 \\times N + 1)M$, without bias: $K^2 \\times NM$. For a standard fully connection layer, the params = $(N+1)M$, without bias: $NM$.\n1.2 Computation complexity (FLOPs) Computational complexity (or cost) is related to speed (but indirect), and it is usually written as FLOPs. Here only multiplication-adds is considered. For a standard convolution operation, the FLOPs = $WHN \\times K^2M$. For a fully connection layer, the FLOPs = $(N+1)M$, without bias: $NM$. Here we can see that the FLOPs is nearly $WH$ times of Params for a conv operation.\n1.3 Compute the params and FLOPs in PyTorch In pytorch, opCounter library can be used to compute the params and FLOPs of a model. Install opCouter first:\npip install thop  For instance, computing these numbers can be done by following code:\nfrom torchvision.models import resnet50 from thop import profile model = resnet50() input = torch.randn(1,3,224,224) flops, params = profile(model, inputs=(input,))  2. Computational cost for convolution layers    Fig 1. The illustration of computational cost in a convolution operation. Image source:this blog.   We will check on the computation of a general convolution operation. For example, in Fig 1, the input feature has the dimensions like width $W$ $\\times$ height $H$ $\\times$ N (input channels), the convolution kernel has dimension like $K \\times K$ (kernel size) $\\times$ M (output channels) and the convolutional operation has stride=1 and padding=1 which keeps the width and height same between input and output, thus the output feature will have the dimension $W \\times H \\times M$. Then the multiply-add computation (standard computational cost) of a general conv operation is $WHN \\times K^2 M$.\n2.1 Computation cost of conv $3 \\times 3$ and conv $1 \\times 1$ Normally, the most used conv kernel in modern neural network is $3 \\times 3$, which is denoted as conv $3 \\times 3$. Its computational cost is $WHN3^2M$ when the convolution operates on both spatial and channel domain. If we illustrate the computation cost on spatial and channel domain, the following fig could be a better visualization. We can there is a fully connection between input channels and output channels.    Fig 2. The illustration of computational cost for conv $3\\times3$ and conv $1\\times1$ operation. Image recreated from this blog.   For conv $1\\times1$, the spatial connect is $1\\times1$ linear projection while channel projection is still fully connected, which leads to computational cost $WHN \\times 1^2 M$. Compared to conv $3 \\times 3$, the computation is reduced by $\\frac{1}{K^3} = \\frac{1}{9}$ in this case.\n2.2 Computation cost of group convolution Group convolution is firstly introduced in AlexNet to deal with the insufficient GPU memory and, to some extent, reduce the learned number of parameters. Grouped convolution operates on channel domain, which divides the channels into $G$ groups and the information in different groups is not shared. In each group, the connection still follows the fully connection way. The computation costs for grouped conv $3\\times3$ and conv $1\\times1$ are as following:    Fig 3. The illustration of computational cost for grouped conv $3\\times3$ where $G=2$ in this case. Image recreated from this blog.   Compared to standard conv, the grouped conv $3\\times3$ (where $G=2$) reduce the connection in channel domain by factor $G$, which results in $\\frac{1}{G}$ times of standard conv.\n2.3 Computation cost of depthwise convolution Depthwise convolution is firstly introduced in MobileNet v1, which performs the convolution operation independently to for each of input channels. Actually, this can be regarded as a special case of grouped conv when $G=N$. Usually, output channel $M \u0026raquo; K^2$, thus depthwise conv significantly reduces the computational cost compared to standard conv operation.    Fig 4. The illustration of computational cost for depthwise conv $3\\times3$. Image recreated from this blog.   2.4 Channel shuffle Channel shuffle is an operation introduced in ShuffleNet v1 to deal with the large computational cost by conv $1\\times1$ in ResNeXt network. In this section, we basic show how channel shuffle works and introduce more details in Section 5. The operation is first divide the input channels $N$ into $G$ groups, which results in $G \\times N^{`}$ channels. Usually, the $N^{`}$ is a certain times of $G$. In this case, $N^{`}=9$ and $G=3$. Then one certain channel in a group will stay in the same group, but each of the rest channel in a group will be separately assigned to other groups. The figure below illustrates the channel shuffle operation.    Fig 5. The illustration of channel shuffle operation. Image recreated from this blog.   3. ResNet, ResNeXt 3.1 Bottleneck architecture comparison Basic idea in ResNeXt is to replace standard conv $3\\times3$ by group conv $3\\times3$.    Fig 6. Architecture comparison of ResNet and ResNeXt.   3.2 FLOPs comparison Note that FLOPs of ResNeXt is only reduced by a small budget of computational cost and it\u0026rsquo;s up to group number $G$ when $N_r=N_x$ and $M_r=M_x$.    Fig 7. FLOPs comparison of ResNet and ResNeXt.   4. MobileNet v1 vs v2 4.1 MobileNet v1 (VGG) Key point is to replace all standard conv $3\\times3$ with depthwise conv $3\\times3$ + conv $1\\times1$ in standard VGGNet. This blog says that the ReLU is also replaced by ReLU6 (ReLU6 = $max(max(0,x),6)$) in MobileNet v1, but the original paper does not say anything about it. Perhaps in engineering projects, people usually use ReLU6.    Fig 8. Comparison of standard conv $3\\times3$ in VGG and Separable Depthwise conv $3\\times4$ + conv $1\\times1$ in MobileNetv1. Image recreated from original paper.   Therefore, the FLOPs is reduced about $\\frac{1}{8}$ ~ $\\frac{1}{9}$ in MobileNetv1 compared to VGGNet.    Fig 9. FLOPs Comparison of standard conv $3\\times3$ in VGG and Separable Depthwise conv $3\\times4$ + conv $1\\times1$ in MobileNetv1. Image recreated from This blog.   4.2 MobileNet v2 (ResNet) Key point is to replace the last ReLU with linear bottleneck and introduce an inverted residual block in ResNet. The reason behind of replacing relu with a linear transformation is that relu causes much information loss when input dimension is low. The inverted residual block consists of a conv $1\\times1$ (expanse low dimension input channels to high dimension channels) + a depthwiseconv $3\\times3$ + a conv $1\\times1$ (decrease high dimension input channels to low dimension (original) channels).    Fig 10. Reason behind of replacing relu with linear botthleneck. Image source from original paper.   Inverted residual block in Mobilenet v2 (see figure 9). Only the last ReLU is replaced by a linear transformation, because the input dimensions of conv $1\\times1$ and depthwise conv $3\\times$ are increased compared to original dimension, thus relu works fine. But when the input dimension is decreased by the last conv $1\\times1$, relu will lose much information, thus we replace relu with a linear transformation to preserve as much as original information.    Fig 11. Comparison of standard residual block in ResNet and inverted residual block in MobileNetv2.   Here we compute the FLOPs for a standard residual block in ResNet and an inverted residual block in MobileNet v2. As can be seen, when the ResNet input channel $N_r$ and output channel $M_r$ is equal to MobileNet v2 input channel $N_{m2}$ and output channel $M_{m2}$, MobileNet v2 has a larger FLOPs than ResNet. However, the advantage of MobileNet v2 is that it only needs a much smaller input channel and output channel while achieves similar accuracy of ResNet, which eventually leads to smaller FLOPs than ResNet.    Fig 12. FLOPs Comparison of standard residual block in ResNet and inverted residual block in MobileNetv2.   4.3 Comparison Here shows the convolution block in MobileNet v1 and v2, and their FLOPs comparison.    Fig 13. Comparison of a convolution block in MobileNet v1 and two types of inverted residual block in MobileNetv2. There is no shortcut connection when stride=2 in DepthwiseConv $3\\times3$ in MobileNet v2.   Note that the FLOPs for a single inverted residual block has an extra term ($N_{m2}$) as there is an extra conv $1\\times1$ used compared to MobileNet v1. However, MobileNet v2 still has smaller params and FLOPs than MobileNet v1 as the input channel $N_{m2}$ and output channel $M_{m2}$ could be smaller than $N_{m1}$ and $M_{m1}$ of MobileNet v1. Please refer to Table 3 in original MobileNet v2 paper for more details.    Fig 14. Comparison of FLOPs of a convolution block in MobileNet v1 and an inverted residual block in MobileNetv2.   Why MobileNet v2 is not faster than MobileNet v1 on Desktop computer? On desktop, the separable depthwise convolution is not directly supported in GPU firmware (cuDNN library). While MobileNet v2 could be slightly slower than MobileNet v1 as V2 has more separable depthwise convolution operations and more larger input channels (96/192/384/768/1536) of using separable depthwise convolution than V1 (64/128/256/512/1024).\nWhy MobileNet is not as fast as FLOPs indicates in practice? One reason could be the application of memory takes much time (according to some interviews).\n5. ShuffleNet v1 vs v2 5.1 ShuffleNet v1 (ResNeXt) ResNeXt is an efficient model for ResNet by introducing group conv $3\\times3$ to reduce computational cost. However, the computational cost of conv $1\\times1$ become the operation consuming most of time. To reduce the FLOPs of conv $1\\times1$, ShuffleNet v1 introduce group conv $1\\times1$ tp replace the standard conv $1\\times1$. However, the features won\u0026rsquo;t be shared between groups by using group conv $1\\times1$, which causes less feature reuse and accuracy. To address this problem, a channel shuffle operation is introduced to share features between groups. The basic blocks of ResNeXt and ShuffleNet v1 is shown in the below figure.    Fig 15. Comparison of a residual block in ResNeXt and ShuffleNet v1.   Here we also give the computational cost of each method:\nResNeXt FLOPs = $WH(2N_r M_r + 9M_r^2/G)$\nShuffleNet v1 FLOPs = $WH(2N_{s1}M_{s1}/G + 9M_{s1})$, where $G$ is the group number.\nIt is obviously that ShuffleNet v1 FLOPs \u0026lt; ResNeXt FLOPs when $N_r=N_{s1}$ and $M_r = M_{s1}$.\n   Fig 16. FLOPs comparison of a residual block in ResNeXt and ShuffleNet v1.   5.2 ShuffleNet v2 ShuffleNet v2 points out that FLOPs is an indirect metric to evaluate computational cost of a model since the run time should also contain the memory access cost (MAC) , degree of parallelism and even hardware platform (e.g., GPU and ARM). Thus shufflenet v2 introduces a few rules to evaluate the computational cost of a model by considering the factors above.\n5.2.1 Guidelines for evaluating computational cost  G1. MAC is minimal when input channel is equal to output channel. Let $WHN$ denote input feature, $WHM$ be output feature. Then FLOPs $F=WHNM$ when conv kernel is $1\\times1$. We simply assume that input feature occupies $WHN$ memory, output feature occupies $WHM$ memory, and conv kernels occupy $NM$ memory. Then MAC can be denoted as: \\begin{eqnarray} MAC \u0026amp;=\u0026amp; WHN + WHM +NM \\\\\\\n\u0026amp;=\u0026amp; WH(N+M) + NM \\\\\\\n\u0026amp;=\u0026amp; \\sqrt{(WH)^2(N+M)^2} + \\frac{F}{WH} \\\\\\\n\u0026amp;\\geqslant\u0026amp; \\sqrt{(WH)^2\\times 4NM} + \\frac{F}{WH} \\\\\\\n\u0026amp;=\u0026amp; \\sqrt{(WH)\\times 4WHNM} + \\frac{F}{WH} \\\\\\\n\u0026amp;=\u0026amp; \\sqrt{(WH)\\times 4F} + \\frac{F}{WH} \\\\\\\n\\end{eqnarray} Thus MAC achieves the minimal value when input channel $N$ is equal to output channel $M$ under same FLOPs. G2. MAC increases when the number of group increases. FLOPs $F=WH \\times N \\times M/G$. Then MAC is denoted as: \\begin{eqnarray} MAC \u0026amp;=\u0026amp; WHN + WHM + \\frac{NM}{G} \\\\\\\n\u0026amp;=\u0026amp; F\\times \\frac{G}{M} + F \\times \\frac{G}{N} + \\frac{F}{WH} \\\\\\\n\\end{eqnarray} Thus MAC increase with the growth of $G$. G3. Network fragmentation reduces degree of parallelism. More fragmentation causes more computational cost in GPU. For example, under the same FLOPs, the computation efficiency is as following order: 1-fragmentation \u0026gt; 2-fragmentation-series \u0026gt; 2-fragmentation-parallel \u0026gt; 4-fragmentation-series \u0026gt; 4-fragmentation-parallel.    Fig 17. Computational cost of different network fragmentations.    G4. Element-wise operations consume much time. Except convolution operations, the element-wise operation is the second operation consuming much time.    Fig 18. Computational cost of different operations.     Based on the guidelines above, we can analyse that shufflenet v1 introduces group convolutions which is against G2, and if it uses bottleneck-like blocks (using conv$1\\times1$ change input channels) then it is against G1. MobileNet v2 introduces an inverted residual bottleneck which is against G1. And it uses depthwise conv $3\\times3$ and ReLU on expansed features and leads to more element-wise operation which violates G4. The autogenerated structures (searched network) add more fragmentations which violates G3.\n5.2.2 ShuffleNet v2 architecture Following the guidelines in aforementioned section, shufflenet v2 introduces their new version of shufflenet.    Fig 19. Architecture of ShuffleNet v1 and ShuffleNet v2.   Since the channel split, concat and channel shuffle are basically not multiplication-adds operations, the FLOPs is computed in the block inside. Note that $N_{s2}=M_{s2}=\\frac{N_{s1}}{2}$, thus shuffleNet v2 is more efficient than ShuffleNet v1 ($G=4$) even under FLOPs evaluation.    Fig 20. FLOPs comparison of ShuffleNet v1 and ShuffleNet v2.      Fig 21. Architecture comparison of Shufflenet v1 and v2. Image source: original paper   5.3 Comparison to other state-of-the-art methods    Fig 22. Comparison to other state-of-the-art methods. Image source: original paper   5.2.3 One more thing ShuffleNet v2 shares the similar idea with DenseNet that is strong feature reuse, which makes ShuffleNet v2 achieve similar high accuracy as DenseNet but in a more efficient manner. More recently, a CondenseNet (upgraded DenseNet) points out that the more short distance features the more important they are to current layer features. Similar to Condensenet, feature map at $j$-th bottleneck building in ShuffleNet v2 reuses $\\frac{c_i}{2^{j-i}}$ of feature maps at $i$-th bottleneck building, which reuses more feature maps when $j$ is more close to $i$.\nReference:  https://zhuanlan.zhihu.com/p/37074222 https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d https://zhuanlan.zhihu.com/p/51566209 https://cloud.tencent.com/developer/article/1461275  ","date":1580766600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580988999,"objectID":"0249353d40f22b0e28b8f318e67791e5","permalink":"https://Alibabade.github.io/post/fast_mobilenet_shufflenet/","publishdate":"2020-02-03T21:50:00Z","relpermalink":"/post/fast_mobilenet_shufflenet/","section":"post","summary":"Brief summary of efficient Mobilenet and Shufflenet","tags":["Academic"],"title":"EfficientNet_mobilenet_shufflenet","type":"post"},{"authors":[],"categories":[],"content":"","date":1580484362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580503178,"objectID":"1ed316683cb07330c5c355d36cd61f12","permalink":"https://Alibabade.github.io/post/algorithms_in_ml/","publishdate":"2020-01-31T15:26:02Z","relpermalink":"/post/algorithms_in_ml/","section":"post","summary":"","tags":[],"title":"Algorithms_in_ml","type":"post"},{"authors":["Li Wang"],"categories":["mathematic"],"content":"This post is an archive for some specific and easy forgotten mathematic knowledge like linear algebra and optimization behind ML/DL.\n1. Linear Algebra 1.1 Rank of a matrix Given $n$ number of math functions with $m$ variables and it is written as following:\n\\begin{eqnarray} \u0026amp;a_{11} x_1 + a_{12} x_2 + \u0026amp;a_{13} x_3 + \u0026amp;\\cdot \\cdot \\cdot + a_{1m} x_{m} \u0026amp;= b_1 \\\\\\\n\u0026amp;a_{21} x_1 + a_{22} x_2 + \u0026amp;a_{23} x_3 + \u0026amp;\\cdot \\cdot \\cdot + a_{2m} x_{m} \u0026amp;= b_2 \\\\\\\n\u0026amp;a_{31} x_1 + a_{32} x_2 + \u0026amp;a_{33} x_3 + \u0026amp;\\cdot \\cdot \\cdot + a_{3m} x_{m} \u0026amp;= b_3 \\\\\\\n\u0026amp;\\cdot \\cdot \\cdot \u0026amp;\\cdot \\cdot \\cdot \u0026amp;\\cdot \\cdot \\cdot \u0026amp;\\cdot \\cdot \\cdot \\\\\\\n\u0026amp;a_{n1} x_1 + a_{n2} x_2 + \u0026amp;a_{n3} x_3 + \u0026amp;\\cdot \\cdot \\cdot + a_{nm} x_{m} \u0026amp;= b_n \\\\\\\n\\end{eqnarray}\nIf we extract all the coefficient from the math functions above, and name each row of coefficient as $\\alpha_i$ where $i=(1,2,\u0026hellip;,n)$, then we have a column of vectors ($\\alpha_1, \\alpha_2, \u0026hellip;, \\alpha_n$) and each of them is as following:\n\\begin{eqnarray} \u0026amp;(a_{11}, a_{12}, \\cdot \\cdot \\cdot, a_{1m}) \u0026amp;= \\alpha_1 \\\\\\\n\u0026amp;(a_{21}, a_{22}, \\cdot \\cdot \\cdot, a_{2m}) \u0026amp;= \\alpha_2 \\\\\\\n\u0026amp;\\cdot\\cdot\\cdot \\\\\\\n\u0026amp;(a_{n1}, a_{n2}, \\cdot \\cdot \\cdot, a_{nm}) \u0026amp;= \\alpha_n \\\\\\\n\\end{eqnarray}\nNow we explain a noun called linear independent set consisting of $\\alpha_i$. If each coefficient $k_i \\in \\mathbb{R}$ where $i=(1,2,\u0026hellip;,s)$ have to be 0 like $k_1 = k_2 = \\cdot \\cdot \\cdot = k_s = 0$ in order to make the following equation equal 0: $$k_1 \\alpha_1 + k_2 \\alpha_2 + \\cdot \\cdot \\cdot + k_s \\alpha_s = 0$$ then we say $(\\alpha_1,\\alpha_2,\u0026hellip;,\\alpha_s)$ is a linear independent set.\nFor a matrix, the Rank is the maximum number of $\\alpha$ to be a linear independent set. In other words, Rank is the number of $\\alpha$ in the maximal linear independent set.\nFor example, only if $k_1$ and $k_2$ have to be 0 to make sure $k_1 \\alpha_1 + k_2 \\alpha_2=0$, then we say $(\\alpha_1, \\alpha_2)$ is a linear independent set, and the number of this linear independent set is 2. Next, only if $k_1=k_2=k_3=0$ makes sure $k_1 \\alpha_1 + k_2 \\alpha_2 + k_3 \\alpha_3=0$, then we say $(\\alpha_1, \\alpha_2, \\alpha_3)$ is a linear independent set, and the number is 3. Continue, we firstly find that NOT all of $k_1$,$k_2$,\u0026hellip;,$k_s$ and $k_{s+1}$ have to be 0 and it still makes $k_1 \\alpha_1 + k_2 \\alpha_2 + k_3 \\alpha_3 + \\cdot \\cdot \\cdot + k_s \\alpha_s=0$ happen, then we say $(\\alpha_1, \\alpha_2, \\alpha_3,\u0026hellip;,\\alpha_{s+1})$ is not a linear independent set. And we call $(\\alpha_1, \\alpha_2, \\alpha_3,\u0026hellip;,\\alpha_{s})$ as the maximal linear independent set, and the number is $s$. If each $\\alpha_i$ contains one row of coefficient in a series of equations, then the Rank of the matrix composed by all the coefficient is $s$.\n1.2 Inverse of a matrix If a matrix $A$ has $n$ rows and $n$ columns, which is named $A_{n \\times n}$, and rank of $A_{n \\times n}$ is equal to $n$, then the matrix has an inverse matrix. The definition of inverse of a matrix is, there is a matrix $B_{n \\times n}$, which makes sure the following equation: $$AB=BA=I_n$$ where $I_n$ is the identity matrix (or unit matrix), of size $n \\times n$ , with ones on the main diagonal and zeros elsewhere.\n1.3 Positive Definite and Positive Semi-Definite Matrix Definition of Positive Definite Matrix: Given a $n \\times n$ matrix $A$, and $A$ is a real symmetric matrix (e.g., $A \\in \\mathbb{R}^{n \\times n}$). If $\\forall x \\in \\mathbb{R}^n$ made: $$x^T A x \u0026gt; 0$$, then matrix $A$ is a positive definite matrix.\nDefinition of Positive Semi-Definite Matrix: Given a $n \\times n$ matrix $A$, and $A$ is a real symmetric matrix (e.g., $A \\in \\mathbb{R}^{n \\times n}$). If $\\forall x \\in \\mathbb{R}^n$ made: $$x^T A x \\geq 0$$, then matrix $A$ is a positive semi-definite matrix.\n2. Convex Optimization An optimization is a convex optimization problem only if the objective function is a convex function and the range of variables is a convex set/domain.\nThe advantage of a convex optimization is that perhaps there are many local mimic points (where the gradient is equal to 0) but all of them are the best minimal point, which makes it easier to find a solution to an optimization problem.\n2.1 Convex Set/Domain Then what is a convex set/domain? Definition: if there is a set of vectors called $C$ and the length of each vector is $n$, and for $\\forall x,y \\in C$, and one real number $\\theta$ falls in the range of [0,1] (e.g., $0 \\leq \\theta \\leq 1 $), the linear combination of $x$ and $y$: $$\\theta x + (1-\\theta) y \\in C$$ then we call $C$ is a convex set/domain(see figure below).\n   Fig 1. A simple illustration of a convex set/domain.   A few sets we\u0026rsquo;ve already known are convex sets. For example, a real number set $\\mathbb{R}$. It\u0026rsquo;s obvious that $\\forall x,y \\in \\mathbb{R}$, and $\\forall \\theta \\in \\mathbb{R}$ and $0 \\leq \\theta \\leq 1$: $$\\theta x + (1-\\theta) y \\in \\mathbb{R}$$ An affine subspace $\\mathbb{R}^n$. Given a $n \\times m$ matrix $A$ and a vector $b$ of length $m$, then an affine subspace is such a set consisting of the following elements:\n$$\\{x \\in \\mathbb{R}^n: Ax=b\\}$$ Now we give the proof that an affine subspace is a convex set/domain. For $\\forall x,y \\in \\mathbb{R}^n$, and $$Ax=b, \\ Ay=b$$ then $\\forall \\theta$ and $0 \\leq \\theta \\leq 1$: $$A(\\theta x + (1-\\theta)y) = A\\theta x + A (1-\\theta)y = \\theta Ax + (1-\\theta) Ay = \\theta b + (1-\\theta)b = b$$ thus, an affine subspace $\\mathbb{R}^n$ is a convex set/domain.\nA polyhedron space $\\mathbb{R}^n$. Similar to an affine space, given a $n \\times m$ matrix $A$ and a vector $b$ of length $m$, then a polyhedron space is such a set consisting of the following elements: $$\\{x \\in \\mathbb{R}^n: Ax \\leq b\\}$$ Now we give the proof that a polyhedron space is a convex set/domain. For $\\forall x,y \\in \\mathbb{R}^n$, and $$Ax \\leq b, \\ Ay \\leq b$$ then $\\forall \\theta$ and $0 \\leq \\theta \\leq 1$: $$A(\\theta x + (1-\\theta)y) = A\\theta x + A (1-\\theta)y = \\theta Ax + (1-\\theta) Ay \\leq \\theta b + (1-\\theta)b = b$$ thus, a polyhedron space $\\mathbb{R}^n$ is a convex set/domain.\nThe interaction of convex sets/domains is still a convex set. For an optimization problem, if all the variable sets/domains of constrains for the objective function are convex sets/domains, then the variable set satisfying all the constrains is still a convex set/domain.\n2.2 Convex Function Then what is a convex function? Definition: in the domain of function, $\\forall x,y$ and $0 \\leq \\theta \\leq 1$, they always satisfy the following equation: $$f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta) f(y)$$ then function $f(x)$ is a convex function (see figure below).\n   Fig 2. A simple illustration of a convex function.   How to determine whether a function is a convex function? For a function $f(x)$ with only one variable, if its second-derivative is equal or greater than 0, then this function is a convex function. For a function $f(x_1,x_2,\u0026hellip;,x_n)$ with multiple variables, if the Hessian Matrix is a positive semi-definite matrix, then the function is a convex function.\nFor a function $f(x_1,x_2,\u0026hellip;,x_n)$ contains multiple variables, its Hessian matrix is the partial derivative matrix:    Since $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}$ where $i,j \\in $ {$1,2,\u0026hellip;,n$}, then the Hessian matrix is symmetric matrix.\nNext, if there is a point $M$=( $x_1^{'}$, $x_2^{'}$,\u0026hellip;,$x_n^{'}$) satisfies $\\lim_{x_i \\to x_i^{'}} \\frac{\\partial f}{\\partial x_i}=0$ where $i \\in $ {$1,2,\u0026hellip;,n$}, and if :\n the Hessian Matrix is positive semi-definite or positive definite, then the value of $f(x_1^{'},x_2^{'},\u0026hellip;,x_n^{'})$ is the global minimal value, and $M$ is the global mimic point. the Hessian Matrix is negative semi-definite or negative definite, then the value of $f(x_1^{'},x_2^{'},\u0026hellip;,x_n^{'})$ is the global maximal value, and $M$ is the global maximal point. the Hessian Matrix is non-definite, then $M$ is a Saddle point of function $f(x_1,x_2,\u0026hellip;,x_n)$.  2.3 Sub-level Set of a Convex Function Given a convex function $f(x)$, and a real number $\\alpha \\in \\mathbb{R}$, a sub-level set $S$ is defined as the set of variable $x$ that satisfies the values of $f(x)$ is equal or lower than $\\alpha$:\n$$\\{ x \\in S: f(x) \\leq \\alpha \\}$$ According to the definition of a polyhedron set above, the sub-level set $S$ is a polyhedron set, which is also a convex set.\n2.4 Convex Optimization In general, a convex optimization is written as: $$\\min_{x \\in C} f(x)$$ where $f(x)$ is the objective convex function, $x$ is the variable and $C$ is a convex set/domain. To prove this optimization to be a convex optimization, we need to prove the objective function is a convex function and its variable set is a convex set. Another general format of a convex optimization is denoted as:\n$$\\min f(x)$$ and the constrains are: $$g_i(x) \\leq 0, \\ i=(1,2,\u0026hellip;,m) \\ and \\ h_i(x)=0, \\ i=(1,2,\u0026hellip;,n)$$ according to the definitions above, if $g_i(x)$ is a convex function, then the variable set of $g_i(x)$ is a polyhedron set which is a convex set. $h_i(x)$ is an affine subspace, which is also a convex set. Then the interaction of these two convex sets is still a convex set.\nThe most important property of a convex optimization is that if we find a point that makes the gradient at this point to be 0, then this point is the global optimal solution. There are perhaps many points in variable set, but if we can find one of them, then we can stop.\n2.5 The convex optimization used in Machine Learning 2.5.1 Linear regression In machine learning, linear regression is a simple supervised learning algorithm. Given feature vectors $x_i$ and its corresponding groundtruth labels $y_i$ where $i =$ {$1,2,\u0026hellip;,N$}, then the linear function can be written as: $$f(x)=w^T x + b$$ and the loss function is denoted as the mean square error between the value $f(x_i)$ and its corresponding label $y_i$: $$L = \\frac{1}{2N} \\sum_{i=0}^{N} (f(x_i) - y_i)^2$$ then we replace $f(x_i)$ with its function and get: $$L = \\frac{1}{2N} \\sum_{i=0}^{N} (w^T x_i + b - y_i)^2$$ if we assume that: $$[w, b] \\rightarrow w, \\ and \\ [x, 1] \\rightarrow x$$ then we get: $$L = \\frac{1}{2N} \\sum_{i=0}^{N} (w^T x_i - y_i)^2$$ and we get: $$L = \\frac{1}{2N} \\sum_{i=0}^{N} ((w^T x_i)^2 - 2 w^T x_i y_i + y_i^2)$$ and the partial derivative (I still do not know how to compute this partial derivative) is: $$\\frac{\\partial^2 L}{\\partial w_i \\partial w_i} = \\frac{1}{N} \\sum_{k=1}^N x_{k,i}x_{k,j}$$ then the Hessian Matrix is:    the Hessian Matrix can be written in matrix-format:\n    where $X$ is a column of vector $x_i$ ($i =$ {$1,2,\u0026hellip;,N$}), we can prove that this Hessian Matrix is a positive semi-definite matrix. For example, $\\forall x \\in \\mathbb{R}^N$ and $x \\neq 0$: $$x^TX^TXx = (Xx)^T Xx \\geq 0$$ thus the loss function of linear regression is a convex function without any constrains, we can find the global optimal solution by SGD or other methods.\n2.5.2 Other ML algorithms Similarly, we can use the same way to prove that the optimization problem of Logistic Regression $f(x)=\\frac{1}{1+e^{-x}}$, Softmax $f(x) = \\frac{e^{x_i}}{\\sum_{j=0}^N e^{x_j}}$ and SVM is also a convex optimization. However, the optimization using Deep Neural Network is not a convex optimization thus people do not understand the mathematic behind it and regard it as a black-box.\nReference: https://zhuanlan.zhihu.com/p/37108430\n","date":1580244601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580941680,"objectID":"d5f84591e91200215c9267c4306b6573","permalink":"https://Alibabade.github.io/post/specific_math/","publishdate":"2020-01-28T20:50:01Z","relpermalink":"/post/specific_math/","section":"post","summary":"Archive for some specific and easy forgotten mathematic.","tags":["Academic","mathematic"],"title":"Specific_math","type":"post"},{"authors":["Li Wang"],"categories":["Software release","Python"],"content":"This is a small prototype that I had been working on when I was a visiting scholar (more like an intern) in Italy. The main purpose is reprogram a sensor receiver software (in python) into an executable file (i.e., EXE) in Windows OS.\nBackground: Two independent programs for data receiving via Bluetooth and visualization via browser respectively. In addition, two programs depend on various softwares and libraries (i.e., google chrome browser, tkinter, matplotlib etc.), which makes it difficult to install.\nGoal: integrate two programs into one executable file without any other software and library dependence.\nMethod: 1. Integrate two programs into one program with multiprocessing; 2. Visualize the sensor data and complete functions (i.e., extract data during usr specific time, export data into csv file, sample rate etc.) on a window created via tkinter and matplotlib; 3. Convert python code into a single executable file via pyinstaller.\n ","date":1579092510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581272175,"objectID":"683f7131fbe1569e44d8329cdd42ca4b","permalink":"https://Alibabade.github.io/project/real_time_recording_sensor_rawdata/","publishdate":"2020-01-15T12:48:30Z","relpermalink":"/project/real_time_recording_sensor_rawdata/","section":"project","summary":"This is a small prototype that I had been working on when I was a visiting scholar (more like an intern) in Italy. The main purpose is reprogram a sensor receiver software (in python) into an executable file (i.e., EXE) in Windows OS.\nBackground: Two independent programs for data receiving via Bluetooth and visualization via browser respectively. In addition, two programs depend on various softwares and libraries (i.e., google chrome browser, tkinter, matplotlib etc.","tags":[],"title":"Real_time_recording_sensor_rawdata","type":"project"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"This project aims to implement a torch version for fast photographic style transfer based on Fast-Neural-Style. The teaser image is a stylized result by the algorithm described in this project, which takes around 1.40 seconds for $852 \\times 480$ resolution image on a single NVIDIA 1080Ti card.\nIn this project, I also provide a torch implementation of the Domain Transform (Recursive Filter) which is described in the paper:\nDomain Transform for Edge-Aware Image and Video Processing Eduardo S. L. Gastal and Manuel M. Oliveira ACM Transactions on Graphics. Volume 30 (2011), Number 4. Proceedings of SIGGRAPH 2011, Article 69.  Introduction Photographic style transfer aims to transfer only the colour information from a given reference image to a source image without detail distortions. However, neural style transfer methods (i.e., Neural-Style and Fast-Neural-Style) usually tend to distort the details of source image to complete artistic transformation (including colours and textures) for reference images. Thus preserving details or structures in source images without affecting colour transformation is the key to photographic style transfer.\nMethod The idea behind this method is purely strightforward, which preserves the artistic style transformation but matches the colour distribution of reference images better to the source image. Fast-Neural-Style (or Neural-Style) tends to keep the structure details of source images on a single high-level conv layer, which distorts these details and transforms the textures (including colours) of reference images to unexpected regions in source images. In experiments, an interesting thing is found that simply restricting the structure details of source images in multiple conv layers (both low-level and high level) is able to suppress texture (of reference image) expression and match better colour distribution on generated images. However, this still causes the detail loss of source images. To address this problem, a post-processing step is introduced to extract the detail information from original source image and transfer it to transformed images. In image processing, an image is composed by its colour and detail information, in math, $I=C+D$ where $C$ and $D$ denotes the colour and detail information, respectively. Thus $D = I - C$ where $C$ is obtained from image smoothing technique like DTRF in this case.\nIn total, the proposed method consists of two steps:1. Fast-Neural-Style with multiple conv layers restriction on detail preservation and a similarity loss; 2, Post-processing Refinement with detail extraction and exchange to transformed image from step 1. The training stage and testing stage are illustrated in below figures:\n    Fig 1. Training Stage.      Fig 2. Testing Stage.   More results Here are more stylized examples by this method:                   Limitations  The source image and reference image should share a similar semantic contents, otherwise the transformation will fail to generate faithful results as we do not apply semantic masks for input images. This method works well for photography images which basically have 1-3 colour tones. To extreme colourful images, this approach usually fails to achieve faithful results.  One more thing The github code is released in here.\n","date":1578951279,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579039915,"objectID":"a38dc01378c1d63c05e728e88abdf09c","permalink":"https://Alibabade.github.io/project/fast_photographic_style_transfer/","publishdate":"2020-01-13T21:34:39Z","relpermalink":"/project/fast_photographic_style_transfer/","section":"project","summary":"This project aims to implement a torch version for fast photographic style transfer based on Fast-Neural-Style. The teaser image is a stylized result by the algorithm described in this project, which takes around 1.40 seconds for $852 \\times 480$ resolution image on a single NVIDIA 1080Ti card.\nIn this project, I also provide a torch implementation of the Domain Transform (Recursive Filter) which is described in the paper:\nDomain Transform for Edge-Aware Image and Video Processing Eduardo S.","tags":["Academic"],"title":"Fast_photographic_style_transfer","type":"project"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"This blog contains four parts:\n Introduction: What is Object Detection? and general thoughts/ideas to deal with Object Detection; Classic Deep Learning based Methods: multi-stage :RCNN and SPP Net , two-stage: Fast RCNN, Faster RCNN, Mask RCNN; Classic One-Stage Methods: YOLOv1-v3, SSD, RetinaNet; More Recent Anchor-Free Object Detection Methods (2018-2019);  1. Introduction What is Object Detection? Given an image, object detection aims to find the categories of objects contained and their corresponding locations (presented as bounding-boxes) in the image. Thus Object Detection contains two tasks: classification and localization.\nGeneral thoughts/ideas to detect objects. The classification has been done by CNNs like AlexNet, VGG and ResNet. Then only localization still needs to be done. There are two intuitive ways: 1. Regression: the location of an object is presented by a vector $(x,y,w,h)$ which are the centre coordinates and width/height of an object bounding-box in the given image. For example, given an image, it only contains one object\u0026mdash;cat. To locate the bounding-box, we apply a CNN to predict the vector $(x_p,y_p,w_p,h_p)$ and learn to regress the predicted vector to be close to groundtruth $(x_t,y_t,w_t,h_t)$ by calculating the L2 loss (see Fig 1.).\n   Fig 1. Regression for localization shown in this blog.   If the initial bounding-box is randomly chosen or there are many objects, then the entire regression will be much difficult and take lots of training time to correct the predicted vector to groundtruth. Sometime, it may not achieve a good convergence. However, if we select approximately initial box coordinates which probably contains the objects, then the regression of these boxes should be much easier and faster as these initial boxes already have closer coordinates to groundtruth than random ones. Thus, we could divide the problem into Box Selection and Regression, which are called Region Proposal Selection and Bounding-box Regression(please go post Basic_understanding_dl if you do not know Bounding-box regression) in Object Detection, respectively. Based on this, the early Object Detection methods contain multi-stage tasks like: Region Proposal Selection, Classification and Bounding-box Regression. In this blog, we only focus on the DL-based techniques, thus We do not review any pre-DL methods here.\nThere are a few candidate Region Proposal Selection methods (shown in below), and some of them are able to select fewer proposals (nearly hundreds or thousands) and keep high recall.    Fig 2. Comparisons between different Region Proposal Selection methods shown in this blog.   2. Classic Deep Learning based Methods Since using Region Proposal Selection can reduce bounding-box candidates from almost infinite to ~2k for one image with multiple objects, Ross et al. 2014 propose the first CNN-based Object Detection method, which uses CNN to extract features of images, classifies the categories and regress bounding-box based on the CNN features.\n2.1 R-CNN (Region CNN) The basic procedure of R-CNN model:\n Use Selective Search to select ~2k Region Proposals for one image. Warp all the Region Proposals into a same size as the fully connection layers in their backbone neural network (i.e., AlexNet) has image size limitation. For example, the FC layers only take 21x21xC feature vector as input, then all the input image size has to be 227x227 if all the Conv + BN + relu layers of a pre-trained AlexNet are preserved. Feed the Region Proposals into the pre-trained AlexNet at each proposal per time rate, and extract the CNN features from FC7 layer for further classification (i.e., SVM). The extracted CNN features will also be used for Bounding-box Regression.  Based on the procedure above, there are twice fine-tuning:\n Fine-tune the pre-trained CNN for classification. For example, the pre-trained CNN (i.e., AlexNet) may have 1000 categories, but we may only need it to classify ~20 categories, thus we need to fine-tune the neural network. Fine-tune the pre-trained CNN for bounding-box regression. For example, we add a regression head behind the FC7 layer, and we need to fine-tune the network for bounding-box regression task.  2.1.1 Some common tricks used 1. Non-Maximum Suppression\nCommonly, sometimes the RCNN model outputs multiple bounding-boxes to localize the same object in the given image. To choose the best matching one, we use non-maximum suppression technique to avoid repeated detection of the same instance. For example, we have a set $B$ of candidate boxes, and a set $S$ of corresponding scores, then we choose the best box by following steps: 1. sort all the boxes with the scores, and remove the box $M$ with highest score from $B$, and add to set $D$; 2. check any box $b_i$ left in $B$, if the IoU of $b_i$ and $M$, remove $b_i$ from $B$; 3. repeat 1-2 until $B$ is empty. The box in $D$ is what we want.    Fig 3. Non-maximum suppression used in RCNN this blog.   2. Hard Negative Mining\nBounding-box containing no objects (i.e., cat or dog) are considered as negative samples. However, not all of them are equally hard to be identified. For example, some samples purely holding background are \u0026ldquo;easily negative\u0026rdquo; as they are easily distinguished. However, some negative samples may hold other textures or part of objects which makes it more difficult to identify. These samples are likely \u0026ldquo;Hard Negative\u0026rdquo;.\nThese \u0026ldquo;Hard Negative\u0026rdquo; are difficult to be correctly classified. What we can do about it is to find explicitly those false positive samples during training loops and add them into the training data in order to improve the classifier.\n2.1.2 Problems of RCNN RCNN extracts CNN features for each region proposal by feeding each of them into CNN once at a time, and the proposals selected by Selective Search are approximately 2k for each image, thus this process consumes much time. Adding pre-processing Selective Search, RCNN needs ~47 second per image.\n2.2 SPP Net (Spatial Pyramid Pooling Network) To speedup RCNN, SPPNet focuses on how to fix the problem that each proposal is fed into the CNN once a time. The reason behind the problem is the fully connected layers need fixed feature size (i.e., 1 x 21 x 256 in He et al.,2014) for further classification and regression. Thus SPPNet comes up with an idea that an additional pooling layer called spatial pyramid pooling is inserted right after the last Conv layer and before the Fc layers. The operation of this pooling first projects the region proposals to the Conv features, then divides each feature map (i.e., 60 x 40 x 256 filters) from the last Conv layer into 3 patch scales (i.e., 1,4 and 16 patches, see Fig 4. For example, the patch size is: 60x40 for 1 patch, 30x20 for 4 patches and 15x10 for 16 patches, next operates max pooling on each scaled patch to obtain a 1 x 21(1+4+16) for each feature map, thus we get 1x21x256 fiexd vector for Fc layers.\n   Fig 4. The spatial pyramid pooling layer in SPPNet.   By proposing spatial pyramid pooling layer, SPPNet is able to reuse the feature maps extracted from CNN by passing the image once through because all information that region proposals need is shared in these feature maps. The only thing we could do next is project the region proposals selected by Selective Search onto these feature maps (How to project Region Proposals to feature maps? Please go to basic_understanding post for ROI pooling.). This operation extremely saves time consumption compared to extract feature maps per proposal per forward (like RCNN does). The total speedup of SPPNet is about 100 times compared to RCNN.\n2.3 Fast RCNN    Fig 6. The pipeline of Fast RCNN in this blog.   Fast RCNN attempts to overcome three notable drawbacks of RCNN:\n Training a multi-stage pipeline: fine-tune a ConvNet based on Region Proposals; train SVM classifiers with Conv Features; train bounding-box regressors. Training is expensive in space and time: 2.5 GPU-days for 5k images and hundreds of gigabytes of storage. Speed is slow: ~47 second per image even on GPU.  Solutions:\n Combine both classification (replace SVM with softmax) and bounding-box regression into one network with multi-task loss. Introduce ROI pooling for: 1. reuse Conv feature maps of one image; 2. speedup both training and testing. Using VGG16 as backbone network, ROI (Region of Interest) pooling converts all different sizes of region proposals into 7x7x512 feature vector fed into Fc layers. Please go to post basic_understanding_dl for more details about ROI pooling.     Fig 6. Speed comparison between RCNN and Fast RCNN in this blog.   Multi-task Loss for Classification and Bounding-box Regression $\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;$\nSymbol Explanation\n$u$ Groundtruth class label, $u \\in 0,1,\u0026hellip;,K$; To simplify, all background class has $u=0$.\n$v$ Groundtruth bounding-box regression target, $v=(v_x,v_y,v_w,v_h)$.\n$p$ Descrete probability distribtion (per RoI), $p=(p_0,p_1,\u0026hellip;,p_K)$ over $K+1$ categories. $p$ is computed by a softmax over the $K+1$ outputs of a fully connected layer.\n$t^u$ Predicted bounding-box vector, $t^u=(t_x^u,t_y^u,t_w^u,t_h^u)$.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;$\nThe multi-task loss on each RoI is defined as:\n$$L(p,u,t^u,v) = L_{cls}(p,u) + \\lambda[u \\geqslant 1]L_{loc}(t^u,v)$$ where $L_{cls}(p,u)$=-log$p_u$ is a log loss for groundtruth class $u$. The Iverson bracket indicator function $[u \\geqslant 1]$ is 1 when $u \\geqslant 1$ (the predicted class is not background), and is 0 otherwise. The $L_{loc}$ term is using smooth L1 loss, which is denoted as: $$L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}}smooth_{L_1}(t_i^u-v_i)$$ and \\begin{equation} smooth_{L_1}(x) = \\begin{cases} 0.5x^2, \\ |x| \u0026lt; 1 \\newline |x|-0.5, \\ otherwise \\end{cases} \\end{equation} $smooth_{L_1}(x)$ is a robust $L_1$ loss that is less sensitive to outliers than $L_2$ loss.\n2.4 Faster RCNN    Fig 7. The pipeline of Faster RCNN in this blog.   Faster RCNN focuses on solving the speed bottleneck of Region Proposal Selection as previous RCNN and Fast RCNN separately compute the region proposal by Selective Search on CPU which still consumes much time. To address this problem, a novel subnetwork called RPN (Region Proposal Network) is proposed to combine Region Proposal Selection into ConvNet along with Softmax classifiers and Bounding-box regressors.\n   Fig 8. The pipeline of RPN in the paper.   To adapt the multi-scale scheme of region proposals, the RPN introduces an anchor box. Specifically, RPN has a classifier and a regressor. The classifier is to predict the probability of a proposal holding an object, and the regressor is to correct the proposal coordinates. Anchor is the centre point of the sliding window. For any image, scale and aspect-ratio are two import factors, where scale is the image size and aspect-ratio is width/height. Ren et al., 2015 introduce 9 kinds of anchors, which are scales (1,2,3) and aspect-ratio(1:1,1:2,2:1). Then for the whole image, the number of anchors is $W \\times H \\times 9$ where $W$ and $H$ are width and height, respectively.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-$\nSymbol Explanation\n$i$ the index of an anchor in a mini-batch.\n$p_i$ the probability that the anchor $i$ being an object.\n$p_i^{*}$ the groundtruth label $p_i^{*}$ is 1 if the anchor is positive, and is 0 if the anchor is negative.\n$t_i$ a vector $(x,y,w,h)$ representing the coordinates of predicted bounding box.\n$t_i^{*}$ that of the groundtruth box associated with a positive anchor.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-$\nThe RPN also has a multi-task loss just like in Fast RCNN, which is defined as:\n$$L({p_i},{t_i}) = \\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{*}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{*} L_{reg}(t_i, t_i^{*})$$ where the classification $L_{cls}$ is log loss over two classes (object vs not object). The regression loss $L_{reg}(p_i, p_i^{*}) = smooth_{L1}(t_i - t_i^{*})$. The term $p_i^{*} L_{reg}(t_i, t_i^{*})$ means the regression loss is activated if $p_i^{*}=1$ and is disabled if $p_i^{*}=0$. These two loss term are normalized by $N_{cls}$ and $N_{reg}$ and weighted by a balancing parameter $\\lambda$. In implementation, $N_{cls}$ is the number of images in a mini-batch (i.e., $N_{cls}=256$), and the $reg$ term is normalized by the number of anchor locations (i.e., $N_{reg} \\sim 2,400$). By default, the $\\lambda$ is set to 10.\nTherefore, there are four loss functions in one neural network:\n one is for classifying whether an anchor contains an object or not (anchor good or bad in RPN); one is for proposal bounding box regression (anchor -\u0026gt; groundtruth proposal in RPN); one is for classifying which category that the object belongs to (over all classes in main network); one is for bounding box regression (proposal -\u0026gt; groundtruth bounding-box in main network);  The total speedup comparison between RCNN, Fast RCNN and Faster RCNN is shown below:    Fig 9. The speedup comparison between RCNN, Fast RCNN and Faster RCNN in this blog.   2.5 Mask RCNN    Fig 10. The pipeline of Mask RCNN, which is Faster RCNN + Instance Segmentation + improved RoIAlign Pooling.   Mask RCNN has three branches: RPN for region proposal + (a pretrained CNN + Headers for classification and bounding-box regression) + Mask Network for pixel-level instance segmentation. Mask RCNN is developed on Faster RCNN and adds RoIAlign Pooling and instance segmentation to output object masks in a pixel-to-pixel manner. The RoIAlign is proposed to improve RoI for pixel-level segmentation as it requires much more fine-grained alignment than Bounding-boxes. The accurate computation of RoIAlign is described in RoIAlign Pooling for Object Detection in Basic_understanding_dl post.\n   Fig 11. Mask RCNN results on the COCO test set. Image source: Mask RCNN paper   Mask Loss During the training, a multi-task loss on each sampled RoI is defined as : $L=L_{cls} + L_{bbox}+L_{mask}$. The $L_{cls}$ and $L_{bbox}$ are identical as those defined in Faster RCNN.\nThe mask branch has a $K\\times m^2$ dimensional output for each RoI, which is $K$ binary masks of resolution $m \\times m$, one for each the $K$ classes. Since the mask branch learns a mask for every class with a per-pixel sigmoid and a binary cross-entropy loss, there is no competition among classes for generating masks. Previous semantic segmentation methods (e.g., FCN for semantic segmentation) use a softmax and a multinomial cross-entropy loss, which causes classification competition among classes.\n$L_{mask}$ is defined as the **average binary mask loss**, which **only includes $k$-th class** if the region is associated with the groundtruth class $k$: $$L_{mask} = -\\frac{1}{m^2} \\sum_{1 \\leqslant i,j \\leqslant m} (y_{ij}log\\hat{y}_{ij}^k +(1-y_{ij})log(1-\\hat{y}_{ij}^k))$$ where $y_{ij}$ is the label (0 or 1) for a cell $(i,j)$ in the groundtruth mask for the region of size $m \\times m$, $\\hat{y}_{ij}$ is the predicted value in the same cell in the predicted mask learned by the groundtruth class $k$.\n2.6 Summary for R-CNN based Object Detection Methods    Fig 12. Summary for R-CNN based Object Detection Methods . Image source: this blog   3. Classic One-Stage Methods 3.1 YOLO (You Only Look Once) Introduction. YOLO is the first approach removing region proposal and learns an object detector in an end-to-end manner. Due to no region proposal, it frames object detection as a total regression problem which spatially separates bounding boxes and associated class probabilities. The proposed YOLO performs extremely fast (around 45 FPS), but less accuracy than main approaches like Faster RCNN.\n   Fig 13. YOLO pipeline. Image source: original paper.   3.1.1 Pipeline   Resize input image from 224x224 to 448x448;\n  Pre-train a single CNN (DarkNet similar to GoogLeNet: 24 conv layer + 2 fc) on ImageNet for classification.\n  Split the input image into $S \\times S$ grid, for each cell in the grids:\n3.1. predict coordinates of B boxes, for each box coordinates: $(x,y,w,h)$ where $x$ and $y$ are the centre location of box, $w$ and $h$ are the width and height of box.\n3.2. predict a confidence score, $C = Pr(obj) \\times IoU(trurh, pred)$ where $Pr(obj)$ denote whether the cell contains an object, $Pr(obj)=1$ if it contains an object, otherwise $Pr(obj)=0$. $IoU(truth, pred)$ is the interaction under union.\n3.3. predict a probability for every class, $p(c_i)$ where $i$ $\\in$ {$1,2,\u0026hellip;,K$} if a cell contains an object. During this stage, each cell only predicts one set of class probabilities regardless of the number of predicted bounding boxes $B$.\n  Output a $S \\times S \\times (5B + K)$ shape tensor after the last FC layer in total, then compute the loss.\n  In inference time, the network maybe outputs multiple candidate bounding boxes for one same object, then it uses non-maximum suppression to preserve the best match box.\n  3.1.2 Loss functions $\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-$\nSymbol Explanation\n$\\mathbb{1}_{ij}^{obj}$: an indicator function. It\u0026rsquo;s 1 when there is an object contained in the $j$-th predicted box of the $i$-th cell and $j$-th predicted box has the largest overlap region with the groundtruth box, otherwise it\u0026rsquo;s 0.\n{$x_{ij}^p, y_{ij}^p, w_{ij}^p, h_{ij}^p$}: the centre coordinates and (width, height) of the predicted $j$-th bounding box in $i$-th cell.\n{$x_{ij}^t, y_{ij}^t, w_{ij}^t, h_{ij}^t$}: the centre coordinates and (width, height) of the groundtruth $j$-th bounding box in $i$-th cell.\n$C_{ij}^p$: the predicted confidence score for the $j$-th bounding box in $i$-th cell.\n$C_{ij}^t$: the groundtruth confidence score for the $j$-th bounding box in $i$-th cell.\n$p_i^{p}(c)$: the predicted class probability for $i$-th class category.\n$p_i^{t}(c)$: the groundtruth class probability for $i$-th class category.\n$\\lambda_{coord}$: a weight parameter for coordinate loss. The default value is 5.\n$\\lambda_{noobj}$: a weight parameter for confidence score loss. The default value is 5.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-$\nThe loss functions basically consists of three parts: coordinates $(x,y,w,h)$, confidence score $C$ and class probabilities $p(c_i)$, $i \\in$ {$1,\u0026hellip;,K$}. The total loss is denoted as: $$\\begin{eqnarray} L_{total} \u0026amp;=\u0026amp; L_{loc} + L_{cls} \\\\\\\n\u0026amp;=\u0026amp; \\lambda_{coor} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} ((x_{ij}^p - x_{ij}^t)^2 + (y_{ij}^p - y_{ij}^t)^2 + (\\sqrt{w_{ij}^p} - \\sqrt{w_{ij}^t})^2 + (\\sqrt{h_{ij}^p} - \\sqrt{h_{ij}^t})^2) \\\\\\\n\u0026amp;+\u0026amp; \\sum_{i=0}^{S^2} \\sum_{ij}^{B} (\\mathbb{1}_{ij}^{obj} + \\lambda_{noobj} (1 - \\mathbb{1}_{ij}^{obj})) (C_{ij}^p - C_{ij}^t)^2 \\\\\\\n\u0026amp;+\u0026amp; \\sum_{i=0}^{S^2} \\mathbb{1}_{ij}^{obj} \\sum_{c\\in classes} (p_i^p(c) - p_i^t(c))^2 \\end{eqnarray}$$\n3.1.3 Differences ( or insights)  remove region proposal and complete the object detection task in an end-to-end manner. the first approach achieves real-time speed. the coordinate loss uses $(x,y,w,h)$ to represent bounding box, which is different from R-CNN based methods. This is because YOLO does not pre-define bounding boxes (i.e., region proposals or anchor boxes), thus YOLO can not use offset of coordinates to compute the loss or train the neural network.  3.1.4 Limitations  Less accurate prediction for irregular shapes of object due to a limited box candidates. Less accurate prediction for small objects.  3.2 SSD (single shot multibox detector) Introduction. SSD is one of early approaches attempts to detect multi-scale objects based on pyramid conv feature maps. It adopts the pre-defined anchor box idea but applies it on multiple scales of conv feature maps, which achieves real-time application via removing region proposal and high detection accuracy (even higher than Faster RCNN) via multi-scale object detection as well, e.g., it is capable of detecting both large objects and small objects in one image which increases the mAP.\n   Fig 14. SSD pipeline recreated based on original paper.   3.2.1 Pipeline  Modify pre-trained VGG16 with replaced conv6-7 layers and extra multi-scale conv features. To detect multiple scales of input objects, a few (4 in this case) extra sizes of conv features are added into base model (see light green color in Fig14). Several default anchor boxes with various scale and ratio (width/height) are introduced for each cell in all feature maps. For each of $m$ level conv feature maps, we compute the scale $s_k$, aspect ratio $a_r$, width $w_k^a$, height $h_k^a$ and centre location ($x_k^a, y_k^a$) of default boxes as:   scale: $$s_k = s_{min} + \\frac{s_{max} -s_{min}}{m-1}(k-1), k \\in [1,m], s_{min}=0.2, s_{max}=0.9$$ aspect ratio: $a_r \\in$ {1,2,3,$\\frac{1}{2}, \\frac{1}{3}$}, additional ratio $s_k^{'}=\\sqrt{s_k s_{k+1}}$, 6 default boxes in total. width: $w_k^a=s_k \\sqrt{a_r}$ height: $h_k^a= s_k / \\sqrt{a_r}$ centre location ($x_k^a, y_k^a$):$(\\frac{i+0.5}{|f_k|}, \\frac{j+0.5}{|f_k|})$ where $|f_k|$ is the size of the $k$-th square feture map, $i,j \\in [0, |f_k|]$ .  where $s_{min}$ is 0.2 and $s_{max}$ is 0.9, which means the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9. Therefore, for each input object, there are $\\sum_{i=0}^m C_i \\times 6$ anchor boxes where $C_i$ is the channel of $i$-th level feature maps. And for all the multiple level feature maps, there are $\\sum_{i=0}^{m} C_i H_i W_i \\times 6$ anchor boxes in total where $H_i$ and $W_i$ are the height and width of $i$-th level feature maps.\n   Fig 15. Matching strategy of anchor boxes during training. Image source: original paper.   Advantage of pre-defined anchor boxes in SSD (matching strategy). During training, we first match each groundtruth box to the default box with highest jaccard overlap, then match default boxes to any groundtruth boxes with jaccard overlap higher than a threshold (0.5). This enables SSD to predict multiple high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap. Thus the network learns match suitable scale of default boxes to groundtruth box. For example, in Fig 15, the network learns from training that anchor boxes of dog on higher layer $4 \\times 4$ are matched to groundtruth as the scale of anchor boxes on one $8 \\times 8$ feature map are too small to cover the large size of dog.\n Hard negative mining. During training, the number of input objects (or labeled anchor boxes) is quite smaller compared to the total number of default anchor boxes, thus most of them are negative samples. This introduces a significant imbalance between negative and positive training examples. The authors of SSD narrow down negative samples by choosing default boxes of top confidence loss, which makes sure the ratio between negative and positive samples at most 3:1.\n  Data augmentation (this contributes most improvement). To make the detector more robust to various input object sizes, SSD introduces a data augmentation which choose training samples by three following options:\n   use the original input image sample a patch of original input image, whose IoU with its corresponding groundtruth box is 0.1,0.3,0.5,0.7 or 0.9. randomly sample a patch of the original input image.  The size of sampled patch is [0.1,1] of the original input image and its aspect ratio is between $\\frac{1}{2}$ and 2. The overlapped region of the groundtruth box is kept if the centre of it is in the sampled patch. After the sampling step above, all the sampled patches are resized to fixed size and is horizontally flipped with probabilitiy of 0.5.\nCompute the loss functions. Non-maximum suppression to find the best match predicted boxes.  3.2.2 Loss functions The training objective is the weighted combination of a localization loss and a classification loss: $$L = \\frac{1}{N}(L_{loc} + \\alpha L_{cls})$$ where $N$ is the number of matched boxes and $\\alpha$ is picked by cross validation.\nThe localization loss is the smooth L1 between the predict offset of default boxes and those of matched groundtruth boxes, which is as same as the bounding box regression in RCNN:\n$$L_{loc} = \\sum_{i=0}^N \\sum_{j\\in(cx,cy,w,h)} \\mathbb{1}_{ij}^k smooth_{L1}(\\Delta t_{j}^i - \\Delta p_{j}^i)$$ $$\\Delta t_{cx}^i = (g_{cx}^i - p_{cx}^i) / p_w^i, \\Delta t_{cy}^i = (g_{cy}^i - p_{cy}^i) / p_h^i,$$ $$\\Delta t_{w}^i = log(\\frac{g_{w}^i}{p_{w}^i}), \\Delta t_{h}^i = log(\\frac{g_{h}^i}{p_{h}^i}),$$ where $\\Delta t_{j}^i$ is the offset of groundtruth boxes, and $\\Delta p_{j}^i$ is the offset of predicted boxes. $\\mathbb{1}_{ij}^k$ is an indicator for matching $i$-th default box to the $j$-th ground truth box of category $k$.\nThe classification loss is the softmax loss over multiple classes confidences ($c$) using cross entropy loss: $$L_{cls} = - \\sum_{i=0}^N \\mathbb{1}_{ij}^k log(\\hat{c}_i^k) - \\sum_{j=0}^M log(\\hat{c}_j^0), \\hat{c}_i^k = softmax(c_i^k) $$ where $N$ and $M$ indicates the positive and negative samples, $c_{i}^k$ is the predicted class probability for $k$-th object class, and $c_i^0$ is the predicted negative probability for non-object class (or background class).\n3.2.3 Differences (or insights)  Multi-scale object detection via extra multiple scales of conv feature maps and matching strategy between default anchor boxes and ground truth boxes. Training tricks: hard negative mining and data augmentation which increases the mAP most.  3.2.4 Limitations Some posts (e.g., this blog and this blog) point out that matching strategy may not help to improve the prediction accuracy for smaller object as it basically only depends on lower layers with high resolution feature maps. These lower layers contain less information for classification. (well, I have not done further experiments to prove this).\n3.3 YOLOv2/YOLO9000 YOLOv2 is an improvement version of YOLOv1 with several fine-tuning tricks (including adding anchor boxes, multi-scale training etc. see next section), and YOLO9000 is built on top of YOLOv2 but with a joint training strategy of COCO detection dataset and 9000 classes from ImageNet. The enhanced YOLOv2 achieves higher detection accuracy (including bounding box prediction and classification) and even more faster speed (480x480,59FPS) than SSD.\n3.3.1 Tricks in YOLOv2 Batch Normalization. YOLOv2 adds BN after each convolutional layer and it helps to fast convergence, and increases the mAP about 2.4%.\nHigh Resolution Classifier. YOLOv1 fine-tunes a pre-trained model with 448x448 resolution image from detection dataset (e.g., COCO). However, the pre-trained model is trained with 224x224 resolution images, which means directly fine-tuning this pre-trained model with higher resolution will not extract features with powerful expression of images. To address this problem, YOLOv2 first trains the pre-trained model with 448x448 resolution images for classification task, then trains the model with high resolution images for detection task. The high resolution is multiple of 32 as its network has 32 stride.\nConvolutional Anchor Boxes. Instead of using 2 fc layers to regress the location of bounding boxes, inspired by RPN with anchor boxes, YOLOv2 uses convolutional layers and anchor boxes to predict bounding boxes and confidence scores. Each anchor box has a predicted $K$ class probability, thus the spatial location of anchor boxes and classification is decoupled. By adding anchor boxes, the mAP of YOLOv2 decreases a bit but it increases recall from 81% to 88%.\nDimension Clusters. Unlike the sizes of anchor box in Faster RCNN are hand-made, YOLOv2 chooses sizes of anchor box better suit to groundtruth bounding boxes. To find more suitable sizes, YOLOv2 uses k-means to cluster groundtruth bounding boxes and choose the sizes of anchor boxes more close to the centroid of each cluster by the following distance metric: $$d(box, centroid) = 1 - IoU(box, centroid)$$ and the best number of centroid $k$ can be chosen by the elbow method.\nDirect location prediction. In Faster RCNN, the offset of an anchor box is predicted by the detector, and it is presented by ($\\Delta x,\\Delta y, \\Delta w,\\Delta h$). Then the predicted centre location of a bounding box is: $$x_p=x_a+(\\Delta x \\times w_a), y_p=y_a+(\\Delta y \\times h_a)$$ where $x_a$ and $y_a$ are centre location of an anchor box, $w_a$ and $h_a$ are the width and height. The centre location of a predict bounding box can be anywhere in a feature map, for example, if $\\Delta x=1$, then the predicted $x_p$ will more a width distance horizontally from $x_a$. This is not good to locate the bounding boxes and could make training unstable. Therefore, YOLOv2 decides to predict the offset to the top-left corner ($c_x,c_y$) of a grid which the anchor box locates at. The scale of a grid is default 1. Then the location ($b_x,b_y,b_w,b_h$) of predicted bounding box is formulated as: $$b_x = (\\sigma(\\Delta x) \\times 1) + c_x, b_y = (\\sigma(\\Delta y) \\times 1) + c_y$$ $$b_w=a_w e^{\\Delta w}, b_h=a_w e^{\\Delta h}$$ where $\\sigma(\\cdot)=sigmoid(\\cdot)$, $a_w$ and $a_h$ are width and height of an anchor box, and the width and height of the grid is set default 1. In this way, the movement of $b_x$ and $b_y$ is constrained in the grid as their maximum move distance is $\\sigma(\\cdot) \\times 1 = 1$. Combining dimension clustering and direct location prediction increases mAP by 5%. The below figure illustrates the process:\n   Fig 16. Illustration of direct location prediction. Image source recreated on original paper.   Add fine-grained feature via passthrough layer. Inspired by ResNet, YOLOv2 also designs a passthrough layer to bring the fine-grained features from an earlier layer to the last output layer. This increases the mAP about 1%.\nMulti-scale Training. To be robust to various input sizes, YOLOv2 inserts a new size of randomly sampled input images every 10 batches. The new sizes are multiple of 32 as its stride is 32.\nLight-weighted base model. YOLOv2 use DarkNet-19 as base model which has 19 conv layers and 5 maxpooling layers. The key point is to add global avg pooling and 1x1 conv layers between 3x3 conv layers. This does not increases significant mAP but decreases the computation by about 33%.\n3.3.2 YOLO9000: Joint Training of Detection and Classification Since drawing bounding boxes in images for detection is much more expensive than tagging image for classification, the paper proposes a joint training strategy which combines small detection dataset and large classification dataset, and extand the detection from around 100 categories in YOLOv1 to 9000 categories. The name of YOLO9000 comes from the top 9000 classes of ImageNet. If one input image is from classification dataset, then the network only back propagates the classification loss during training.\nThe small detection dataset basically has the coarse labels (e.g., cat, person), while the large classification dataset may contain much more detailed labels (e.g., persian cat). Without mutual exclusiveness, this does not make sense to apply softmax to predict all over the classes. Thus YOLO9000 proposes a WordTree to combine the class labels into one hierarchical tree structure with reference to WordNet. For example, the root node of the tree is a physical object, then next level is coarse-grained labels like animal and artifact, then next level is more detailed labels like cat, dog, vehicle and equipment. Thus, physical object is the parant node of animal and artifact, and animal is the parent node of cat and dog. The labels on the same level should be classified by softmax as they are mutual exlusive.\n   Fig 17. Word tree in YOLO9000. Image source: original paper.   To predict the probability of a class node, we follow the path one node to the root, the searching stops when the probability is over a threshold. For example, the probability of a class label persian cat is: \\begin{eqnarray} Pr(perisan \\ cat \\ | \\ contain \\ a \\ physical \\ object) \u0026amp;=\u0026amp; Pr(persian \\ cat \\ | \\ cat) \\\\\\\n\u0026amp;\\times\u0026amp; Pr(cat \\ | \\ animal) \\\\\\\n\u0026amp;\\times\u0026amp; Pr(animal \\ | \\ physical \\ object) \\end{eqnarray} where $Pr(animal \\ | \\ physical \\ object)$ is the confidence score, predicted separately from the bounding box detection.\n3.3.3 Differences (or insights)  Dimension clustering and direct location prediction gives the most contribution of increasing mAP. Word Tree is a creative thing in YOLO9000.  3.3.4 Limitations ( or unsolved problems) During training, the significant imbalance number between positive anchor boxes containing objects and negative boxes containing background still hinders further improvement of detection accuracy.\nInsight Questions for Object Detection Why does the measurement of object detection only use mAP but no classification measurement? Honestly, I don\u0026rsquo;t know.\nWhy does improving the classification (binary classification: contains objects of interests or not) increase the detection accuracy? Well, I think that better classification will reduce negative samples (including easy and hard negative examples), then the network focuses on learning more positive samples to predict bounding boxes, which increases the mAP.\n4. RetinaNet RetinaNet is an one-stage object detector, which proposes two critical contributions: 1. focal loss for addressing class imbalance between foreground containing objects of interest and background containing no object; 2. FPN + ResNet as backbone network for detecting objects at different scales.\n4.1 Focal Loss The extreme class imbalance between training examples is one critical issue for object detection. To address the problem, a focal loss is designed to increase weights for hard yet easily misclassified examples (e.g., background with noisy texture or partial object) and down-weight easy classified examples (e.g., background obviously contains no objects and foreground obviously contains object of interests).\nStarting with normal Cross Entropy loss for binary classification:\n$$CE(p,y) = -ylog(p) - (1-y)log(1-p)$$ where $y \\in$ {0,1} is a groundtruth binary label which indicates a bounding box contains an object or not. $p \\in$ [0,1] is the predicted probability of a bounding box containing an object (aka confidence score).\nFor notational convenience, let $p_t$:\n\\begin{equation} p_t = \\begin{cases} p, \\ \u0026amp;if \\ y=1 \\newline (1-p), \\ \u0026amp;otherwise, \\end{cases} \\end{equation}\nthen $$CE(p,y) \\ = \\ CE(p_t) \\ = \\ -log(p_t)$$\nTo down-weigh the $CE(p_t)$ when $p \\gg 0.5$ (e.g., easily classified examples) and increase weight of loss when $p$ approaching 0 (e.g., hard classified examples), a focal loss is designed by adding a weight $(1-p_t)^\\gamma$ into CE loss, which comes to the form:\n$$FL(p_t) = -(1-p_t)^\\gamma log(p_t)$$ here is the illustration of focal loss, as can be seen, easily classified examples with $p \\gg 0.5$ is decreased and the loss of hard examples increases rapidly when $p$ is more closer to 0.\n   Fig 18. The Focal Loss decreases along with predicted probability with a factor of $(1-p_t)^\\gamma$. Image source: original paper.   In practise, RetinaNet uses an $\\alpha$-balanced variant of the focal loss: $$FL(p_t) = - \\alpha (1-p_t)^\\gamma log(p_t)$$ and there are experiments prove this form slightly improves accuracy over the non-$\\alpha$-balanced form. In addition, implementing the loss layer with sigmoid operation for computing $p$ results in greater numerical stability.\nTo better illustrate the $\\alpha$-balanced FL form, here are a few weighted Focal Loss with various combinations of $\\alpha$ and $\\gamma$:\n   Fig 19. The illustration of various combinations of $\\alpha$ and $\\gamma$ in $\\alpha$-balanced Focal Loss. Image source: LilianWeng\u0026rsquo;s blog.   4.2 FPN + ResNet as Backbone Network Feature Pyramid Network (FPN) proposes that the hierarchic feature pyramids boost the detection accuracy. Thus, RetinaNet exploits this feature pyramid into their backbone network. Here is a brief introduction about the FPN.\nThe key point of feature pyramid network is that multi-scale features in different stages are combined together via bottom-up and top-down pathways. For example, Fig 20, the basic pathways in FPN:\n bottom-up pathway: a forward pass via ResNet and features from different residual blocks (downscale by 2) form the scaled pyramid. top-down pathway: merges the strong semantic features from later coarse layer back to front fine layer by x2 upscale and 1x1 lateral connection and element-wise addition. The upscale operation is using nearest neighbour upsample in RetinaNet. While other upscale methods like deconv may also be suitable. The conv 1x1 lateral connection is to reduce the feature channel. The prediction happens after each top-down stage by a conv 3x3.     Fig 20. The illustration of feature pyramid network. Image source: LilianWeng\u0026rsquo;s blog.   The improvement rank of these introduced components is as follows: 1x1 lateral connnect \u0026gt; detect object across multiple layers \u0026gt; top-down enrichment \u0026gt; pyramid representation (compared to only use single scale image like the finest layer).\n4.3 Model Architecture    Fig 21. The model architecture in RetinaNet: ResNet + FPN + Class subnet (focal loss) + Box subnet. Modified on fig 3 in original paper.   The basic model architecture of RetinaNet is built on top of ResNet by adding FPN and two subnets for classification and box regression. The ResNet has 5 residual blocks which are used to extract feature pyramid. Let $C_i$ denote the output of last conv layer of the $i$-th pyramid level (residual block) and $P_i$ denote the prediction based on $C_i$. As the downscale is 2 used between every two residual blocks, then $C_i$ is $2^i$ downscale lower than the original input image resolution.\nRetinaNet uses prediction $P_s$ to $P_7$ for prediction, where:\n $P_3$ to $P_5$ are computed on features obtained from the element-wise addition of features after 1x1 conv $C_i$ (bottom-up pathway) and x2 upscaled $C_{i+1}$ (top-down pathway). $P_6$ is computed on features after 3x3 stride-2 conv on $C_5$. $P_7$ is computed on features after 3x3 stride-2 conv plus relu on $C_6$.  Prediction on higher level features leads to better detect large objects. In addition, all pyramid features are fixed to channel=256 as they share the same class subnet and box subnet.\nThe anchor boxes are also applied in RetinaNet, and they are set default to 3 scales {$2^0, 2^{\\frac{1}{3}}, 2^{\\frac{1}{2}}$} and 3 aspect ratios {$\\frac{1}{2},1,2$}, thus 9 pre-defined anchor boxes in total. For each anchor box, the model predicts a classification probability for $K$ classes after passing through the class subnet (applying the proposed focal loss), and regresses the offset of the anchor box to the nearest groundtruth box via the box subnet.\n4.3 Insights  Focal loss does a good job to address the imbalance between foreground containing objects of interests and background containing no object. FPN+ResNet will be backbone for further one-stage object detectors.  YOLOv3 YOLOv3 proposes a series of incremental tricks for YOLOv2, and these tricks are inspired by recent researches:\n  logistic regression for objectiveness score (aka confidence score). Unlike using squared error for location loss in YOLOv1-2, YOLOv3 change this loss to a logistic loss to predict the offset of bounding boxes. The yolov3 paper tells that linear regression for predicting offset decreases in mAP.\n  independent logistic classifier instead of softmax for class prediction.\n  FPN works well in YOLOv3. YOLOv3 adopts the FPN to predict boxes at 3 different scales of features.\n  Features extracted from a new net called DarkNet-53. DarkNet-53 adopts the shortcut connections (residual blocks) from ResNet, and it performs similar to ResNet-152 but 2x faster.\n  Well, the authors of YOLO series tried focal loss, but it dropped mAP about 2 points. It maybe has something with no subnets for class and box prediction in YOLOv3, or the parameters $\\lambda_{coord}$ and $\\lambda_{noobj}$ in YOLO have done the job to balance the loss. Not sure yet.\nOverall, YOLOv3 is still a good real-time object detector, it performs less accruacy than RetinaNet-101 (ResNet-101-FPN) but around 3.5~4.0x faster.\nTO BE CONTINUED\u0026hellip;\nReference  https://blog.csdn.net/v_JULY_v/article/details/80170182 https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html https://medium.com/egen/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9 https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review- https://zhuanlan.zhihu.com/p/37998710 https://blog.csdn.net/heiheiya/article/details/81169758 https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html https://zhuanlan.zhihu.com/p/35325884  ","date":1578572488,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580941680,"objectID":"092317b203c8d1e2a34b394edbc30e63","permalink":"https://Alibabade.github.io/post/object_detection/","publishdate":"2020-01-09T12:21:28Z","relpermalink":"/post/object_detection/","section":"post","summary":"Summary of object detection in DL","tags":["Academic"],"title":"Object_detection","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"Fine-tuning neural networks In practise, researchers tend to use pre-trained neural networks on datasets like ImageNet to train their own neural network for new tasks due to their dataset perhaps not big enough (compared to millions of images in ImageNet). Thus this type of operation is called fine-tuning the neural network.\nThere are two typical scenarios:\n use the pre-trained CNNs as feature extractors. For example, we remove the fully connected layers from a pre-trained image classification CNN, then add a classification operator (i.e., softmax and SVM) at the end of left fully convolutional networks to classify images. fine-tune the pre-trained CNNs. For example, we preserve part/all of the layers in a pre-trained CNNs, and retrain it on our own dataset. In this case, the front layers extract low-level features which can be used for many tasks (i.e., object recognition/detection and image segmentation), and the rear layers extract high-level features related to specific classification task, thus we only need fine-tune the rear layers.  How to fine-tune There are normally four different situations:\n New dataset is small and similar to pre-trained dataset. Since the dataset is small, then retrain the CNN may cause overfitting. And the new dataset is similar to pre-trained dataset, thus we hope the high-level features are similar as well. In this case, we could just use the features extracted from pre-trained CNN and train a classification operator like softmax. New dataset is small but not similar to pre-trained datasets. Since the dataset is small then we can not retrain the CNN. And the new dataset is not similar to pre-trained datasets, then we do not use high-level features which means we do not use rear layers. Thus we can just use front layers as feature extractor and training a classification operator like softmax or SVM. New dataset is big and similar to pre-trained datasets. We can fine-tune the entire pre-trained CNN. dataset is big but not similar to pre-trained datasets. We can fine-tune the entire pre-trained CNN.  In practise, a smaller learning-rate is suggested as the weights in network is already smooth and a larger learning-rate may distort the weights of pre-trained CNN.\nCoding in experiments In Pytorch, you can set \u0026ldquo;param.requires_grad = False\u0026rdquo; to freeze any pre-trained CNN part. For example, to freeze some layers in BERT model, you could do something like this:\nif freeze_embeddings: for param in list(model.bert.embeddings.parameters()): param.requires_grad = False print (\u0026quot;Froze Embedding Layer\u0026quot;) # freeze_layers is a string \u0026quot;1,2,3\u0026quot; representing layer number if freeze_layers is not \u0026quot;\u0026quot;: layer_indexes = [int(x) for x in freeze_layers.split(\u0026quot;,\u0026quot;)] for layer_idx in layer_indexes: for param in list(model.bert.encoder.layer[layer_idx].parameters()): param.requires_grad = False print (\u0026quot;Froze Layer: \u0026quot;, layer_idx)  ","date":1578517390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578696038,"objectID":"2cd8a3ec49cf5b20aa026a962a81fc8c","permalink":"https://Alibabade.github.io/post/training_tricks_dl/","publishdate":"2020-01-08T21:03:10Z","relpermalink":"/post/training_tricks_dl/","section":"post","summary":"Summary neural network training tricks.","tags":["Academic"],"title":"Training_tricks_dl","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"This blog simply clarifies the concepts of graph embedding, graph neural networks and graph convolutional networks.\nGraph Embedding Graph Embedding (GE) is in representation learning of neural network, which often contains two types:\n embed each node in a graph into a low-dimension, scalar and dense vector, which can be represented and inferenced for learning tasks. embed the whole graph into a low-dimension, scalar and dense vector, which can be used for graph structure classification.  There are three types of method to complete graph embedding:\n Matrix Factorization. A representable matrix of a graph is factorized into vectors, which can be used for learning tasks. The representable matrix of a graph are often adjacency matrix, laplacian matrix etc. Deepwalk. Inspired by word2vec, the deepwalk considers the reached node list by random walk as a word, which is fed into word2vec network to obtain a embeded vector for learning tasks. Graph Neural Network. It is basically a series of neural networks operating on graphs. The graph information like representable matrix is fed into neural networks in order to get embedded vectors for learning tasks.  Graph Neural Networks Graph Neural Networks (GNN) are deep neural networks with graph information as input. In general, the GNN can be divided into different types: 1. Graph Convolutional Networks (GCN); 2. Graph Attention Networks (GAT); 3. Graph Adversarial Networks; 4. Graph LSTM. The basic relationship between GE, GNN,GCN is shown as following picture:\n   Fig 3. Relation between GE, GNN and GCN in this blog   Graph Convolutional Networks Graph Convolutional Networks (GCN) operate convolution on graph information like adjacency matrix, which is similar to convolution on pixels in CNN. To better clarify this concept, we will use equations and pictures in the following paragraph.\nConcepts There are two concepts should be understood first before GCN. Degree Matrix (D): this matrix ($N \\times N$, N is the node number) is a diag matrix in which values in diag line means the degree of each node; Adjacency Matrix (A): this matrix is also a $N \\times N$ matrix in which value $A_{i,j}=1$ means there is an edge between node $i$ and $j$, otherwise $A_{i,j}=0$;\nSimple GCN example Let\u0026rsquo;s consider one simple GCN example, which has one GCN layer and one activation layer, the formulation is as following: $$f(H^{l}, A) = \\sigma(AH^{l}W^{l})$$ where $W^l$ denotes the weight matrix in the $l$th layer and $\\sigma(\\dot)$ denotes the activation function like ReLU. This is the simplest expression of GCN example, but it\u0026rsquo;s already much powerful (we will show example below). However, there are two basic limitations of this simple formulation:\n there is no node self information as adjacency matrix $A$ does not contain any information of nodeself. there is no normalization of adjacency matrix. The formulation $AH^{l}$ is actually a linear transformation which scales node feature vectors $H^l$ by summing the features of all neighbour nodes. The nodes having more neighbour nodes has more impact, which should be normalized.  Fix limitation 1. We introduce the identity matrix $I$ into adjacency matrix $A$ to add nodeself information. For example, $\\hat{A} = A + I_n$ where $I_n$ is the identity matrix with $n \\times n$ size.\nFix limiation 2. Normalizing $A$ means that all rows of $A$ should sum to be one, and we realize this by $D^{-1}A$ where $D$ is the diag degree matrix. In practise, we surprisingly find that using a symmetric normalization, e.g., $D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ is more dynamically more interesting (I still do not get it why use symmetric normalization $D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$). Combining these two tricks, we get the propagation rule introduced in Kipf et al. 2017: $$f(H^l, A) = \\sigma(\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^lW^l)$$ where $\\hat{A} = I_n + A$ and $\\hat{D}$ is the diagonal node degree matrix of $\\hat{A}$. In general, whatever matrix multiplies $H^lW^l$ (i.e. $A$, $D^{-1}A$ or $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$) is called Laplacian Matrix, which can be denoted as $L_{i,j}$ the value for $i$th and $j$th node. Taking the laplacian Matrix introduced in Kipf et al. 2017 as an example, the $L_{i,j}$ is as following: \\begin{equation} L_{i,j}^{sym} = \\begin{cases} 1, \u0026amp;i=j, deg(j) \\neq 0 \\newline -\\frac{1}{\\sqrt{deg(i)deg(j)}}, \u0026amp;i \\neq j, j \\in \\Omega_i \\newline 0, \u0026amp;otherwise \\end{cases} \\end{equation} where deg($\\cdot$) denotes the degree matrix and $\\Omega_i$ denotes all the neighbour nodes of node $i$. This symmetric Laplacian matrix not only considers the degree of node $i$ but also takes the degree of its neighbour node $j$ into account, which refers to symmetric normalization. This propagation rule weighs neighbour in the weighted sum higher if the node $i$ has a low-degree and lower if the node $i$ has a high-degree. This may be useful when low-degree neighbours have bigger impact than high-degree neighbours.\nCode example for simple GCN Considering the following simple directed graph:\n   Fig 2. Graph example in this blog   Then the Adjacency Matrix $A$ is:\nA = np.matrix([ [0.,1.,0.,1.], [0.,0.,1.,1.], [0.,1.,0.,0.], [1.,0.,1.,0.]], dtype=float)  the identity matrix $I_n$ of $A$ is:\nI_n = np.matrix([ [1.,0.,0.,0.], [0.,1.,0.,0.], [0.,0.,1.,0.], [0.,0.,0.,1.]], dtype=float)  We assign random weight matrix of one GCN layer:\nW = np.matrix([[1,-1],[-1,1]])  Next we randomly give 2 integer features for each node in the graph.\nH = np.matrix([[i,-i] for i in range(A.shape[0])], dtype=float) H matrix([[ 0., 0.], [ 1., -1.], [ 2., -2.], [ 3., -3.]])  Now the unnormalized features $\\hat{A}H$ are:\nA_hat * H matrix([[ 1., -1.], [ 6., -6.], [ 3., -3.], [ 5., -5.]])  the output of this GCN layer:\nA_hat * H * W matrix([[ 2., -2.], [ 12., -12.], [ 6., -6.], [ 10., -10.]]) # f(H,A)=relu(A_hat * H * W) relu(A_hat * H * W) matrix([ [2., 0.], [12.,0.] [6., 0.], [10., 0]])  Next we apply the propagation rule introduced in Kipf et al. 2017. First, we add self-loop information $\\hat{A} = A + I$ is :\nA_hat = A + I_n A_hat matrix([ [1.,1.,0.,0.], [0.,1.,1.,1.], [0.,1.,1.,0.], [1.,0.,1.,1.]])  Second, we add normalization by computing $\\hat{D}$ and $\\hat{D}^{-\\frac{1}{2}}$ (inverse matrix of square root of $\\hat{D}$):\nD_hat = np.array(np.sum(A_hat, axis=0))[0] D_hat = np.matrix(np.diag(D_hat)) D_hat matrix([[ 2., 0., 0., 0.], [ 0., 3., 0., 0.], [ 0., 0., 3., 0.], [ 0., 0., 0., 2.]]) inv_D_hat_sqrtroot = np.linalg.inv(np.sqrt(D_hat)) inv_D_hat_sqrtroot matrix([[ 0.70710678, 0. , 0. , 0. ], [ 0. , 0.57735027, 0. , 0. ], [ 0. , 0. , 0.57735027, 0. ], [ 0. , 0. , 0. , 0.70710678]])  Next, we compute the Laplacian matrix of $L = \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ and the nomalized features $L * H$ :\nLaplacian_matrix = inv_D_hat_sqrtroot * A_hat * inv_D_hat_sqrtroot Laplacian_matrix matrix([[ 0.5 , 0.40824829, 0. , 0. ], [ 0. , 0.33333333, 0.33333333, 0.40824829], [ 0. , 0.33333333, 0.33333333, 0. ], [ 0.5 , 0. , 0.40824829, 0.5 ]])  # normalized feature vectors Laplacian_matrix * H matrix([[ 0.40824829, -0.40824829], [ 2.22474487, -2.22474487], [ 1. , -1. ], [ 2.31649658, -2.31649658]]) #non-normalized feature vectors A_hat * H matrix([[ 1., -1.], [ 6., -6.], [ 3., -3.], [ 5., -5.]])  As can be seen, all the values of feature vectors are scaled to smaller absolute values than Non-normalized feature vectors.\nFinally, the output of GCN layer with applying the propagation rule:\n# f(H,A) = relu(L*H*W) relu(Laplacian_matrix*H*W) matrix([[ 0.81649658, 0.], [ 4.44948974, 0.], [ 2. , 0.], [ 4.63299316, 0.]]) # compared to f(H,A) = relu(A_hat*H*W) relu(A_hat*H*W) matrix([[ 2., 0.], [ 12., 0.], [ 6., 0.], [ 10., 0.]])  I suggest to verify this operation by yourself.\nReal example: Semi-Supervised Classification with GCNs Kipf \u0026amp; Welling ICLR 2017 demonstrates that the propgation rule in GCNs can predict semi-supervised classification for social networks. In this semi-supervised learning example, we assume that we know all the graph information including nodes and their neighbours, but not all the node labels, which means some nodes are labeled but others are not labeled.\nWe train the GCNs on labeled nodes and propagate the node label information to unlabedled nodes by updating weight matrices shared arcoss all nodes. This is done by following steps:\n perform forward propagation through the GCN layers. apply sigmoid function row-wise at the last layer of GCN. compute the cross entropy loss on known node labels. backpropagate the loss and update the weight matrices $W$ in each layer.  Zachary\u0026rsquo;s Karate Club Zachary\u0026rsquo;s Karate Club is a typical small social network where there are a few main class labels. The task is to predict which class each member belongs to.    Fig 3. Graph structure of Karate Club in this blog   We run a 3-layer GCN with randomly initialized weights. Now before training the weights, we simply insert Adjacency matrix $A$ and feature $H=I$ (i.e., $I$ is the identity matrix) into the model, then perform three propagation steps during the forward pass and effectively convolves the 3rd-order neighbourhood of each node. The model already produces predict results like picture below without any training updates:\n   Fig 4. Predicted nodes for Karate Club in this blog   Now we rewrite the propagation rule in layer-wise GCN (in vector form): $$h_{i}^{l+1} = \\sigma (\\sum_{j} \\frac{1}{c_{i,j}} h_{j}^{l} W^l)$$ where $\\frac{1}{c_{i,j}}$ originates from $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ and $h_j^l$ denotes the feature vector of neighbour node $j$. Now the propagation rule is interpreted as a differentiable and parameterized (with $W^l$) variant, if we choose an appropriate non-linear activation and initialize the random weight matrix such that is orthogonal (or using the initialization from Glorot \u0026amp; Bengio, AISTATS 2010), this update rule becomes stable in practice (also thanks to the normalization with $c_{i,j}$).\nNext we simply label one node per class and use the semi-supervised learning algorithm for GCNs introduced in Kipf \u0026amp; Welling, ICLR 2017, and we start to train for a couple of iterations:  The video above shows Semi-supervised classification with GCNs in this blog. And the model directly produces a 2-dimensional laten space which we can visualize.\nNote that we just use random feature vectors (e.g., identity matrix we used here) and random weight matrices, only after a couple of iteration, the model used in Kipf \u0026amp; Welling is already able to achieve remarkable results. If we choose more serious initial node feature vectors, then the model can achieve state-of-the-art classification results on a various number of graph datasets.\nFurther Reading on GCN Well, the GCN is developing rapidly, here are a few papers for further reading:\n Inductive Representation Learning on Large graph FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification  Reference  https://zhuanlan.zhihu.com/p/89503068 https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780 https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0 https://tkipf.github.io/graph-convolutional-networks/  ","date":1578245173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578943577,"objectID":"99a069dafe5d68a4745b2e6222d70ae8","permalink":"https://Alibabade.github.io/post/graph_nn/","publishdate":"2020-01-05T17:26:13Z","relpermalink":"/post/graph_nn/","section":"post","summary":"Brief summary of Graph Convolutional Networks","tags":["Academic"],"title":"Graph_NN","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","Computer Graphics"],"content":"Deep Correlations for Texture Synthesis (TOG2017) Motivation The texture synthesis using deep feature maps has difficulty to synthesize structure textures, which is a challenge to preserve the non-local structures.\nCore Ideas  Introduce a structure matrix to represent the deep correlation among deep features. In another word, Gatys et al.,2015 consider the Gram loss of deep features between channels, while acutally the structure information is included inside each feature channel. Thus Sendik et al., 2017 propose an intra-feature based Gram loss, which is fomulated as:  \\begin{equation} R_{i,j}^{l,n} = \\sum_{q,m} w_{i,j} f_{q,n}^{l,n} f_{q-i,m-j}^{l,n} \\end{equation} where $R_{i,j}^{l,n}$ denotes the intra-feature Gram loss of $n$th channel in $l$th layer, $i \\in [-Q/2, Q/2]$ and $ j \\in [-M/2,M/2]$. This means that $f_{q,m}^{l,n}$ is shifted by $i$ pixels vertically and $j$ pixels horizontally, and applying a point-wise multiplication across the overlapping region, weighted by the inverse of the total amount of overlapping regions, That is $$w_{i,j} = [(Q-|i|)(M-|j|)]^{-1}$ Then the structure loss based on intra-feature Gram loss is denoted as:\n$$E_{DCor}^l = \\frac{1}{4}\\sum_{i,j,n}(R_{i,j}^{l,n} - \\widetilde{R}_{i,j}^{l,n})^2$$\n Introduce a diversity loss to make sure the method can synthesize a larger texture than input exemplar. The idea is to shift the input deep correlation matrix $f_{i,j}^{l,n}$ to the size of desired output texture.\n  Introduce an edge-preserving smooth loss, which only penalizes the pixel difference when none of neighbouring pixels are smiliar to the pixel under consideration. The authors claim that this smooth loss is useful to reduce checker artefacts.\n  The total loss function is weighted $E_{DCor}$, Gram loss, diversity loss and edge-preserving smooth loss.\n  ","date":1578229042,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578943074,"objectID":"7f34bbe5927931de207d4c2ed722ba90","permalink":"https://Alibabade.github.io/post/archive_papers/","publishdate":"2020-01-05T12:57:22Z","relpermalink":"/post/archive_papers/","section":"post","summary":"Archive for regular papers","tags":["Academic"],"title":"Archive_papers","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"Attention The attention mechanism was created to simulate the human visual attention on images or understanding attention on texts. It was firstly born for solving a problem that widely exists in natural language processing (NLP) models like seq2seq, which NLP models often tend to forget the first part of processed sentences.\nSeq2seq model The encoder-decoder architecture commonly used in Seq2seq model:\n encoder, compress the input sentence into a context vector in a fixed length way which is regarded as a representation of the meaning of input sentence. decoder, fed by the context vector and translate the vector to output. In some early works, the last state of encoder is usually used as the initial state of decoder.  Both of the encoder and decoder are recurrent neural networks, using LSTM or GRU units.\nThe critical problem of seq2seq model. The seq2seq model often forgets the first part of a long sentence once it completes translation from the entire sentence to a context vector. To address this problem, the attention mechanism is proposed.\nThe attention mechanism The new architecture for encoder-decoder machine translaion is as following:    Fig 1. The encoder-decoder architecture in Bahdanau et al. 2015   The encoder is composed by a bidirection RNN, a context vector is the sum of weighted hidden states and the decoder translates the context vector to a output target based on previous output targets.\nFormula Let x=$(x_1, x_2,\u0026hellip;,x_n)$ denote the source sequence of length $n$ and y=$(y_1, y_2,\u0026hellip;,y_m)$ denote the output sequence of length $m$, $\\overrightarrow{h_i}$ denotes the forward direction state and $\\overleftarrow{h_i}$ presents the backward direction state, then the hidden state for $i$th input word is fomulated as: $$h_i = [\\overrightarrow{h_i}^T; \\overleftarrow{h_i}^T], i=1,2,\u0026hellip;,n$$\nThe hidden states at position $t$ in decoder includes previous hidden states $s_{t-1}$, previous output target $y_{t-1}$ the context vector $c_t$, which is denoted as $s_{t} = f(s_{t-1}, y_{t-1}, c_{t})$, where the context vector $c_{t}$ is a sum of encoder hidden states of input sequence, weighted by alignment scores. For output target at position $t$, we have: $$c_{t} = \\sum_{i=1}^{n} \\alpha_{t,i} h_i$$\n$$ \\alpha_{t,i}= align(y_t, x_i) = \\frac{exp(score(s_{t-1},h_i))}{\\Sigma^n_{i=1} exp(score(s_{t-1},h_{i}))} $$ The score $\\alpha_{t,i}$ is assigned to the pair $(y_t, x_i)$ of input at position $i$ and output at position $t$, and the set of weights ${\\alpha_{t,i}}$ denotes how much each source hidden state matches for each output. In Bahdanau et al. 2015, the score $\\alpha$ is learnt by a feed-forward network with a single hidden layer and this network is jointly learnt with other part of the model. Since the score is modelled in a network which also has weight matrices (i.e., $v_a$ and $W_a$) and activation layer (i.e., tanh), then the learning function is fomulated as: $$score(s_t, h_i) = v_a^T tanh(W_a[s_t;h_i])$$\nSelf-attention Self-attention (or intra-attention) is such an attention mechnaism that assigns correlation in a single sequence for an effective representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarizatin or image description generation.\nIn Cheng et al., 2016, an application of self-attention mechanism is shown in machine reading. For example, the self-attention mechanism enables the model to learn a correlation between the current word and the previous part of the input sentence.    Fig 2. An example of self-attention mechanism in Cheng et al., 2016   Soft and Hard Attention In image caption generation, the attention mechanism is applied and shown to be very helpful. Xu et al.,2015 shows a series of attention visualization to demonstrate how the model learn to summarize the image by paying attention to different regions.    Fig 2. The visulation of attention mechanism for image caption generation in Xu et al., 2015   The soft attention and hard attention is telled by whether the attention has access to the entire image or only a patch region:\nSoft attention: the alignment weights are assigned to all the patches in the source image, which is the same type used in Bahdanau et al. 2015 Pro: the model is smooth and differentiable Con: computationally expensive when the source image is large\nHard attention: the alignment weights are only assigned to a patch in the source image at a time Pro: less computation at the inference time Con: the model is non-differentiable and requires more complicated techniques such as variance reduction and reinforcement learning to train.(Luong et al., 2015)\nwhy hard attention is non-differentiable? Hard attention is non-differentiable because it’s stochastic. Both hard and soft attention calculate a context vector using a probability distribution (usually over some set of annotation vectors), but soft attention works by taking the expected context vector at that time (i.e. a weighted sum of the annotation vectors using the probability distribution as weights), which is a differentiable action. Hard attention, on the other hand, stochastically chooses a context vector; when the distribution is multinomial over a set of annotation vectors (similar to how soft attention works, but we aren’t calculating the expected context vector now), you just sample from the annotation vectors using the given distribution. The advantage of hard attention is that you can also attend to context spaces that aren’t multinomial (e.g. a Gaussian-distributed context space), which is very helpful when the context space is continuous rather than discrete (soft attention can only work over discrete spaces).\nAnd since hard attention is non-differentiable, models using this method have to be trained using reinforcement learning.\nGlobal and Local Attention Luong et al. 2015 proposed a global and local attention, where the global attention calculate the entire weighted sum of hidden states for a target output (which is similar to soft attention). While the local attention is more smiliar to the blend of soft and hard attention. For example, in their paper, the model first predict a aligned position for the current target word then a window centred around the source position is used to compute a context vector.\n   Fig 1. The global and local attention architecture in Luong et al., 2015   Reference  This attention blog heavily borrowed from Lilian Weng\u0026rsquo;s blog, more details refer to https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html  ","date":1577570595,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578954790,"objectID":"d7323692466e95a1bd1eca0d480be9b1","permalink":"https://Alibabade.github.io/post/attention/","publishdate":"2019-12-28T22:03:15Z","relpermalink":"/post/attention/","section":"post","summary":"Brief summary about attention","tags":["Academic","attention in dl"],"title":"Attention","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"","date":1577479276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578943577,"objectID":"5acf0a9265de05af60c769d7e9347a9c","permalink":"https://Alibabade.github.io/post/meta_learning_in_dl/","publishdate":"2019-12-27T20:41:16Z","relpermalink":"/post/meta_learning_in_dl/","section":"post","summary":"Summary about the meta learning","tags":["Academic","Meta-learning in dl"],"title":"Meta_learning_in_dl","type":"post"},{"authors":["Li Wang"],"categories":["DL/ML","CV/CG"],"content":"Basic knowledge in neural networks  Graph Neural Network Graph Convolutional Network Transform learning: mete learning, few shot learning, zero-shot learning RNN, GRU Siamese Network Reinforcement learning NAS Optimization: Adam, SGD Normalization AlexNet, VGG, Inception, ResNet (ResNet-50,ResNet-101, ResNeXt-50/101), Xecption.  SOTA research areas  Object recognition/detection: YOLO, LOGAN, Anchor, Anchor free, two-stage,one-stage Object tracking face recognition/detection NLP: BERT, seq2seq, bag of words Image segmentation/instance segmentation: deeplab ShuffleNet, MobileNet 2D Image \u0026mdash;\u0026gt; 3D model Human pose estimation: 2D skeleton, 3D skeleton, 3D mesh Optical flow Attention FPN, Mask-rcnn,faster-rcnn, RPN, RetinaNet, ROI pooling,  Machine Learning  KNN,SVM, GBDT, XGBOOST  ","date":1577269663,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578943074,"objectID":"dfd8aa50d70e25a70f0dc59876316d5a","permalink":"https://Alibabade.github.io/post/record_discrete_knowledge/","publishdate":"2019-12-25T10:27:43Z","relpermalink":"/post/record_discrete_knowledge/","section":"post","summary":"A simple archive for discrete knowledge","tags":["Academic"],"title":"Archive_discrete_knowledge","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"Activation functions Description Activation functions works like an on-off button that determines whether the output of a neuron or what information should be passed to next layer. In biology, it works like synaptic in brain which decides what information it passes from one neuron cell to next one. There are several activation functions widely used in neural networks.\nBinary function (step function) In one word, the output of binary function is 1 or 0 which is based on whether the input is greater or lower than a threshold. In math, it looks like this: f(x) = {1, if x \u0026gt; T; 0, otherwise}.\nCons: it does not allow multiple outputs, and it can not support to classify inputs to one of categories.\nLinear function f(x) = $cx$. Cons: 1. the deviation of linear function is a constant, which does not help for backpropagation as the deviation is not correlated to its inputs, in another word, it can not distinguih what weights or parameters help to learn the task; 2. linear function makes the entire multiple neural network into one linear layer (as the combination of linear functions is still a linear function), which becomes a simple regression model. It can not handle complex tasks by varying parameters of inputs.\nNon-linear functions Non-linear functions address the problems by two aspects:\n The deviation of non-liear function is a function correlated to its inputs, which contributes the backpropagation to learn how to update weights for high accurancy. Non-linear functions form the layers with hidden neurons into a deep neural network which is capable of predicting for complicated tasks by learning from complex datasets.  There are several popular activation functions used in modern deep neural networks.\nSigmoid/Logistic Regression    Fig 1. Sigmoid Visualization   Equation: $$Sigmoid(x) = \\frac{1}{1+e^{-x}}$$ Derivative (with respect to $x$): $$Sigmoid^{'}(x) = Sigmoid(x)(1-Sigmoid(x))$$ Pros:\n smooth gradient, no jumping output values compared to binary function. output value lies between 0 and 1, normalizing output of each neuron. right choice for probability prediction, the probability of anything exists only between 0 and 1.  Cons:\n vanishing gradient, the gradient barely changes when $x\u0026gt;2$ or $x\u0026lt;-2$. computationally expensive. non zero centered outputs. The outputs after applying sigmoid are always positive, during gradient descent, the gradients on weights in backpropagation will always be positive or negative, which means the gradient updates go too far in different directions, and makes the optimization harder.  Softmax $$Softmax(x_i)= \\frac{x_i}{\\Sigma_{j=1}^{n}{x_j}}$$\nPros: capable of handling multiple classification and the sum of predicted probabilities is 1. Cons: only used for output layer.\nSoftmax is more suitable for multiple classification case when the predicted class must and only be one of categories. k-sigmoid/LR can be used to classify such multi-class problem that the predicted class could be multiple.\nTanh Equation: $$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$ Derivative (with respect to $x$): $$tanh^{'}(x) = 1 -tanh(x)^2$$\nPros:\n zero centered. make it easier to model inputs that have strongly positive, strongly negative, and natural values. similar to sigmoid  Cons:\n vanishing gradient computationally expensive as it includes division and exponential operation.  Vanishing gradient Vanishing gradient means that the values of weights and biases are barely change along with the training.\nExploding gradient Gradient explosion means that the values of weights and biases are increasing rapidly along with the training.\nReLU     Fig 2. ReLU Visualization   Equation: $$ReLU(x) = max(0, x)$$ Derivative (with respect to $x$): \\begin{equation} ReLU^{'}(x) = \\begin{cases} 0, \u0026amp;x \\leqslant 0; \\newline 1, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation} Pros:\n computationally efficient non-linear  Why ReLU performs better in modern NNs? The answer is not so sure right now, but its propeties like non-saturation gradient and computionally efficient indeed lead to fast convergence. Additionally, its property sparsing the network also improves the modeling preformance. The non-zero centered issue can be tackled by other regularization techniques like Batch Normalization which produces a stable distribution for ReLU.\nCons:\n Dying ReLU problem. The backpropagation won\u0026rsquo;t work when inputs approach zero or negative. However, to some extent, dying ReLU problem makes input values sparse which is helpful for neural network to learn more important values and perform better. Non differentiable at zero. Non zero centered. Don\u0026rsquo;t avoid gradient explode  ELU     Fig 3. ELU Visualization   Equation: \\begin{equation} ELU(x) = \\begin{cases} \\alpha (e^x-1), \u0026amp; x \\leqslant 0 \\newline x, \u0026amp;x \u0026gt; 0 \\end{cases} \\end{equation}\nDerivative: \\begin{equation} ELU^{'}(x) = \\begin{cases} ELU(x) + \\alpha, \u0026amp; x \\leqslant 0 \\newline 1, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation}\nPros:\n prevent dying ReLU problem. gradient works when input values are negative. non-linear, gradient is not zero.  Cons:\n don\u0026rsquo;t avoid gradient explode. not computationally efficient. $\\alpha$ is not learnt by neural networks.  Leaky ReLU     Fig 4. LReLU Visualization ($\\alpha=0.1$)   Equation: \\begin{equation} LReLU(x) = \\begin{cases} \\alpha x, \u0026amp;x \\leqslant 0 \\newline x, \u0026amp;x \u0026gt; 0 \\end{cases} \\end{equation}\nDerviative: \\begin{equation} LReLU^{'}(x) = \\begin{cases} \\alpha, \u0026amp;x \\leqslant 0 \\newline 1, \u0026amp;x \u0026gt; 0 \\end{cases} \\end{equation}\nPros:\n prevent Dying ReLU problem computationally efficient non-linear  Cons:\n don\u0026rsquo;t avoid gradient explode Non consistent results for negative input values. non-zero centered non differentiable at Zeros  SELU     Fig 5. SELU Visualization   Equation: \\begin{equation} SELU(x) = \\lambda \\begin{cases} \\alpha e^x-\\alpha, \u0026amp; x \\leqslant 0 \\newline x, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation}\nDerivative: \\begin{equation} SELU^{'}(x) = \\lambda \\begin{cases} \\alpha e^x, \u0026amp; x \\leqslant 0 \\newline 1, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation} where $\\alpha \\approx 1.6732632423543772848170429916717$ and $\\lambda \\approx 1.0507009873554804934193349852946$.\nPros:\n Internal normalization, which means faster convergence. Preventing vanishing gradient and exploding gradient.  Cons: Need more applications to prove its performance on CNNs and RNNs.\nGELU     Fig 6. GELU Visualization   Equation: \\begin{equation} GELU(x) = 0.5x(1 + tanh(\\sqrt{\\frac{2}{\\pi}} (x + 0.044715x^3))) \\end{equation}\nPros:\n Best performance in NLP, especially BERT and GPT-2 Avoid vanishing gradient  Cons: Need more applications to prove its performance.\nReference  https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/ https://www.jianshu.com/p/6db999961393 https://towardsdatascience.com/activation-functions-b63185778794 https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions  ","date":1577222141,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578954790,"objectID":"a5a05febc73f1d219093a9c23e6b6b66","permalink":"https://Alibabade.github.io/post/activation_functions_in_dl/","publishdate":"2019-12-24T21:15:41Z","relpermalink":"/post/activation_functions_in_dl/","section":"post","summary":"Brief summary for understanding activation functions in NN","tags":["Academic","Activation functions in DL"],"title":"Activation_functions_in_dl","type":"post"},{"authors":["Li Wang"],"categories":["ML/DL"],"content":"1. Bag of words (BOW) BOW is a method to extract features from text documents, which is usually used in NLP, Information retrieve (IR) from documents and document classification. In general, BOW summarizes words in documents into a vocabulary (like dict type in python) that collects all the words in the documents along with word counts but disregarding the order they appear.\nFor examples, two sentences:\nLei Li would like to have a lunch before he goes to watch a movie.  James enjoyed the movie of Star War and would like to watch it again.  BOW will collect all the words together to form a vocabulary like:\n{\u0026quot;Lei\u0026quot;:1, \u0026quot;Li\u0026quot;:1, \u0026quot;would\u0026quot;:2, \u0026quot;like\u0026quot;:2, \u0026quot;to\u0026quot;:3, \u0026quot;have\u0026quot;:1, \u0026quot;a\u0026quot;:2, \u0026quot;lunch\u0026quot;:1, \u0026quot;before\u0026quot;:1, \u0026quot;he\u0026quot;:1, \u0026quot;goes\u0026quot;:1, \u0026quot;watch\u0026quot;:2, \u0026quot;movie\u0026quot;:2, \u0026quot;James\u0026quot;:1, \u0026quot;enjoyed\u0026quot;:1, \u0026quot;the\u0026quot;:1, \u0026quot;of\u0026quot;:1, \u0026quot;Star\u0026quot;:1, \u0026quot;War\u0026quot;:1, \u0026quot;and\u0026quot;:1, \u0026quot;it\u0026quot;:1, \u0026quot;again\u0026quot;:1 }  The length of vector represents each sentence is equal to the word number, which is 22 in our case. Then first sentence is presented in vector (in the order of vocabulary) as: {1,1,1,1,2,1,2,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0}, and the second sentence is presented in vector as: {0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1}.\nReference https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/\n2. Principal Component Analysis (PCA) To better understand the mathematic theory behind PCA, please refer to this Blog (Chinese only).\n mean: $\\mu_i = \\frac{1}{n} \\Sigma_{i=1}^n{x_i}$ variance: $\\sigma^2 = \\frac{1}{n} \\Sigma_{i=1}^n{(x_i - \\mu_i)^2}$ standard deviation: $\\sigma^2 = \\frac{1}{n-1} \\Sigma_{i=1}^n{(x_i - \\mu_i)^2}$ covariance: $cov(x,y) = \\frac{1}{n-1} \\Sigma_{i=1}^n{(x_i-\\mu_x)*(y_i -\\mu_y)}$  Let\u0026rsquo;s denote the input data $X$ is a $N \\times M$ matrix where $N$ is the dimension of each input sample and $M$ is the number of input samples. Usually, the PCA aims to reduce the dimension $N$ to $R$ by a linear transformation matrix $P$ of $X$, which can be written as $Y=PX$. The dimension reduction actually changes the basis vector set of $X$ to the row vectors in $P$, and we hope the new basis vector set is independent from each other and the projected points in one basis vector is as far as possible from each other. In other words, we hope the variance in one basis vector is as large as possible and the covariance between each two basis vector is as small as possible. To this end, we compute the covariance matrix $B$ of $Y$, and we find the elements at diagonal are variances and other elements are covariance of new each basis vectors (in fact, the covariance matrix B is a symmetry matrix), thus we then seek $P$ to make the covariance matrix $B$ diagonalization, whcih means the elements are as large as possible at diagonal and elsewhere is zero.\n$$B = \\frac{1}{m} YY^T = \\frac{1}{m} (PX)(PX)^T = P (\\frac{1}{m}XX^T) P^T = P A P^T$$ where A denotes the covariance matrix of input data $X$. Equation above indicates that the $P$ diagonalize A is what we seek. In linear algebra, $P$ is actually made of eigenvectors of $A$ and elements at diagnoal are corresponding eigenvalues, which is called eigen decomposition. Based on this, we only need to compute the eigenvectors and their eigenvalues of $A$ and sort the eigenvectors into a matrix $C$ by ordering from larger corresponding eigenvalues to smaller ones. If we want to reduce the dimension from $N$ to $R$ ($R\u0026lt;N$), then we choose the top $R$ rows of $C$ to form $P$, and perform $PX$ to get the final result $Y$.\nAs we can see, PCA needs to compute the mean, variance and covariance to form the covariance matrix of original input data, and compute eigenvectors and their corresponding eigenvalues, which indicates this could be quite slow when input dimension $N$ is large.\nReference https://blog.csdn.net/u010376788/article/details/46957957\n3. Cross Entropy 3.1 Amount of information that an event gives In general, the amount of information should be greater when an event with low probability happens. For example, event A: China won the table-tennis world champion; event B: Eygpt won the table-tennis world champion. Obviously, event B will give people more information if it happens. The reason behind this is that event A has great probability to happen while event B is rather rare, so people will get more information if event B happens.\nThe amount of information that an event gives is denoted as following equation:\n$$f(x) = -log(p(x))$$ where $p(x)$ denotes the probability that event $x$ happens.\n3.2 Entropy For a given event $X$, there may be several possible situations/results, and each situation/result has its own probability, then the amount of information that this event gives is denoted as: $$f(X)= -\\Sigma_{i=1}^{n}p(x_i)log(p(x_i))$$\nwhere $n$ denotes the number of situations/results and $p(x_i)$ is the probability of situation/result $x_i$ happens.\n3.3 Kullback-Leibler (KL) divergence The KL divergence aims to describe the difference between two probability distributions. For instance, for a given event $X$ consisting of a series events ${x_1,x_2,\u0026hellip;,x_n}$, if there are two probability distributions of possible situations/results: $P={p(x_1),p(x_2),\u0026hellip;,p(x_n)}$ and $Q={q(x_1),q(x_2),\u0026hellip;,q(x_n)}$, then the KL divergence distance between $P$ and $Q$ is formulated as:\n$$D_{KL}(P||Q) = \\Sigma_{i=1}^n p(x_i)log(\\frac{p(x_i)}{q(x_i)})$$ further, $$D_{KL}(P||Q) = \\Sigma_{i=1}^n p(x_i)log(p(x_i)) - \\Sigma_{i=1}^n p(x_i)log(q(x_i))$$ where $Q$ is closer to $P$ when $D_{KL}(P||Q)$ is smaller.\n3.4 Cross Entropy In machine learning or deep learning, let $y={p(x_1),p(x_2),\u0026hellip;,p(x_n)}$ denote the groundturth probability distribution, and $\\widetilde{y}={q(x_1),q(x_2),\u0026hellip;,q(x_n)}$ present the predicted probability distribution, then KL divergence is just a good way to compute the distance between predicted distribution and groudtruth. Thus, the loss could be just formulated as: $$Loss(y,\\widetilde{y}) = \\Sigma_{i=1}^n p(x_i)log(p(x_i)) - \\Sigma_{i=1}^n p(x_i)log(q(x_i))$$ where the first term $\\Sigma_{i=1}^n p(x_i)log(p(x_i))$ is a constant, then the $Loss(y,\\widetilde{y})$ is only related to the second term $- \\Sigma_{i=1}^n p(x_i)log(q(x_i))$ which is called **Cross Entropy** as a training loss.\nReference https://blog.csdn.net/tsyccnh/article/details/79163834\n4. Conv 1x1 Conv 1x1 in Google Inception is quite useful when the filter dimension of input featues needs to be increased or decreased meanwhile keeping the spaital dimension, and reduces the convolution computation. For example, the input feautre dimension is $(B, C, H, W)$ where $B$ is batch size, $C$ is channel number, $H$ and $W$ are height and width. Using $M$ filters of Conv 1x1, then the output of Conv 1x1 is $(B,M,H,W)$, only channel number changes but spatial dimension ($H \\times W$) is still the same as input. To demonstrate the computation efficiency using conv 1x1, take a look at next example. For instance, the output feature we want is $(C, H, W)$, using M filters of conv 3x3, then the compuation is $3^2C \\times MHW$. Using conv 1x1, the computation is $1^2C \\times MHW$, which is $\\frac{1}{9}$ of using conv 3x3.\nWhy do we need to decrease filter dimension or the number of feature maps? The filters or the number of feature maps often increases along with the depth of the network, it is a common network design pattern. For example, the number of feature maps in VGG19, is 64,128,512 along with the depth of network. Further, some networks like Inception architecture may also concatenate the output feature maps from multiple front convolution layers, which also rapidly increases the number of feature maps to subsequent convolutional layers.\nReference https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network\n5. IOU (Intersection of Union) in Object Detection    Fig 1. IOU Visualization in this blog.   Bounding-box Regression in Object Detection Why do we need Bounding-box Regression? In general, our object detection method predicts bounding-box for an object like blue box in below image. But the groundturth box is shown in green colour, thus we can see the bounding-box of the plane is not accurate compared to the groundtruth box as IOU is lower than 0.5 (intersection of union). If we want to get box location more close to groundtruth, then Bounding-box Regression will help us do this.\n   Fig 2. Predicted box for airplane and its corresponding groudtruth in this blog.   What is Bounding-box Regression? We use $P = (P_x,P_y, P_w, P_y)$ presents the centre coordinates and width/height for the Region Proposals, which is shown as Blue window in the Fig 3. The groundtruth box is represented by $G=(G_x,G_y,G_w,G_h)$. Our aim is to find a projection function $F$ which finds a box $\\hat{G}=(\\hat{G}_x,\\hat{G}_y,\\hat{G}_w,\\hat{G}_h)$ closer to $G$. In math, we need to find a $F$ which makes sure that $F(P) = \\hat{G}$ and $\\hat{G} \\approx G$.\n   Fig 3. Bounding box regression in this blog.   How to do Bounding-box Regression in R-CNN? We want to transform $P$ to $\\hat{G}$, then we need a transformation $(\\Delta x, \\Delta y, \\Delta w, \\Delta h)$ which makes the following happen: $$\\hat{G}_x = P_x + \\Delta x * P_w \\Rightarrow \\Delta x = (\\hat{G}_x - P_x)/P_w$$ $$\\hat{G}_y = P_y + \\Delta y * P_h \\Rightarrow \\Delta y = (\\hat{G}_y - P_y)/P_h$$ $$\\hat{G}_w = P_w e^{\\Delta w} \\Rightarrow \\Delta w = log(\\hat{G}_w/P_w)$$ $$\\hat{G}_h = P_h e^{\\Delta h} \\Rightarrow \\Delta h = log(\\hat{G}_h/P_h)$$\nWhile the groundtruth $(\\Delta t_x, \\Delta t_y, \\Delta t_w, \\Delta t_h)$ is defined as: $$\\Delta t_x = (G_x - P_x)/P_w$$ $$\\Delta t_y = (G_y - P_y)/P_h$$ $$\\Delta t_w = log(G_w/P_w)$$ $$\\Delta t_h = log(G_h/P_h)$$ Next, we denote $W_i \\Phi(P_i)$ where $i \\in {x,y,w,h}$ as learned transformation through the neural network, then the loss function is to minimize the L2 distance between $\\Delta t_i$ and $W_i \\Phi(P_i)$ where $i \\in {x,y,w,h}$ by SGD: $$L_{reg} = \\sum_{i}^{N} (\\Delta t_i - W_i \\Phi(P_i))^2 + \\lambda ||W||^2$$\n6. Upsampling, Deconvolution and Unpooling Upsampling: upsample any image to higher resolution. It uses upsample and interpolation.\nDeconvolution: also called transpose convolution. For example, your input for deconvolution layer is 4x4, deconvolution layer multiplies one point in the input with a 3x3 weighted kernel and place the 3x3 results in the output image. Where the outputs overlap you sum them. Often you would use a stride larger than 1 to increase the overlap points where you sum them up, which adds upsampling effect (see blue points). The upsampling kernels can be learned just like normal convolutional kernels.    Fig 4. Visualization of Deconvolution in this Quora answer.   Unpooling: We use an unpooling layer to approximately simulate the inverse of max pooling since max pooling is non-invertible. The unpooling operates on following steps: 1. record the maxima positions of each pooling region as a set of switch variables; 2. place the maxima back to the their original positions according to switch variables; 3. reset all values on non-maxima positions to $0$. This may cause some information loss.\nReference:s  https://www.quora.com/What-is-the-difference-between-Deconvolution-Upsampling-Unpooling-and-Convolutional-Sparse-Coding  7. RoI Pooling in Object Detection (Fast RCNN) RoI pooling is simple version of Spatial Pyramid Pooling (multiple division scales, i.e., divide the entire feature maps into (1,4,16) patches/grids), which has only one scale division. For example, the original input image size is (1056x640) and one region proposal ($x_1=0, y_1=80,x_2=245, y_2=498$), after convolutional layers and pooling layers, the feature map size is (66x40), then we should rescale the proposal from ($x_1=0, y_1=80,x_2=245, y_2=498$) to ($x_1=0, y_1=5,x_2=15, y_2=31$) as the scale is 16 (1056/66=16 and 640/40=16). Then we divide the proposal on feature map into 7x7 sections/grids (the proposal size is no need of 7 times) if the output size is 7x7. Next we operate max pooling on each grid, and place the maxima into output 7x7. Here is another simple example below, the input feature size is 8x8, proposal is (0,3,7,8), and the output size is 2x2 thus divide the proposal region into 2x2 sections:\n   Fig 5. Visualization of ROI pooling in this blog.   Reference  https://zhuanlan.zhihu.com/p/73654026ß  8. RoIAlign Pooling in Object Detection RoI align Pooling is proposed in Mask RCNN to address the problem that RoI pooling causes misalignments by rounding quantization.\nProblem of RoI pooling. There are twice misalignments for each RoI pooling operation. For example, the size of original input image is 800x800, and one region proposal is 515x482 and its corresponding coordinates are ($x_{tl}=20, y_{tl}=267, x_{br}=535, y_{br}=749$) where \u0026lsquo;tl\u0026rsquo; means top left and \u0026lsquo;br\u0026rsquo; means bottom right. And the stride of last conv layer is **16** which means each feature map extracted from the last conv layer is **50x50** (800/16). If the output size is fixed to 7x7, then RoI pooling would quantize (by floor operation) the region proposal to **32x30** and its corresponding coordinates to ($x_{tl}=1,y_{tl}=16, x_{br}=33, y_{br}=46$). The twice misalignments in each feature map are visualized in the below figure (acutal coordinates in blue colour).\n   Fig 7. Visualization of RoI quantization/misaligments and RoIAlign corrections.   Solution. RoI Align removes all the quantizations of RoI and keeps the float number. Taking the example above, RoI Align Pooling keeps the projected region proposal size 32.1875x30.125 and its corresponding coordinates are ($x_{tl}=1.25,y_{tl}=16.6875, x_{br}=33.4375, y_{br}=46.8125$). Then its corresponding grid size is **4.598x4.303**. We assume the **sample rate is 2**, then 4 points will be sampled. For each grid, we compute the coordinates of 4 sampled points. The coordinates of top left sampled point is (1.25+(4.598/2)/2=2.3995, (16.6875+(4.303/2)/2)=17.76325), the top right sampled point is (1.25+(4.598/2)x1.5=4.6985, 17.76325), the bottom left sampled point is (1.25+(4.598/2)/2=2.3995, 16.6875+(4.303/2)x1.5=19.91475), and the bottom right samples point is (1.25+(4.598/2)x1.5=4.6985, 16.6875+(4.303/2)x1.5=19.91475). For the first sampled point, we compute the value at (2.3995,17.76325) by interpolating values at four nearest points ((2,17),(3,17),(2,18) and (3,18)) in each feature map. The computation of one sampled point can be visualized by the below figure.\n    Fig 6. Visualization of one sampled value computation in RoI Align.   where area0=(2.3995-2)x(17.76325-17)=0.304918375, area1=(3-2.3995)x(17.76325-17)=0.458331625, area2=(2.3995-2)x(18-17.76325)=0.094581625, area3=(3-2.3995)x(18-17.76325)=0.142168375.\nReference  https://zhuanlan.zhihu.com/p/61317964  9. mAP (mean average precision) in Object Detection mAP is a measurement metric for the accuracy of object detectors, and it actually computes the mean average precision values along with recall from 0 to 1. Before going deep into mAP, a few concepts should be introduced first, i.e., True Positive, True Negative, False Positive, False Negative, Precision and Recall.\nFor example, there is a classification task to distinguish whether an image contains apples, then for an image dataset:\nTrue Positive (TP): is how many images containing apples (True) and you predict them contain apples (Positive).\nTrue Negative (TN): is how many images containing apples (True) but you predict them NOT contain apples (Negative).\nFalse Positive (FP): is how many images not containing apples (False) but you predict them contain apples (Positive).\nFalse Negative (FN): is how many images not containing apples (False) and you predict them NOT contain apples (Negative).\nPrecision: is the percentage of TP among the total number of images that you predict containing apples, which is denoted in math as: $$P = \\frac{TP}{TP+FP}$$\nRecall: is the percentage of TP among how many images you predict correctly including TP and FN. Correct prediction consists of two parts: 1. you predict an image contains apples and the fact is that it indeed contains apples (this is actually TP); 2. you predict an image not contain apples and the fact is that it indeed not contain apples (this is actually FN). Thus recall is denoted as: $$R = \\frac{TP}{TP+FN}$$\n9.1 AP AP is the exact area under the precision-recall curve. In object detection, the prediction is correct if IoU $\\geqslant$ 0.5, which means the True Positive is when your prediction satisfies IoU $\\geqslant$ 0.5. Then False Positive is IoU \u0026lt; 0.5. For example, if we have a precision-recall curve (red line) like below figure, we first smooth out the zigzag pattern, then at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.\n   Fig 7. Visualization of mAP.   We first smooth out the precision zigzag pattern (recall between 0.3 and 0.6,0.6 and 0.8) by replacing maximum precision value to the right of that recall level (blue line). This happens when the FP number increases first then TP number increases. ($Precision = \\frac{TP}{TP+FP}$, FP $\\uparrow$, Precision $\\downarrow$. TP $\\uparrow$, Precision $\\uparrow$. ). Then $AP = \\frac{1}{10}(1 \\times 3 + 0.7 \\times 3 + 0.6 \\times 4)=0.75$ (green area).\nmAP is the average of AP. In some context, we compute the AP for each class then average them. But in some context, they mean the same thing. For example, in COCO context, AP is averaged over all categories, which means there is no difference between mAP and AP.\nReference https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\n10. Cross Validation 10.1 Model vs Learner Model is general a mapping function from input to output, and it could be a machine learning algorithm (e.g., Logistic regression/SVM) or deep neural network (e.g., CNN/GCN/GAN). While Learner is general a model with learned weights which is a learned model after training.\n10.2 Hyperparameters vs parameters in Model Hyperparameters are the user pre-defined parameters of a model before training (e.g., learning-rate), while parameters are the learned weights after training, for example, $w^T$ in linear regression $f(x) = w^T x + b$.\n10.3 K-fold Cross Validation Given a dataset, we first divide it into training set and testing set which is not accessible to model during training stage. Then we further divide the training set into a training part and a validate part by k-fold cross validation.\nFor example, if we choose $k=5$ then the training set will be divided into 5 parts of equal size, like part 1, part 2, part 3, part 4 and part 5. During training, we take part 1 as validate samples and the rest as true training samples, which means the model will be trained with the part 2-4 and validated by part 1. Then we choose part 2 as validate samples and the rest as true training samples, which means the model will be trained with part1,3-5 and validated by part 2. Continue, until each part has been chosen as validate samples. Thus this process is repeated $k=5$ times. After this, the model has been trained with all the samples of the training set (though it is not in once time). Then we use mean of mean square error: $$cv_{k} = \\frac{1}{k} \\sum_{i=1}^k MSE(x_i)$$ to see the cross validate score. The illustration is as follow:\n   Fig 8. A simple illustration of k-fold cross validation ($k=5$ in this case).   References https://zhuanlan.zhihu.com/p/24825503\n11 Why do Variational Autoencoders (VAE) compute KL divergence loss? Variational autoencoders aim to map the input data into a distribution (aka latent vector) instead of just a fixed vector, as once the model is learned we can generate arbitrary output by just sampling from this learnt distribution. In this way, during test time, we can discard the encoder part and just use the decoder to produce new outputs that are not seen in training set. However, in general cases, the distribution of learned latent vector could be variant to inputs in training dataset. To address this problem, we assign the learned distribution of latent vectors to be a Gaussian distribution, then we sample arbitrary $z$ from this distribution and produce new outputs by passing $z$ into the decoder. To enforce the learned distribution, we use Kullback-Leibler (KL) divergence loss to emphasize the distribution distance between learned distribution and Gaussian distribution.\n    Fig 9. Visualization of KL in VAE. Image source: this paper   During the training-time, a variational autoencoder implemented as a feed-forward neural network, where $P(X|z)$ is Gaussian. Left is without the \u0026ldquo;reparameterization trick\u0026rdquo;, and right is the with it. Red shows sampling operation is non-differentiable. Blue shows loss layers. The feed-forward behaviour of these networks is identical, but backpropagation can be applied only to the right network. This KL divergence loss is to promote Gaussian distribution in the latent space. Regularly, there should also be a regularization term (i.e., $l_2$ norm) which is used to avoid overfitting.\n    Fig 10. Visualization of KL in conditional VAE. Image source: this paper   For a conditional variational autoencoder, we hope the autoencoder could produce some plausible outputs with additional input (aka condition). For example, we want an MINST digit autoencoder could produce a digit just like a particular person\u0026rsquo;s writing or style. In figure 10, left is a training-time conditional variational autoencoder implemented as a feefforward nerual network, following the same notation as Fig 9. Right is same decoder at test time, where $X$ denotes the condition in both training and testing time.\nReference  https://arxiv.org/pdf/1606.05908.pdf https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html  ","date":1577202754,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583701436,"objectID":"096c7d801f98983face3501dd500a162","permalink":"https://Alibabade.github.io/post/basic_understanding_dl/","publishdate":"2019-12-24T15:52:34Z","relpermalink":"/post/basic_understanding_dl/","section":"post","summary":"Some discrete techniques across research areas like NLP, IR, image/video and geometry","tags":["Academic","Discrete techniques in ML/DL"],"title":"Discrete_specific_techniques_dl","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"Batch Normalization (BN) Short Description Batch Normalization is a basic method to initialize inputs to neural networks. In the early years, the neural network is sensitive to the hyperparameters, which makes it difficult to train and stabilize. To address this problem, Loffe et al. proposed a novel normalization method to accelerate the neural network trianing process.\nTraining Problems 1. Slow learning speed. The $W$ weights and $b$ bias (parameters) in each layer are updated along with each SGD iterative optimization (back propagation), which makes the distribution of inputs to the next layer changes all the time. Thus the learning speed becomes slow as each layer has to adapt to input changes.\n2. Slow convergence speed. For saturating nonlinearities like Sigmoid and Tanh, more and more inputs to activation functions may lay in the saturation regions along with the $W$ and $b$ increase. This further causes the gradient becomes close to 0 and weights update in a slow rate.\nThese problems are described as Internal Covariate Shift in Loffe et al. 2015.\nSolutions to Internal Covariate Shift 1. Whitening (machine learning). Whitening aims to linearly transform the inputs to zero mean and unit variances, which decorrelates the inputs. There are normally two whitening methods: PCA and ZCA, where PCA whitening transforms inputs to zero mean and unit variance, and ZCA whitening transforms inputs to zero mean and same variance as original inputs.\n2. Batch Normalization. Motivation: 1. whitening costs too much computation if it is put before each layer; 2. the distribution of transformed inputs does not have expressive power as original inputs.\nSolutions:\n  simplify the linearly transformation by following equations: $$ \\mu_j = \\frac{1}{m}\\Sigma_{i=1}^{m}{x_j^i}$$ $$\\sigma^2_j = \\frac{1}{m}\\Sigma_{i=1}^{m}{(x_j^i - \\mu_j)^2}$$ $$x_j^{i'} = \\frac{x_j^i - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}$$ where $j$ denotes the $j$th layer, $\\mu_j$ and $\\sigma_j^2$ denote the mean and variance of inputs $x_j$. $x_j^{i'}$ denotes the transformation output which has zero mean and unit variance, $m$ denotes the sample number in $x_i$ and $\\varepsilon$ ($\\varepsilon=10^{-8}$) prevents the zero in variance.\n  learn a linear transformation with parameter $\\gamma$ and $\\beta$ to restore original input distribution by following equation: $$x_j^{i\u0026quot;} = \\gamma_j x_j^{i'} + \\beta_j$$\nwhere the transformed output will have the same distribution of original inputs when $\\gamma_j^2$ and $\\beta_i$ equal to $\\sigma_j^2$ and $\\mu_j$.\n  Normalization in testing stage. Generally, there may be just one or a few examples to be predicted in testing stage, the mean and variance computed from such examples could be baised. To address this problem, the $\\mu_{batch}$ and $\\sigma^2_{batch}$ of each layer are stored to compute the mean and variance for testing stage. For example, $\\mu_{test} = \\frac{1}{n} \\Sigma \\mu_{batch}$ and $\\sigma^2_{batch}=\\frac{m}{m-1} \\frac{1}{n} \\Sigma \\sigma^2_{batch}$, then $BN(x_{test})=\\gamma \\frac{x_{test} - \\mu_{test}}{\\sqrt{\\sigma^2_{test} + \\varepsilon}} + \\beta$.\nAdvantages of BN  Faster learning speed due to stable input distribution. Saturating nonlinearities like Sigmoid and Tanh can still be used since gradients are prevented from disappearing. Neural network is not sensitive to parameters, simplfy tuning process and stabilize the learning process. BN partially works as regularization, increases generalization ability. The mean and variance of each mini -batch is different from each other, which may work as some noise input for the nerual network to learn. This has same function as Dropout shutdowns some neurons to produce some noise input to the neural networks.  Disadvantages of BN  NOT well for small mini-batch as the mean ans variance of small mini-batch differ great from other mini-batches which may introduce too much noise into the NN training. NOT well for recurrent neural network as one hidden state may deal with a series of inputs, and each input has different mean and variance. To remember these mean and variance, it may need more BN to store them for each input. NOT well for noise-sensitive applications such as generative models and deep reinforcement learning.  Rethink the resaon behind the effectiveness of BN Why does the BN works so well in CNNs? This paper revisits the BN and proposes that the success of BN has little to do with reducing Internal Covariate Shift. Well, the ICS does exist in the deeper neural layers, but adding artifiical ICS after BN into a deep neural network, the added ICS does not affect the good performance of BN, which indicates that the performance of BN has little to do with ICS. Then what does the BN do to improve the training performance? The work mentioned above points out that BN smoothes the optimization landscape and makes the optimizer more easier to find the global minimic solution (c.f. Figure 4 in the paper). For more details, please refer to the paper.\nWeight Normalization (WN) Short description Weight normalization is designed to address the disadvantages of BN which are that BN usually introduces too much noise when the mini-batch is small and the mean and variance of each mini-batch is correlated to inputs. It eliminates the correlations by normalizing weight parameters directly instead of mini-batch inputs.\nMotivation and methodology The core limitaion of BN is that the mean and variance is correlated to each mini-batch inputs, thus a better way to deal with that is design a normalization without the correlation. To achieve this, Salimans et al. proposed a Weight Normalization which is denoted as: $$w = \\frac{g}{||v||} v$$ where $v$ denotes the parameter vector, $||v||$ is the Euclidean norm of $v$, and $g$ is scalar. This reparameterization has the effect of fixing the Euclidean norm of weight vector $w$, and we now have $||w||=g$ which is totally independent from parameter vector $v$. This operation is similar to divide inputs by standard deviation in batch normalization.\nThe mean of neurons still depends on $v$, thus the authors proposed a \u0026lsquo;mean-only batch normalization\u0026rsquo; which only allows the inputs to subtract their mean but not divided by variance. Compared to variance divide, the mean subtraction seems to introduce less noise.\nLayer Normalization Short description Layer normalization is inspired by batch normalization but designed to small mini-batch cases and extend such technique to recurrent neural networks.\nMotivation and methodology As mentioned in short description. To achieve the goal, Hinton et al. 2016 alter the sum statistic of inputs from batch dimension to feature dimension (multiple channels). For example, in a mini-batch (containing multiple input features), the computation can be described as following picture:\n   Fig 1. Layer Normalization Visualization   As can be seen, batch normalization computes the sum statistic of inputs across the batch dimension while layer normalization does across the feature dimension. The computation is almost the same in both normalization cases, but the mean and variance of layer normalization is independent of other examples in the same mini-batch. Experiments show that layer normalization works well in RNNs.\nInstance Normalization (IN) Short description Instance normalization is like layer normalization mentioned above but it goes one step further that it computes mean and variance of each channel in each input feature. In this way, the statistic like mean and variance is independent to each channel. The IN is originally designed for neural style transfer which discovers that stylization network should be agnostic to the contrast of the style image. Thus it is usually specific to image.\nGroup Normalization (GN) Short description Group normalization computes the mean and variance across a group of channels in each training example, which makes it sound like a combination of layer normalization and instance normalization. For example, group normalization becomes layer normalization when all the channels are put into one single group, and becomes instance normalization when each channel is put into one single group. The picture below shows the visual comparisons of batch normalization, layer normalization, instance normalization and group normalization.\n   Fig 2. Normalization Visualization   Motivation Training small batch size with BN introduces noise into network which decreases the accuary. However, for larger models like object detection, segmentation and video, they have to require small batches considering the memory consumption. In addition, dependence bewteen channels widely exists but it is not extremely like all channels have dependences (layer normalization) or totally no dependence between channels (instance normalization). Based on this oberservation, He et al. 2018 proposed a group-wise normalization which divides the channels into groups and makes it flexiable for different applications.\nBatch-Instance Normalization (BIN) Short description Batch-instance normalization is actually a interpolation of BN and IN, and lets the gradient descent to learn a parameter to interploates the weight of BN and IN. The equation below shows the defination of BIN:\n$$BIN(x) = \\gamma (\\rho BN(x) + (1-\\rho) IN(x)) + \\beta$$ To some extend, this BIN inspires readers that models can learn to adaptively use different normalization methods using gradient descent. Would the network be capable of learning to use even wider range of normalization methods in one single model?\nMotivation Rethinking the instance normalization, Nam et al. 2019 regard instance normalization as an effective method to earse unnecessary style information from image and perserve useful styles for tasks like object classification, multi-domain learning and domain adaptation.\nSwitchable Normalization (SN) Luo et al. 2018 investigated into whether different layers in a CNN needs different normalization methods. Thus they proposed a Switchable Normalization which learns parameters to switch normalizers between BN, LN and IN. As for results, their experiments suggest that (1) using distinct normalizations in different layer indeed improves both learning and generation of a CNN;(2) the normalization choices are more related to depth and batch size but less relevant to parameter initialization, learning rate decay and solver;(3) different tasks and datasets influence tha normalization choices. Additionally, the experiments in general also suggest that IN works well in early layers, LN works better in later layers while BN is preferred in middle layers.\nSpectral Normalization Spectral normalization (another form of weight normalization) is designed to improve the training of GANs by tuning the Lipschitz constant of the discriminator. The Lipschitz constant is a constant $L$ used in the following equation: $$||f(x) - f(y)|| \\leqslant L ||x-y||$$\nThe Lipschitz constant is tuned by normalizing the weight matrices where is by their largest eigenvalue. And experiments show that spectral normalization stabilize the training by minimal tuning.\nConclusion BN is a millstone research on training deep neural network which makes it much easier and more robust. However, the limitations like small batch size, noise-sensitie applications and distributed training still need to be fixed in further researches. And different applications/tasks may prefer different normalizations respect to accurancy. New dimensions of normalization still need to be discovered.\nReference:  BN, https://arxiv.org/pdf/1502.03167.pdf WN, https://arxiv.org/pdf/1602.07868.pdf LN, https://arxiv.org/pdf/1607.06450.pdf IN, https://arxiv.org/pdf/1607.08022.pdf GN, https://arxiv.org/pdf/1803.08494.pdf BIN, https://arxiv.org/pdf/1805.07925.pdf SN, https://arxiv.org/pdf/1811.07727v1.pdf https://arxiv.org/pdf/1805.11604.pdf Spectral Normalization, https://arxiv.org/pdf/1802.05957.pdf https://zhuanlan.zhihu.com/p/34879333 https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/  ","date":1577100366,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578943577,"objectID":"55127274bb980c1bfb10f617e7ec7c18","permalink":"https://Alibabade.github.io/post/normalizaion_in_dl/","publishdate":"2019-12-23T11:26:06Z","relpermalink":"/post/normalizaion_in_dl/","section":"post","summary":"Some brief summary to understand normalizaion in DL","tags":["Academic","Normalization--Neual Network Optimization"],"title":"Normalization_in_DL","type":"post"},{"authors":[],"categories":[],"content":"This is the first test blog post, which aims to see how the post creation goes and how to modify the post properties like font, math, diagram and codes.\nExamples: Code Try code examples:\nimport pandas as pd import torch print(\u0026quot;hello world!\u0026quot;)  Math Try math latex block: $$L_{total} = \\alpha L_c(x, c) + \\beta L_s(x, s) + \\gamma L_{tv}$$\nDiagram Try diagram example:\ngraph TD; A--\u0026gt;B; B--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;E; D--\u0026gt;E;  ","date":1577096110,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577138488,"objectID":"89b8b7c95c9e5efc362d50816cdedfdb","permalink":"https://Alibabade.github.io/post/test_post1/","publishdate":"2019-12-23T10:15:10Z","relpermalink":"/post/test_post1/","section":"post","summary":"This is the first test blog post, which aims to see how the post creation goes and how to modify the post properties like font, math, diagram and codes.\nExamples: Code Try code examples:\nimport pandas as pd import torch print(\u0026quot;hello world!\u0026quot;)  Math Try math latex block: $$L_{total} = \\alpha L_c(x, c) + \\beta L_s(x, s) + \\gamma L_{tv}$$\nDiagram Try diagram example:\ngraph TD; A--\u0026gt;B; B--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;E; D--\u0026gt;E;  ","tags":[],"title":"Test_post1","type":"post"},{"authors":["Nan Xiang","Li Wang","Tao Jiang","Yanran Li","Xiaosong Yang","Jianjun Zhang"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"12ae3ed59759a4cf57633844292fa027","permalink":"https://Alibabade.github.io/publication/xiang-2019-single/","publishdate":"2019-12-22T22:19:13.536821Z","relpermalink":"/publication/xiang-2019-single/","section":"publication","summary":"","tags":null,"title":"Single-image Mesh Reconstruction and Pose Estimation via Generative Normal Map","type":"publication"},{"authors":["Li Wang","Nan Xiang","Xiaosong Yang","Jianjun Zhang"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"1c7bdeb1aca5067221f5d12ef0122f65","permalink":"https://Alibabade.github.io/publication/wang-2018-fast/","publishdate":"2019-12-22T22:19:13.534953Z","relpermalink":"/publication/wang-2018-fast/","section":"publication","summary":"","tags":null,"title":"Fast photographic style transfer based on convolutional neural networks","type":"publication"},{"authors":["Li Wang","Zhao Wang","Xiaosong Yang","Shi-Min Hu","Jianjun Zhang"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"6e4f26e1a5306d394de9b992d9eb244f","permalink":"https://Alibabade.github.io/publication/wang-2018-photographic/","publishdate":"2019-12-22T22:19:13.533202Z","relpermalink":"/publication/wang-2018-photographic/","section":"publication","summary":"","tags":null,"title":"Photographic style transfer","type":"publication"},{"authors":["Li Wang","Xiang-Jiu Che","Ning Wang","Jie Li","Ming-Hui Zhu"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"a3a4964cce2a06b97559077e24d02480","permalink":"https://Alibabade.github.io/publication/wang-2014-regulatory/","publishdate":"2019-12-22T22:19:13.537892Z","relpermalink":"/publication/wang-2014-regulatory/","section":"publication","summary":"","tags":null,"title":"Regulatory network analysis of microRNAs and genes in neuroblastoma","type":"publication"}]