<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Li Wang">

  
  
  
    
  
  <meta name="description" content="Brief summary of efficient Mobilenet and Shufflenet">

  
  <link rel="alternate" hreflang="en-us" href="https://Alibabade.github.io/post/fast_mobilenet_shufflenet/">

  


  
  
  
  <meta name="theme-color" content="#dc7633">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CLora:400,700%7CEB Garamond%7CRoboto+Mono%7CRoboto:400,400italic,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://Alibabade.github.io/post/fast_mobilenet_shufflenet/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Li Wang">
  <meta property="og:url" content="https://Alibabade.github.io/post/fast_mobilenet_shufflenet/">
  <meta property="og:title" content="EfficientNet_mobilenet_shufflenet | Li Wang">
  <meta property="og:description" content="Brief summary of efficient Mobilenet and Shufflenet"><meta property="og:image" content="https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-02-03T21:50:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-02-06T11:36:39&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Alibabade.github.io/post/fast_mobilenet_shufflenet/"
  },
  "headline": "EfficientNet_mobilenet_shufflenet",
  
  "datePublished": "2020-02-03T21:50:00Z",
  "dateModified": "2020-02-06T11:36:39Z",
  
  "author": {
    "@type": "Person",
    "name": "Li Wang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Li Wang",
    "logo": {
      "@type": "ImageObject",
      "url": "img/https://Alibabade.github.io/"
    }
  },
  "description": "Brief summary of efficient Mobilenet and Shufflenet"
}
</script>

  

  


  


  





  <title>EfficientNet_mobilenet_shufflenet | Li Wang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Li Wang</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Li Wang</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>EfficientNet_mobilenet_shufflenet</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Li Wang</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Feb 6, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/computer-vision/">Computer Vision</a>, <a href="/categories/dl/">DL</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="1-background">1. Background</h2>
<p>In this post, we use the following symbols for all sections:</p>
<ol>
<li>$W$: width, $H$: height</li>
<li>$N$: input channel, $M$: output channel</li>
<li>$K$: convolution kernel size.</li>
</ol>
<h3 id="11-params-for-a-neural-network">1.1 Params for a neural network</h3>
<p>Params is related to the model size, the unit is Million in float32, thus the model size is approximately 4 times of params. For a standard convolution operation, the params = $(K^2 \times N + 1)M$, without bias: $K^2 \times NM$. For a standard fully connection layer, the params = $(N+1)M$, without bias: $NM$.</p>
<h3 id="12-computation-complexity-flops">1.2 Computation complexity (FLOPs)</h3>
<p>Computational complexity (or cost) is related to speed (but indirect), and it is usually written as FLOPs. Here only multiplication-adds is considered. For a standard convolution operation, the FLOPs = $WHN \times K^2M$. For a fully connection layer, the FLOPs = $(N+1)M$, without bias: $NM$.
<strong>Here we can see that the FLOPs is nearly $WH$ times of Params for a conv operation</strong>.</p>
<h3 id="13-compute-the-params-and-flops-in-pytorch">1.3 Compute the params and FLOPs in PyTorch</h3>
<p>In pytorch, opCounter library can be used to compute the params and FLOPs of a model.
Install opCouter first:</p>
<pre><code class="language-python">pip install thop
</code></pre>
<p>For instance, computing these numbers can be done by following code:</p>
<pre><code class="language-python">from torchvision.models import resnet50
from thop import profile

model = resnet50()
input = torch.randn(1,3,224,224)
flops, params = profile(model, inputs=(input,))
</code></pre>
<h2 id="2-computational-cost-for-convolution-layers">2. Computational cost for convolution layers</h2>



  











<figure>


  <a data-fancybox="" href="/img/computation_conv.png" data-caption="Fig 1. The illustration of computational cost in a convolution operation. Image source:this blog.">
<img data-src="/img/computation_conv.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 1. The illustration of computational cost in a convolution operation. Image source:<a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d">this blog</a>.
  </figcaption>


</figure>

<p>We will check on the computation of a general convolution operation. For example, in Fig 1, the input feature has the dimensions like width $W$ $\times$ height $H$ $\times$ N (input channels), the convolution kernel has dimension like $K \times K$ (kernel size) $\times$ M (output channels) and the convolutional operation has stride=1 and padding=1 which keeps the width and height same between input and output, thus the output feature will have the dimension $W \times H \times M$. Then the <strong>multiply-add computation</strong> (standard computational cost) of a general conv operation is $WHN \times K^2 M$.</p>
<h3 id="21-computation-cost-of-conv-3-times-3-and-conv-1-times-1">2.1 Computation cost of conv $3 \times 3$ and conv $1 \times 1$</h3>
<p>Normally, the most used conv kernel in modern neural network is $3 \times 3$, which is denoted as conv $3 \times 3$. Its computational cost is $WHN3^2M$ when the convolution operates on both spatial and channel domain. If we illustrate the computation cost on spatial and channel domain, the following fig could be a better visualization. We can there is a fully connection between input channels and output channels.



  











<figure>


  <a data-fancybox="" href="/img/computation_cost1.png" data-caption="Fig 2. The illustration of computational cost for conv $3\times3$ and conv $1\times1$ operation. Image recreated from this blog.">
<img data-src="/img/computation_cost1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 2. The illustration of computational cost for conv $3\times3$ and conv $1\times1$ operation. Image recreated from <a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d">this blog</a>.
  </figcaption>


</figure>
</p>
<p>For conv $1\times1$, the spatial connect is $1\times1$ linear projection while channel projection is still fully connected, which leads to computational cost $WHN \times 1^2 M$. Compared to conv $3 \times 3$, the computation is reduced by $\frac{1}{K^3} = \frac{1}{9}$ in this case.</p>
<h3 id="22-computation-cost-of-group-convolution">2.2 Computation cost of group convolution</h3>
<p>Group convolution is firstly introduced in <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> to deal with the insufficient GPU memory and, to some extent, reduce the learned number of parameters. Grouped convolution operates on channel domain, which divides the channels into $G$ groups and the information in different groups is not shared. In each group, the connection still follows the fully connection way.
The computation costs for grouped conv $3\times3$ and conv $1\times1$ are as following:



  











<figure>


  <a data-fancybox="" href="/img/computation_cost2.png" data-caption="Fig 3. The illustration of computational cost for grouped conv $3\times3$ where $G=2$ in this case. Image recreated from this blog.">
<img data-src="/img/computation_cost2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 3. The illustration of computational cost for grouped conv $3\times3$ where $G=2$ in this case. Image recreated from <a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d">this blog</a>.
  </figcaption>


</figure>
</p>
<p>Compared to standard conv, the grouped conv $3\times3$ (where $G=2$) reduce the connection in channel domain by factor $G$, which results in $\frac{1}{G}$ times of standard conv.</p>
<h3 id="23-computation-cost-of-depthwise-convolution">2.3 Computation cost of depthwise convolution</h3>
<p>Depthwise convolution is firstly introduced in <a href="https://arxiv.org/pdf/1704.04861.pdf">MobileNet v1</a>, which performs the convolution operation independently to for each of input channels. Actually, this can be regarded as a special case of grouped conv when $G=N$. Usually, output channel $M &raquo; K^2$, thus depthwise conv significantly reduces the computational cost compared to standard conv operation.



  











<figure>


  <a data-fancybox="" href="/img/computation_cost3.png" data-caption="Fig 4. The illustration of computational cost for depthwise conv $3\times3$. Image recreated from this blog.">
<img data-src="/img/computation_cost3.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 4. The illustration of computational cost for depthwise conv $3\times3$. Image recreated from <a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d">this blog</a>.
  </figcaption>


</figure>
</p>
<h3 id="24-channel-shuffle">2.4 Channel shuffle</h3>
<p>Channel shuffle is an operation introduced in <a href="https://arxiv.org/pdf/1707.01083.pdf">ShuffleNet v1</a> to deal with the large computational cost by conv $1\times1$ in <a href="https://arxiv.org/pdf/1611.05431.pdf">ResNeXt network</a>. In this section, we basic show how channel shuffle works and introduce more details in Section 5.
The operation is first divide the input channels $N$ into $G$ groups, which results in $G \times N^{`}$ channels. Usually, the $N^{`}$ is a certain times of $G$. In this case, $N^{`}=9$ and $G=3$. Then one certain channel in a group will stay in the same group, but each of the rest channel in a group will be separately assigned to other groups. The figure below illustrates the channel shuffle operation.



  











<figure>


  <a data-fancybox="" href="/img/computation_cost4.png" data-caption="Fig 5. The illustration of channel shuffle operation. Image recreated from this blog.">
<img data-src="/img/computation_cost4.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 5. The illustration of channel shuffle operation. Image recreated from <a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d">this blog</a>.
  </figcaption>


</figure>
</p>
<h2 id="3-resnet-resnext">3. ResNet, ResNeXt</h2>
<h3 id="31-bottleneck-architecture-comparison">3.1 Bottleneck architecture comparison</h3>
<p>Basic idea in ResNeXt is to replace standard conv $3\times3$ by group conv $3\times3$.



  











<figure>


  <a data-fancybox="" href="/img/resnet_resnext.png" data-caption="Fig 6. Architecture comparison of ResNet and ResNeXt.">
<img data-src="/img/resnet_resnext.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 6. Architecture comparison of ResNet and ResNeXt.
  </figcaption>


</figure>
</p>
<h3 id="32-flops-comparison">3.2 FLOPs comparison</h3>
<p>Note that FLOPs of ResNeXt is only reduced by a small budget of computational cost and it's up to group number $G$ when $N_r=N_x$ and $M_r=M_x$.



  











<figure>


  <a data-fancybox="" href="/img/resnet_resnext_1.png" data-caption="Fig 7. FLOPs comparison of ResNet and ResNeXt.">
<img data-src="/img/resnet_resnext_1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 7. FLOPs comparison of ResNet and ResNeXt.
  </figcaption>


</figure>
</p>
<h2 id="4-mobilenet-v1-vs-v2">4. MobileNet v1 vs v2</h2>
<h3 id="41-mobilenet-v1-vgg">4.1 MobileNet v1 (VGG)</h3>
<p><strong>Key point is to replace all standard conv $3\times3$ with depthwise conv $3\times3$ + conv $1\times1$ in standard VGGNet</strong>. <a href="https://cloud.tencent.com/developer/article/1461275">This blog</a> says that the ReLU is also replaced by ReLU6 (ReLU6 = $max(max(0,x),6)$) in MobileNet v1, but the original paper does not say anything about it. Perhaps in engineering projects, people usually use ReLU6.



  











<figure>


  <a data-fancybox="" href="/img/vgg_mobilenetv1_1.png" data-caption="Fig 8. Comparison of standard conv $3\times3$ in VGG and Separable Depthwise conv $3\times4$ &#43; conv $1\times1$ in MobileNetv1. Image recreated from original paper.">
<img data-src="/img/vgg_mobilenetv1_1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 8. Comparison of standard conv $3\times3$ in VGG and Separable Depthwise conv $3\times4$ + conv $1\times1$ in MobileNetv1. Image recreated from <a href="https://arxiv.org/pdf/1704.04861.pdf">original paper</a>.
  </figcaption>


</figure>
</p>
<p>Therefore, the FLOPs is reduced about $\frac{1}{8}$ ~ $\frac{1}{9}$ in MobileNetv1 compared to VGGNet.



  











<figure>


  <a data-fancybox="" href="/img/vgg_mobilenetv1.png" data-caption="Fig 9. FLOPs Comparison of standard conv $3\times3$ in VGG and Separable Depthwise conv $3\times4$ &#43; conv $1\times1$ in MobileNetv1. Image recreated from This blog.">
<img data-src="/img/vgg_mobilenetv1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 9. FLOPs Comparison of standard conv $3\times3$ in VGG and Separable Depthwise conv $3\times4$ + conv $1\times1$ in MobileNetv1. Image recreated from <a href="https://cloud.tencent.com/developer/article/1461275">This blog</a>.
  </figcaption>


</figure>
</p>
<h3 id="42-mobilenet-v2-resnet">4.2 MobileNet v2 (ResNet)</h3>
<p><strong>Key point is to replace the last ReLU with linear bottleneck and introduce an inverted residual block in ResNet</strong>. The reason behind of replacing relu with a linear transformation is that relu causes much information loss when input dimension is low. The inverted residual block consists of a conv $1\times1$ (expanse low dimension input channels to high dimension channels) + a depthwiseconv $3\times3$ + a conv $1\times1$ (decrease high dimension input channels to low dimension (original) channels).



  











<figure>


  <a data-fancybox="" href="/img/mobilenet2_relu.png" data-caption="Fig 10. Reason behind of replacing relu with linear botthleneck. Image source from original paper.">
<img data-src="/img/mobilenet2_relu.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 10. Reason behind of replacing relu with linear botthleneck. Image source from <a href="https://arxiv.org/pdf/1801.04381.pdf">original paper</a>.
  </figcaption>


</figure>

Inverted residual block in Mobilenet v2 (see figure 9). Only the last ReLU is replaced by a linear transformation, because the input dimensions of conv $1\times1$ and depthwise conv $3\times$ are increased compared to original dimension, thus relu works fine. But when the input dimension is decreased by the last conv $1\times1$, relu will lose much information, thus we replace relu with a linear transformation to preserve as much as original information.



  











<figure>


  <a data-fancybox="" href="/img/resnet_mobilenetv2.png" data-caption="Fig 11. Comparison of standard residual block in ResNet and inverted residual block in MobileNetv2.">
<img data-src="/img/resnet_mobilenetv2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 11. Comparison of standard residual block in ResNet and inverted residual block in MobileNetv2.
  </figcaption>


</figure>

Here we compute the FLOPs for a standard residual block in ResNet and an inverted residual block in MobileNet v2. As can be seen, when the ResNet input channel $N_r$ and output channel $M_r$ is equal to MobileNet v2 input channel $N_{m2}$ and output channel $M_{m2}$, MobileNet v2 has a larger FLOPs than ResNet. However, the advantage of MobileNet v2 is that it only needs a much smaller input channel and output channel while achieves similar accuracy of ResNet, which eventually leads to smaller FLOPs than ResNet.



  











<figure>


  <a data-fancybox="" href="/img/resnet_mobilenetv2_1.png" data-caption="Fig 12. FLOPs Comparison of standard residual block in ResNet and inverted residual block in MobileNetv2.">
<img data-src="/img/resnet_mobilenetv2_1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 12. FLOPs Comparison of standard residual block in ResNet and inverted residual block in MobileNetv2.
  </figcaption>


</figure>
</p>
<h3 id="43-comparison">4.3 Comparison</h3>
<p>Here shows the convolution block in MobileNet v1 and v2, and their FLOPs comparison.



  











<figure>


  <a data-fancybox="" href="/img/mobilenet_v1-2.png" data-caption="Fig 13. Comparison of a convolution block in MobileNet v1 and two types of inverted residual block in MobileNetv2. There is no shortcut connection when stride=2 in DepthwiseConv $3\times3$ in MobileNet v2.">
<img data-src="/img/mobilenet_v1-2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 13. Comparison of a convolution block in MobileNet v1 and two types of inverted residual block in MobileNetv2. There is no shortcut connection when stride=2 in DepthwiseConv $3\times3$ in MobileNet v2.
  </figcaption>


</figure>

Note that the FLOPs for a single inverted residual block has an extra term ($N_{m2}$) as there is an extra conv $1\times1$ used compared to MobileNet v1. However, MobileNet v2 still has smaller params and FLOPs than MobileNet v1 as the input channel $N_{m2}$ and output channel $M_{m2}$ could be smaller than $N_{m1}$ and $M_{m1}$ of MobileNet v1. Please refer to Table 3 in original <a href="https://arxiv.org/pdf/1801.04381.pdf">MobileNet v2 paper</a> for more details.



  











<figure>


  <a data-fancybox="" href="/img/mobilenet_v1-2-FLOPs.png" data-caption="Fig 14. Comparison of FLOPs of a convolution block in MobileNet v1 and an inverted residual block in MobileNetv2.">
<img data-src="/img/mobilenet_v1-2-FLOPs.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 14. Comparison of FLOPs of a convolution block in MobileNet v1 and an inverted residual block in MobileNetv2.
  </figcaption>


</figure>
</p>
<p><strong>Why MobileNet v2 is not faster than MobileNet v1 on Desktop computer?</strong> On desktop, the separable depthwise convolution is not directly supported in GPU firmware (cuDNN library). While MobileNet v2 could be slightly slower than MobileNet v1 as V2 has more separable depthwise convolution operations and more larger input channels (96/192/384/768/1536) of using separable depthwise convolution than V1 (64/128/256/512/1024).</p>
<p><strong>Why MobileNet is not as fast as FLOPs indicates in practice?</strong> One reason could be the application of memory takes much time (according to some interviews).</p>
<h2 id="5-shufflenet-v1-vs-v2">5. ShuffleNet v1 vs v2</h2>
<h3 id="51-shufflenet-v1-resnext">5.1 ShuffleNet v1 (ResNeXt)</h3>
<p>ResNeXt is an efficient model for ResNet by introducing group conv $3\times3$ to reduce computational cost. However, the computational cost of conv $1\times1$ become the operation consuming most of time. To reduce the FLOPs of conv $1\times1$, <a href="https://arxiv.org/pdf/1707.01083.pdf">ShuffleNet v1</a> introduce group conv $1\times1$ tp replace the standard conv $1\times1$. However, the features won't be shared between groups by using group conv $1\times1$, which causes less feature reuse and accuracy. To address this problem, a channel shuffle operation is introduced to share features between groups. The basic blocks of ResNeXt and ShuffleNet v1 is shown in the below figure.



  











<figure>


  <a data-fancybox="" href="/img/resnext_shufflenetv1.png" data-caption="Fig 15. Comparison of a residual block in ResNeXt and ShuffleNet v1.">
<img data-src="/img/resnext_shufflenetv1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 15. Comparison of a residual block in ResNeXt and ShuffleNet v1.
  </figcaption>


</figure>
</p>
<p>Here we also give the computational cost of each method:</p>
<p>ResNeXt FLOPs = $WH(2N_r M_r + 9M_r^2/G)$</p>
<p>ShuffleNet v1 FLOPs = $WH(2N_{s1}M_{s1}/G + 9M_{s1})$, where $G$ is the group number.</p>
<p>It is obviously that ShuffleNet v1 FLOPs &lt; ResNeXt FLOPs when $N_r=N_{s1}$ and $M_r = M_{s1}$.</p>



  











<figure>


  <a data-fancybox="" href="/img/resnext_shufflenetv1_1.png" data-caption="Fig 16. FLOPs comparison of a residual block in ResNeXt and ShuffleNet v1.">
<img data-src="/img/resnext_shufflenetv1_1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 16. FLOPs comparison of a residual block in ResNeXt and ShuffleNet v1.
  </figcaption>


</figure>

<h3 id="52-shufflenet-v2">5.2 ShuffleNet v2</h3>
<p><a href="https://arxiv.org/pdf/1807.11164.pdf">ShuffleNet v2</a> points out that FLOPs is an indirect metric to evaluate computational cost of a model since the run time should also contain the <em>memory access cost (MAC)</em> , <em>degree of parallelism</em> and even <em>hardware platform</em> (e.g., GPU and ARM). Thus shufflenet v2 introduces a few rules to evaluate the computational cost of a model by considering the factors above.</p>
<h4 id="521-guidelines-for-evaluating-computational-cost">5.2.1 Guidelines for evaluating computational cost</h4>
<ul>
<li><strong>G1. MAC is minimal when input channel is equal to output channel</strong>. Let $WHN$ denote input feature, $WHM$ be output feature. Then FLOPs $F=WHNM$ when conv kernel is $1\times1$. We simply assume that input feature occupies $WHN$ memory, output feature occupies $WHM$ memory, and conv kernels occupy $NM$ memory. Then MAC can be denoted as:
\begin{eqnarray}
MAC &amp;=&amp; WHN + WHM +NM \\\<br>
&amp;=&amp; WH(N+M) + NM \\\<br>
&amp;=&amp; \sqrt{(WH)^2(N+M)^2} + \frac{F}{WH} \\\<br>
&amp;\geqslant&amp; \sqrt{(WH)^2\times 4NM} + \frac{F}{WH} \\\<br>
&amp;=&amp; \sqrt{(WH)\times 4WHNM} + \frac{F}{WH} \\\<br>
&amp;=&amp; \sqrt{(WH)\times 4F} + \frac{F}{WH} \\\<br>
\end{eqnarray}
Thus MAC achieves the minimal value when input channel $N$ is equal to output channel $M$ under same FLOPs.</li>
<li><strong>G2. MAC increases when the number of group increases</strong>. FLOPs $F=WH \times N \times M/G$. Then MAC is denoted as:
\begin{eqnarray}
MAC &amp;=&amp; WHN + WHM + \frac{NM}{G} \\\<br>
&amp;=&amp; F\times \frac{G}{M} + F \times \frac{G}{N} + \frac{F}{WH} \\\<br>
\end{eqnarray}
Thus MAC increase with the growth of $G$.</li>
<li><strong>G3. Network fragmentation reduces degree of parallelism</strong>. More fragmentation causes more computational cost in GPU. For example, under the same FLOPs, the computation efficiency is as following order: 1-fragmentation &gt; 2-fragmentation-series &gt; 2-fragmentation-parallel &gt; 4-fragmentation-series &gt; 4-fragmentation-parallel.



  











<figure>


  <a data-fancybox="" href="/img/shufflenetv2_guideline3.png" data-caption="Fig 17. Computational cost of different network fragmentations.">
<img data-src="/img/shufflenetv2_guideline3.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 17. Computational cost of different network fragmentations.
  </figcaption>


</figure>
</li>
<li><strong>G4. Element-wise operations consume much time</strong>. Except convolution operations, the element-wise operation is the second operation consuming much time.



  











<figure>


  <a data-fancybox="" href="/img/shufflenetv2_guideline4.png" data-caption="Fig 18. Computational cost of different operations.">
<img data-src="/img/shufflenetv2_guideline4.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 18. Computational cost of different operations.
  </figcaption>


</figure>
</li>
</ul>
<p>Based on the guidelines above, we can analyse that shufflenet v1 introduces group convolutions which is against G2, and if it uses bottleneck-like blocks (using conv$1\times1$ change input channels) then it is against G1. MobileNet v2 introduces an inverted residual bottleneck which is against G1. And it uses depthwise conv $3\times3$ and ReLU on expansed features and leads to more element-wise operation which violates G4. The autogenerated structures (<a href="https://arxiv.org/pdf/1802.01548.pdf">searched network</a>) add more fragmentations which violates G3.</p>
<h4 id="522-shufflenet-v2-architecture">5.2.2 ShuffleNet v2 architecture</h4>
<p>Following the guidelines in aforementioned section, shufflenet v2 introduces their new version of shufflenet.



  











<figure>


  <a data-fancybox="" href="/img/shufflenetv2.png" data-caption="Fig 19. Architecture of ShuffleNet v1 and ShuffleNet v2.">
<img data-src="/img/shufflenetv2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 19. Architecture of ShuffleNet v1 and ShuffleNet v2.
  </figcaption>


</figure>

Since the channel split, concat and channel shuffle are basically not multiplication-adds operations, the FLOPs is computed in the block inside. Note that $N_{s2}=M_{s2}=\frac{N_{s1}}{2}$, thus shuffleNet v2 is more efficient than ShuffleNet v1 ($G=4$) even under FLOPs evaluation.



  











<figure>


  <a data-fancybox="" href="/img/FLOPs_shufflenetv1_2.png" data-caption="Fig 20. FLOPs comparison of ShuffleNet v1 and ShuffleNet v2.">
<img data-src="/img/FLOPs_shufflenetv1_2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 20. FLOPs comparison of ShuffleNet v1 and ShuffleNet v2.
  </figcaption>


</figure>
</p>



  











<figure>


  <a data-fancybox="" href="/img/shufflenetv1_2.png" data-caption="Fig 21. Architecture comparison of Shufflenet v1 and v2. Image source: original paper">
<img data-src="/img/shufflenetv1_2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 21. Architecture comparison of Shufflenet v1 and v2. Image source: <a href="https://arxiv.org/pdf/1807.11164.pdf">original paper</a>
  </figcaption>


</figure>

<h3 id="53-comparison-to-other-state-of-the-art-methods">5.3 Comparison to other state-of-the-art methods</h3>



  











<figure>


  <a data-fancybox="" href="/img/shufflenetv2_comparison.png" data-caption="Fig 22. Comparison to other state-of-the-art methods. Image source: original paper">
<img data-src="/img/shufflenetv2_comparison.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 22. Comparison to other state-of-the-art methods. Image source: <a href="https://arxiv.org/pdf/1807.11164.pdf">original paper</a>
  </figcaption>


</figure>

<h4 id="523-one-more-thing">5.2.3 One more thing</h4>
<p>ShuffleNet v2 shares the similar idea with <a href="https://arxiv.org/abs/1608.06993">DenseNet</a> that is strong feature reuse, which makes ShuffleNet v2 achieve similar high accuracy as DenseNet but in a more efficient manner. More recently, a <a href="https://arxiv.org/pdf/1711.09224.pdf">CondenseNet</a> (upgraded DenseNet) points out that the more short distance features the more important they are to current layer features. Similar to Condensenet, feature map at $j$-th bottleneck building in ShuffleNet v2 reuses $\frac{c_i}{2^{j-i}}$ of feature maps at $i$-th bottleneck building, which reuses more feature maps when $j$ is more close to $i$.</p>
<h2 id="reference">Reference:</h2>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/37074222">https://zhuanlan.zhihu.com/p/37074222</a></li>
<li><a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d">https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/51566209">https://zhuanlan.zhihu.com/p/51566209</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1461275">https://cloud.tencent.com/developer/article/1461275</a></li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/academic/">Academic</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://Alibabade.github.io/post/fast_mobilenet_shufflenet/&amp;text=EfficientNet_mobilenet_shufflenet" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://Alibabade.github.io/post/fast_mobilenet_shufflenet/&amp;t=EfficientNet_mobilenet_shufflenet" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=EfficientNet_mobilenet_shufflenet&amp;body=https://Alibabade.github.io/post/fast_mobilenet_shufflenet/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://Alibabade.github.io/post/fast_mobilenet_shufflenet/&amp;title=EfficientNet_mobilenet_shufflenet" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=EfficientNet_mobilenet_shufflenet%20https://Alibabade.github.io/post/fast_mobilenet_shufflenet/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://Alibabade.github.io/post/fast_mobilenet_shufflenet/&amp;title=EfficientNet_mobilenet_shufflenet" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu8c1967adf340de7ae400ce95dd873c2d_1186948_250x250_fill_lanczos_center_2.png" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://Alibabade.github.io/">Li Wang</a></h5>
      <h6 class="card-subtitle">PhD candidate</h6>
      <p class="card-text">My research focuses on image/video based neural style transfer.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/lwang@bournemouth.ac.uk" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.facebook.com/profile.php?id=100013393752495" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=KKNBDWQAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Alibabade" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://drive.google.com/open?id=1e8VLeY3jj3NPgieuaTloNc4t8XllUQFJ" target="_blank" rel="noopener">
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/specific_math/">Specific_math</a></li>
      
      <li><a href="/project/fast_photographic_style_transfer/">Fast_photographic_style_transfer</a></li>
      
      <li><a href="/post/object_detection/">Object_detection</a></li>
      
      <li><a href="/post/training_tricks_dl/">Training_tricks_dl</a></li>
      
      <li><a href="/post/graph_nn/">Graph_NN</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.3/mermaid.min.js" integrity="" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
