<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Li Wang">

  
  
  
    
  
  <meta name="description" content="Some discrete techniques across research areas like NLP, IR, image/video and geometry">

  
  <link rel="alternate" hreflang="en-us" href="https://Alibabade.github.io/post/basic_understanding_dl/">

  


  
  
  
  <meta name="theme-color" content="#dc7633">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CLora:400,700%7CEB Garamond%7CRoboto+Mono%7CRoboto:400,400italic,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://Alibabade.github.io/post/basic_understanding_dl/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Li Wang">
  <meta property="og:url" content="https://Alibabade.github.io/post/basic_understanding_dl/">
  <meta property="og:title" content="Discrete_specific_techniques_dl | Li Wang">
  <meta property="og:description" content="Some discrete techniques across research areas like NLP, IR, image/video and geometry"><meta property="og:image" content="https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-12-24T15:52:34&#43;00:00">
    
    <meta property="article:modified_time" content="2020-01-19T14:48:40&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Alibabade.github.io/post/basic_understanding_dl/"
  },
  "headline": "Discrete_specific_techniques_dl",
  
  "datePublished": "2019-12-24T15:52:34Z",
  "dateModified": "2020-01-19T14:48:40Z",
  
  "author": {
    "@type": "Person",
    "name": "Li Wang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Li Wang",
    "logo": {
      "@type": "ImageObject",
      "url": "img/https://Alibabade.github.io/"
    }
  },
  "description": "Some discrete techniques across research areas like NLP, IR, image/video and geometry"
}
</script>

  

  


  


  





  <title>Discrete_specific_techniques_dl | Li Wang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Li Wang</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Li Wang</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Discrete_specific_techniques_dl</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Li Wang</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Jan 19, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/ml/dl/">ML/DL</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="bag-of-words-bow">Bag of words (BOW)</h2>
<p>BOW is a method to extract features from text documents, which is usually used in <strong>NLP</strong>, <strong>Information retrieve (IR) from documents</strong> and <strong>document classification</strong>. In general, BOW summarizes words in documents into a vocabulary (like dict type in python) that <strong>collects all the words in the documents along with word counts but disregarding the order they appear.</strong></p>
<p>For examples, two sentences:</p>
<pre><code class="language-python">Lei Li would like to have a lunch before he goes to watch a movie.
</code></pre>
<pre><code class="language-python">James enjoyed the movie of Star War and would like to watch it again.
</code></pre>
<p>BOW will collect all the words together to form a vocabulary like:</p>
<pre><code class="language-python">{&quot;Lei&quot;:1, &quot;Li&quot;:1, &quot;would&quot;:2, &quot;like&quot;:2, &quot;to&quot;:3, &quot;have&quot;:1, &quot;a&quot;:2, &quot;lunch&quot;:1, &quot;before&quot;:1, &quot;he&quot;:1, &quot;goes&quot;:1, &quot;watch&quot;:2, &quot;movie&quot;:2, &quot;James&quot;:1, &quot;enjoyed&quot;:1, &quot;the&quot;:1, &quot;of&quot;:1, &quot;Star&quot;:1, &quot;War&quot;:1, &quot;and&quot;:1, &quot;it&quot;:1, &quot;again&quot;:1 }
</code></pre>
<p>The length of vector represents each sentence is equal to the word number, which is 22 in our case.
Then first sentence is presented in vector (in the order of vocabulary) as: {1,1,1,1,2,1,2,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0}, and the second sentence is presented in vector as: {0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1}.</p>
<h3 id="reference">Reference</h3>
<p><a href="https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/">https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/</a></p>
<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<ol>
<li>mean: $\mu_i = \frac{1}{n} \Sigma_{i=1}^n{x_i}$</li>
<li>variance: $\sigma^2 = \frac{1}{n} \Sigma_{i=1}^n{(x_i - \mu_i)^2}$</li>
<li>standard deviation: $\sigma^2 = \frac{1}{n-1} \Sigma_{i=1}^n{(x_i - \mu_i)^2}$</li>
<li>covariance: $cov(x,y) = \frac{1}{n-1} \Sigma_{i=1}^n{(x_i-\mu_x)*(y_i -\mu_y)}$</li>
</ol>
<h2 id="cross-entropy">Cross Entropy</h2>
<h3 id="amount-of-information-that-an-event-gives">Amount of information that an event gives</h3>
<p>In general, the amount of information should be greater when an event with low probability happens. For example, event A: China won the table-tennis world champion; event B: Eygpt won the table-tennis world champion. Obviously, event B will give people more information if it happens. The reason behind this is that event A has great probability to happen while event B is rather rare, so people will get more information if event B happens.</p>
<p>The amount of information that an event gives is denoted as following equation:</p>
<p>$$f(x) = -log(p(x))$$
where $p(x)$ denotes the probability that event $x$ happens.</p>
<h3 id="entropy">Entropy</h3>
<p>For a given event $X$, there may be several possible situations/results, and each situation/result has its own probability, then the amount of information that this event gives is denoted as:
$$f(X)= -\Sigma_{i=1}^{n}p(x_i)log(p(x_i))$$<br>
where $n$ denotes the number of situations/results and $p(x_i)$ is the probability of situation/result $x_i$ happens.</p>
<h3 id="kullback-leibler-kl-divergence">Kullback-Leibler (KL) divergence</h3>
<p>The KL divergence aims to describe the difference between two probability distributions. For instance, for a given event $X$ consisting of a series events ${x_1,x_2,&hellip;,x_n}$, if there are two probability distributions of possible situations/results: $P={p(x_1),p(x_2),&hellip;,p(x_n)}$ and $Q={q(x_1),q(x_2),&hellip;,q(x_n)}$, then the KL divergence distance between $P$ and $Q$ is formulated as:</p>
<p>$$D_{KL}(P||Q) = \Sigma_{i=1}^n p(x_i)log(\frac{p(x_i)}{q(x_i)})$$
further,
$$D_{KL}(P||Q) = \Sigma_{i=1}^n p(x_i)log(p(x_i)) - \Sigma_{i=1}^n p(x_i)log(q(x_i))$$
where $Q$ is closer to $P$ when $D_{KL}(P||Q)$ is smaller.</p>
<h3 id="cross-entropy-1">Cross Entropy</h3>
<p>In machine learning or deep learning, let $y={p(x_1),p(x_2),&hellip;,p(x_n)}$ denote the groundturth probability distribution, and $\widetilde{y}={q(x_1),q(x_2),&hellip;,q(x_n)}$ present the predicted probability distribution, then KL divergence is just a good way to compute the distance between predicted distribution and groudtruth. Thus, the loss could be just formulated as:
$$Loss(y,\widetilde{y}) = \Sigma_{i=1}^n p(x_i)log(p(x_i)) - \Sigma_{i=1}^n p(x_i)log(q(x_i))$$
where the first term $\Sigma_{i=1}^n p(x_i)log(p(x_i))$ is a constant, then the $Loss(y,\widetilde{y})$ is only related to the second term $- \Sigma_{i=1}^n p(x_i)log(q(x_i))$ which is called **Cross Entropy** as a training loss.</p>
<h3 id="reference-1">Reference</h3>
<p><a href="https://blog.csdn.net/tsyccnh/article/details/79163834">https://blog.csdn.net/tsyccnh/article/details/79163834</a></p>
<h2 id="conv-1x1">Conv 1x1</h2>
<p><a href="https://arxiv.org/pdf/1409.4842.pdf">Conv 1x1</a> in Google Inception is quite useful when the filter dimension of input featues needs to be increased or decreased meanwhile keeping the spaital dimension, and reduces the convolution computation. For example, the input feautre dimension is $(B, C, H, W)$ where $B$ is batch size, $C$ is channel number, $H$ and $W$ are height and width. Using $M$ filters of Conv 1x1, then the output of Conv 1x1 is $(B,M,H,W)$, only channel number changes but spatial dimension ($H \times W$) is still the same as input. To demonstrate the computation efficiency using conv 1x1, take a look at next example. For instance, the output feature we want is $(C, H, W)$, using M filters of conv 3x3, then the compuation is $3^2C \times MHW$. Using conv 1x1, the computation is $1^2C \times MHW$, which is $\frac{1}{9}$ of using conv 3x3.</p>
<p><a href="https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/">Why do we need to decrease filter dimension or the number of feature maps?</a> The filters or the number of feature maps often increases along with the depth of the network, it is a common network design pattern. For example, the number of feature maps in VGG19, is 64,128,512 along with the depth of network. Further, some networks like Inception architecture may also concatenate the output feature maps from multiple front convolution layers, which also rapidly increases the number of feature maps to subsequent convolutional layers.</p>
<h3 id="reference-2">Reference</h3>
<p><a href="https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network">https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network</a></p>
<h2 id="iou-intersection-of-union-in-object-detection">IOU (Intersection of Union) in Object Detection</h2>



  











<figure>


  <a data-fancybox="" href="/img/IOU.png" data-caption="Fig 1. IOU Visualization in this blog.">
<img data-src="/img/IOU.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 1. IOU Visualization in <a href="https://blog.csdn.net/fendoubasaonian/article/details/78981636">this blog</a>.
  </figcaption>


</figure>

<h2 id="bounding-box-regression-in-object-detection">Bounding-box Regression in Object Detection</h2>
<p><strong>Why do we need Bounding-box Regression?</strong> In general, our object detection method predicts bounding-box for an object like blue box in below image. But the groundturth box is shown in green colour, thus we can see the bounding-box of the plane is not accurate compared to the groundtruth box as IOU is lower than 0.5 (intersection of union). If we want to get box location more close to groundtruth, then Bounding-box Regression will help us do this.</p>



  











<figure>


  <a data-fancybox="" href="/img/Bounding-box1.png" data-caption="Fig 2. Predicted box for airplane and its corresponding groudtruth in this blog.">
<img data-src="/img/Bounding-box1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 2. Predicted box for airplane and its corresponding groudtruth in <a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/2139">this blog</a>.
  </figcaption>


</figure>

<p><strong>What is Bounding-box Regression?</strong> We use $P = (P_x,P_y, P_w, P_y)$ presents the centre coordinates and width/height for the Region Proposals, which is shown as Blue window in the Fig 3. The groundtruth box is represented by $G=(G_x,G_y,G_w,G_h)$. Our aim is to find a projection function $F$ which finds a box $\hat{G}=(\hat{G}_x,\hat{G}_y,\hat{G}_w,\hat{G}_h)$ closer to $G$. In math, we need to find a $F$ which makes sure that $F(P) = \hat{G}$ and $\hat{G} \approx G$.</p>



  











<figure>


  <a data-fancybox="" href="/img/bounding_box2.png" data-caption="Fig 3. Bounding box regression in this blog.">
<img data-src="/img/bounding_box2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 3. Bounding box regression in <a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/2139">this blog</a>.
  </figcaption>


</figure>

<p><strong>How to do Bounding-box Regression in R-CNN?</strong> We want to transform $P$ to $\hat{G}$, then we need a transformation $(\Delta x, \Delta y, \Delta w, \Delta h)$ which makes the following happen:
$$\hat{G}_x = P_x + \Delta x * P_w \Rightarrow \Delta x = (\hat{G}_x - P_x)/P_w$$
$$\hat{G}_y = P_y + \Delta y * P_h \Rightarrow \Delta y = (\hat{G}_y - P_y)/P_h$$
$$\hat{G}_w = P_w e^{\Delta w} \Rightarrow \Delta w = log(\hat{G}_w/P_w)$$
$$\hat{G}_h = P_h e^{\Delta h} \Rightarrow \Delta h = log(\hat{G}_h/P_h)$$</p>
<p>While the groundtruth $(\Delta t_x, \Delta t_y, \Delta t_w, \Delta t_h)$ is defined as:
$$\Delta t_x = (G_x - P_x)/P_w$$
$$\Delta t_y = (G_y - P_y)/P_h$$
$$\Delta t_w = log(G_w/P_w)$$
$$\Delta t_h = log(G_h/P_h)$$
Next, we denote $W_i \Phi(P_i)$ where $i \in {x,y,w,h}$ as learned transformation through the neural network, then the loss function is to minimize the L2 distance between $\Delta t_i$ and $W_i \Phi(P_i)$ where $i \in {x,y,w,h}$ by SGD:
$$L_{reg} = \sum_{i}^{N} (\Delta t_i - W_i \Phi(P_i))^2 + \lambda ||W||^2$$</p>
<h2 id="upsampling-deconvolution-and-unpooling">Upsampling, Deconvolution and Unpooling</h2>
<p><strong>Upsampling</strong>: upsample any image to higher resolution. It uses <strong>upsample</strong> and <strong>interpolation</strong>.</p>
<p><strong><a href="https://www.quora.com/How-do-fully-convolutional-networks-upsample-their-coarse-output">Deconvolution</a></strong>: also called transpose convolution. For example, your input for deconvolution layer is 4x4, deconvolution layer multiplies one point in the input with a 3x3 weighted kernel and place the 3x3 results in the output image. Where the outputs overlap you sum them. Often you would use a stride larger than 1 to increase the overlap points where you sum them up, which adds upsampling effect (see blue points). The upsampling kernels can be learned just like normal convolutional kernels.



  











<figure>


  <a data-fancybox="" href="/img/deconvolution_stride1.gif" data-caption="Fig 4. Visualization of Deconvolution in this Quora answer.">
<img data-src="/img/deconvolution_stride1.gif" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 4. Visualization of Deconvolution in <a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/2139">this Quora answer</a>.
  </figcaption>


</figure>
</p>
<p><strong><a href="https://arxiv.org/pdf/1311.2901v3.pdf">Unpooling</a></strong>: We use an unpooling layer to approximately simulate the inverse of max pooling since max pooling is non-invertible. The unpooling operates on following steps: 1. record the maxima positions of each pooling region as a set of switch variables; 2. place the maxima back to the their original positions according to switch variables; 3. reset all values on non-maxima positions to $0$. This may cause some information loss.</p>
<h3 id="references">Reference:s</h3>
<ol>
<li><a href="https://www.quora.com/What-is-the-difference-between-Deconvolution-Upsampling-Unpooling-and-Convolutional-Sparse-Coding">https://www.quora.com/What-is-the-difference-between-Deconvolution-Upsampling-Unpooling-and-Convolutional-Sparse-Coding</a></li>
</ol>
<h2 id="roi-pooling-in-object-detection-fast-rcnn">RoI Pooling in Object Detection (Fast RCNN)</h2>
<p><a href="https://arxiv.org/pdf/1504.08083.pdf">RoI pooling</a> is simple version of Spatial Pyramid Pooling (multiple division scales, i.e., divide the entire feature maps into (1,4,16) patches/grids), which has only one scale division. For example, the original input image size is (1056x640) and one region proposal ($x_1=0, y_1=80,x_2=245, y_2=498$), after convolutional layers and pooling layers, the feature map size is (66x40), then we should rescale the proposal from ($x_1=0, y_1=80,x_2=245, y_2=498$) to ($x_1=0, y_1=5,x_2=15, y_2=31$) as the scale is 16 (1056/66=16 and 640/40=16). Then we divide the proposal on feature map into 7x7 sections/grids (the proposal size is no need of 7 times) if the output size is 7x7. Next we operate max pooling on each grid, and place the maxima into output 7x7. Here is another simple example below, the input feature size is 8x8, proposal is (0,3,7,8), and the output size is 2x2 thus divide the proposal region into 2x2 sections:</p>



  











<figure>


  <a data-fancybox="" href="/img/roi_visualization.gif" data-caption="Fig 5. Visualization of ROI pooling in this blog.">
<img data-src="/img/roi_visualization.gif" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 5. Visualization of ROI pooling in <a href="https://towardsdatascience.com/region-of-interest-pooling-f7c637f409af">this blog</a>.
  </figcaption>


</figure>

<h3 id="reference-3">Reference</h3>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/73654026">https://zhuanlan.zhihu.com/p/73654026</a>ß</li>
</ol>
<h2 id="roialign-pooling-in-object-detection">RoIAlign Pooling in Object Detection</h2>
<p>RoI align Pooling is proposed in <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask RCNN</a> to address the problem that RoI pooling causes misalignments by rounding quantization.</p>
<p><strong>Problem of RoI pooling.</strong> There are twice misalignments for each RoI pooling operation. For example, the size of original input image is 800x800, and one region proposal is 515x482 and its corresponding coordinates are ($x_{tl}=20, y_{tl}=267, x_{br}=535, y_{br}=749$) where &lsquo;tl&rsquo; means top left and &lsquo;br&rsquo; means bottom right. And the stride of last conv layer is **16** which means each feature map extracted from the last conv layer is **50x50** (800/16). If the output size is fixed to 7x7, then RoI pooling would quantize (by floor operation) the region proposal to **32x30** and its corresponding coordinates to ($x_{tl}=1,y_{tl}=16, x_{br}=33, y_{br}=46$). The twice misalignments in each feature map are visualized in the below figure (acutal coordinates in blue colour).</p>



  











<figure>


  <a data-fancybox="" href="/img/RoI_misalignments.png" data-caption="Fig 7. Visualization of RoI quantization/misaligments and RoIAlign corrections.">
<img data-src="/img/RoI_misalignments.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 7. Visualization of RoI quantization/misaligments and RoIAlign corrections.
  </figcaption>


</figure>

<p><strong>Solution.</strong> RoI Align removes all the quantizations of RoI and keeps the float number. Taking the example above, RoI Align Pooling keeps the projected region proposal size <strong>32.1875x30.125</strong> and its corresponding coordinates are ($x_{tl}=1.25,y_{tl}=16.6875, x_{br}=33.4375, y_{br}=46.8125$). Then its corresponding grid size is **4.598x4.303**. We assume the **sample rate is 2**, then 4 points will be sampled. For each grid, we compute the coordinates of 4 sampled points. The coordinates of top left sampled point is (1.25+(4.598/2)/2=2.3995, (16.6875+(4.303/2)/2)=17.76325), the top right sampled point is (1.25+(4.598/2)x1.5=4.6985, 17.76325), the bottom left sampled point is (1.25+(4.598/2)/2=2.3995, 16.6875+(4.303/2)x1.5=19.91475), and the bottom right samples point is (1.25+(4.598/2)x1.5=4.6985, 16.6875+(4.303/2)x1.5=19.91475). For the first sampled point, we compute the value at (2.3995,17.76325) by interpolating values at four nearest points ((2,17),(3,17),(2,18) and (3,18)) in each feature map. The computation of one sampled point can be visualized by the below figure.</p>
<p>


  











<figure>


  <a data-fancybox="" href="/img/RoIAlign_computation2.png" data-caption="Fig 6. Visualization of one sampled value computation in RoI Align.">
<img data-src="/img/RoIAlign_computation2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 6. Visualization of one sampled value computation in RoI Align.
  </figcaption>


</figure>

where area0=(2.3995-2)x(17.76325-17)=0.304918375, area1=(3-2.3995)x(17.76325-17)=0.458331625, area2=(2.3995-2)x(18-17.76325)=0.094581625, area3=(3-2.3995)x(18-17.76325)=0.142168375.</p>
<h3 id="reference-4">Reference</h3>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/61317964">https://zhuanlan.zhihu.com/p/61317964</a></li>
</ol>
<h2 id="map-mean-average-precision-in-object-detection">mAP (mean average precision) in Object Detection</h2>
<p>mAP is a measurement metric for the accuracy of object detectors, and it actually computes the mean average precision values along with recall from 0 to 1. Before going deep into mAP, a few concepts should be introduced first, i.e., True Positive, True Negative, False Positive, False Negative, Precision and Recall.</p>
<p>For example, there is a classification task to distinguish whether an image contains apples, then for an image dataset:</p>
<p><strong>True Positive (TP)</strong>: is how many images containing apples (True) and you predict them contain apples (Positive).</p>
<p><strong>True Negative (TN)</strong>: is how many images containing apples (True) but you predict them <strong>NOT</strong> contain apples (Negative).</p>
<p><strong>False Positive (FP)</strong>: is how many images not containing apples (False) but you predict them contain apples (Positive).</p>
<p><strong>False Negative (FN)</strong>: is how many images not containing apples (False) and you predict them <strong>NOT</strong> contain apples (Negative).</p>
<p><strong>Precision</strong>: is the percentage of TP among the total number of images that you predict containing apples, which is denoted in math as:
$$P = \frac{TP}{TP+FP}$$</p>
<p><strong>Recall</strong>: is the percentage of TP among how many images you predict correctly including TP and FN. Correct prediction consists of two parts: 1. you predict an image contains apples and the fact is that it indeed contains apples (this is actually TP); 2. you predict an image not contain apples and the fact is that it indeed not contain apples (this is actually FN). Thus recall is denoted as:
$$R = \frac{TP}{TP+FN}$$</p>
<h3 id="ap">AP</h3>
<p>AP is the <strong>exact area under the precision-recall curve</strong>. In object detection, the prediction is correct if IoU $\geqslant$ 0.5, which means the True Positive is when your prediction satisfies IoU $\geqslant$ 0.5. Then False Positive is IoU &lt; 0.5. For example, if we have a precision-recall curve (red line) like below figure, we first smooth out the zigzag pattern, then at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.</p>



  











<figure>


  <a data-fancybox="" href="/img/mAP.png" data-caption="Fig 7. Visualization of mAP.">
<img data-src="/img/mAP.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 7. Visualization of mAP.
  </figcaption>


</figure>

<p>We first smooth out the precision zigzag pattern (recall between 0.3 and 0.6,0.6 and 0.8) by replacing maximum precision value to the right of that recall level (blue line). This happens when the FP number increases first then TP number increases. ($Precision = \frac{TP}{TP+FP}$, FP $\uparrow$, Precision $\downarrow$. TP $\uparrow$, Precision $\uparrow$. ). Then $AP = \frac{1}{10}(1 \times 3 + 0.7 \times 3 + 0.6 \times 4)=0.75$ (green area).</p>
<p><strong>mAP is the average of AP</strong>. In some context, we compute the AP for each class then average them. But in some context, they mean the same thing. For example, in <strong>COCO context</strong>, AP is averaged over all categories, which means <strong>there is no difference between mAP and AP</strong>.</p>
<h3 id="reference-5">Reference</h3>
<p><a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173</a></p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/academic/">Academic</a>
  
  <a class="badge badge-light" href="/tags/discrete-techniques-in-ml/dl/">Discrete techniques in ML/DL</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://Alibabade.github.io/post/basic_understanding_dl/&amp;text=Discrete_specific_techniques_dl" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://Alibabade.github.io/post/basic_understanding_dl/&amp;t=Discrete_specific_techniques_dl" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Discrete_specific_techniques_dl&amp;body=https://Alibabade.github.io/post/basic_understanding_dl/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://Alibabade.github.io/post/basic_understanding_dl/&amp;title=Discrete_specific_techniques_dl" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Discrete_specific_techniques_dl%20https://Alibabade.github.io/post/basic_understanding_dl/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://Alibabade.github.io/post/basic_understanding_dl/&amp;title=Discrete_specific_techniques_dl" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu8c1967adf340de7ae400ce95dd873c2d_1186948_250x250_fill_lanczos_center_2.png" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://Alibabade.github.io/">Li Wang</a></h5>
      <h6 class="card-subtitle">PhD candidate</h6>
      <p class="card-text">My research focuses on image/video based neural style transfer.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/lwang@bournemouth.ac.uk" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.facebook.com/profile.php?id=100013393752495" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=KKNBDWQAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Alibabade" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://drive.google.com/open?id=1Y9vBdN0x6iNrVob4fP4pBo_AcZTI6Y4G" target="_blank" rel="noopener">
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/normalizaion_in_dl/">Normalization_in_DL</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.3/mermaid.min.js" integrity="" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
