<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Li Wang">

  
  
  
    
  
  <meta name="description" content="Summary of object detection in DL">

  
  <link rel="alternate" hreflang="en-us" href="https://Alibabade.github.io/post/object_detection/">

  


  
  
  
  <meta name="theme-color" content="#dc7633">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CLora:400,700%7CEB Garamond%7CRoboto+Mono%7CRoboto:400,400italic,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://Alibabade.github.io/post/object_detection/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Li Wang">
  <meta property="og:url" content="https://Alibabade.github.io/post/object_detection/">
  <meta property="og:title" content="Object_detection | Li Wang">
  <meta property="og:description" content="Summary of object detection in DL"><meta property="og:image" content="https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://Alibabade.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-01-09T12:21:28&#43;00:00">
    
    <meta property="article:modified_time" content="2020-01-23T18:57:27&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Alibabade.github.io/post/object_detection/"
  },
  "headline": "Object_detection",
  
  "datePublished": "2020-01-09T12:21:28Z",
  "dateModified": "2020-01-23T18:57:27Z",
  
  "author": {
    "@type": "Person",
    "name": "Li Wang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Li Wang",
    "logo": {
      "@type": "ImageObject",
      "url": "img/https://Alibabade.github.io/"
    }
  },
  "description": "Summary of object detection in DL"
}
</script>

  

  


  


  





  <title>Object_detection | Li Wang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Li Wang</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Li Wang</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Object_detection</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Li Wang</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Jan 23, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    29 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/computer-vision/">Computer Vision</a>, <a href="/categories/dl/">DL</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>This blog contains four parts:</p>
<ol>
<li>Introduction: What is Object Detection? and general thoughts/ideas to deal with Object Detection;</li>
<li>Classic Deep Learning based Methods: multi-stage :RCNN and SPP Net , two-stage: Fast RCNN, Faster RCNN, Mask RCNN;</li>
<li>Classic One-Stage Methods: YOLOv1-v3, SSD, RetinaNet;</li>
<li>More Recent Anchor-Free Object Detection Methods (2018-2019);</li>
</ol>
<h2 id="1-introduction">1. Introduction</h2>
<p><strong>What is Object Detection?</strong> Given an image, object detection aims to find the categories of objects contained and their corresponding locations (presented as bounding-boxes) in the image. Thus Object Detection contains two tasks: classification and localization.</p>
<p><strong>General thoughts/ideas to detect objects.</strong> The classification has been done by CNNs like AlexNet, VGG and ResNet. Then only localization still needs to be done. There are two intuitive ways: 1. Regression: the location of an object is presented by a vector $(x,y,w,h)$ which are the centre coordinates and width/height of an object bounding-box in the given image. For example, given an image, it only contains one object&mdash;cat. To locate the bounding-box, we apply a CNN to predict the vector $(x_p,y_p,w_p,h_p)$ and learn to regress the predicted vector to be close to groundtruth $(x_t,y_t,w_t,h_t)$ by calculating the L2 loss (see Fig 1.).</p>



  











<figure>


  <a data-fancybox="" href="/img/object_detection_regression1.png" data-caption="Fig 1. Regression for localization shown in this blog.">
<img data-src="/img/object_detection_regression1.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 1. Regression for localization shown in <a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">this blog</a>.
  </figcaption>


</figure>

<p>If the initial bounding-box is <strong>randomly chosen</strong> or <strong>there are many objects</strong>, then the entire regression will be much difficult and take lots of training time to correct the predicted vector to groundtruth. Sometime, it may not achieve a good convergence. However, if we select approximately initial box coordinates which probably contains the objects, then the regression of these boxes should be much easier and faster as these initial boxes already have closer coordinates to groundtruth than random ones. Thus, we could divide the problem into Box Selection and Regression, which are called <strong>Region Proposal Selection</strong> and <strong>Bounding-box Regression</strong>(please go post Basic_understanding_dl if you do not know Bounding-box regression) in Object Detection, respectively. Based on this, the early Object Detection methods contain multi-stage tasks like: Region Proposal Selection, Classification and Bounding-box Regression. In this blog, we only focus on the DL-based techniques, thus We do not review any pre-DL methods here.</p>
<p>There are a few candidate Region Proposal Selection methods (shown in below), and some of them are able to select fewer proposals (nearly hundreds or thousands) and keep high recall.



  











<figure>


  <a data-fancybox="" href="/img/Region_Proposal_Selections.png" data-caption="Fig 2. Comparisons between different Region Proposal Selection methods shown in this blog.">
<img data-src="/img/Region_Proposal_Selections.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 2. Comparisons between different Region Proposal Selection methods shown in <a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">this blog</a>.
  </figcaption>


</figure>
</p>
<h2 id="2-classic-deep-learning-based-methods">2. Classic Deep Learning based Methods</h2>
<p>Since using Region Proposal Selection can reduce bounding-box candidates from almost infinite to ~2k for one image with multiple objects, <a href="https://arxiv.org/pdf/1311.2524.pdf">Ross et al. 2014</a> propose the first CNN-based Object Detection method, which uses CNN to extract features of images, classifies the categories and regress bounding-box based on the CNN features.</p>
<h3 id="21-r-cnn-region-cnn">2.1 R-CNN (Region CNN)</h3>
<p>The basic procedure of R-CNN model:</p>
<ol>
<li>Use <strong>Selective Search</strong> to select ~2k Region Proposals for one image.</li>
<li><strong>Warp</strong> all the Region Proposals into a <strong>same size</strong> as the fully connection layers in their backbone neural network (i.e., AlexNet) has image size limitation. For example, the FC layers only take 21x21xC feature vector as input, then all the input image size has to be 227x227 if all the Conv + BN + relu layers of a pre-trained AlexNet are preserved.</li>
<li>Feed the Region Proposals into the pre-trained AlexNet at <strong>each proposal per time</strong> rate, and extract the CNN features from FC7 layer for further <strong>classification</strong> (i.e., SVM).</li>
<li>The extracted CNN features will also be used for <strong>Bounding-box Regression</strong>.</li>
</ol>
<p>Based on the procedure above, there are twice fine-tuning:</p>
<ol>
<li>Fine-tune the pre-trained CNN for classification. For example, the pre-trained CNN (i.e., AlexNet) may have 1000 categories, but we may only need it to classify ~20 categories, thus we need to fine-tune the neural network.</li>
<li>Fine-tune the pre-trained CNN for bounding-box regression. For example, we add a regression head behind the FC7 layer, and we need to fine-tune the network for bounding-box regression task.</li>
</ol>
<h4 id="211-some-common-tricks-used">2.1.1 Some common tricks used</h4>
<p><strong>1. Non-Maximum Suppression</strong></p>
<p>Commonly, sometimes the RCNN model outputs multiple bounding-boxes to localize the same object in the given image. To choose the best matching one, we use non-maximum suppression technique to avoid repeated detection of the same instance. For example, we have a set $B$ of candidate boxes, and a set $S$ of corresponding scores, then we choose the best box by following steps: 1. sort all the boxes with the scores, and remove the box $M$ with highest score from $B$, and add to set $D$; 2. check any box $b_i$ left in $B$, if the IoU of $b_i$ and $M$, remove $b_i$ from $B$; 3. repeat 1-2 until $B$ is empty. The box in $D$ is what we want.   <br>



  











<figure>


  <a data-fancybox="" href="/img/non-max-suppression.png" data-caption="Fig 3. Non-maximum suppression used in RCNN this blog.">
<img data-src="/img/non-max-suppression.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 3. Non-maximum suppression used in RCNN <a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">this blog</a>.
  </figcaption>


</figure>
</p>
<p><strong>2. Hard Negative Mining</strong></p>
<p>Bounding-box containing no objects (i.e., cat or dog) are considered as negative samples. However, not all of them are equally hard to be identified. For example, some samples purely holding background are &ldquo;easily negative&rdquo; as they are easily distinguished. However, some negative samples may hold other textures or part of objects which makes it more difficult to identify. These samples are likely  &ldquo;Hard Negative&rdquo;.</p>
<p>These &ldquo;Hard Negative&rdquo; are difficult to be correctly classified. What we can do about it is to find explicitly those false positive samples during training loops and add them into the training data in order to improve the classifier.</p>
<h4 id="212-problems-of-rcnn">2.1.2 Problems of RCNN</h4>
<p>RCNN extracts CNN features for each region proposal by feeding each of them into CNN once at a time, and the proposals selected by Selective Search are approximately 2k for each image, thus this process consumes much time. Adding pre-processing Selective Search, RCNN needs ~47 second per image.</p>
<h3 id="22-spp-net-spatial-pyramid-pooling-network">2.2 SPP Net (Spatial Pyramid Pooling Network)</h3>
<p>To speedup RCNN, SPPNet focuses on how to fix the problem that each proposal is fed into the CNN once a time. The reason behind the problem is the fully connected layers need fixed feature size (i.e., 1 x 21 x 256 in <a href="https://arxiv.org/pdf/1406.4729.pdf">He et al.,2014</a>) for further classification and regression. Thus SPPNet comes up with an idea that an additional pooling layer called spatial pyramid pooling is inserted right after the last Conv layer and before the Fc layers. The operation of this pooling first projects the region proposals to the Conv features, then divides each feature map (i.e., 60 x 40 x 256 filters) from the last Conv layer into 3 patch scales (i.e., 1,4 and 16 patches, see Fig 4. For example, the patch size is: 60x40 for 1 patch, 30x20 for 4 patches and 15x10 for 16 patches, next operates max pooling on each scaled patch to obtain a 1 x 21(1+4+16) for each feature map, thus we get 1x21x256 fiexd vector for Fc layers.</p>



  











<figure>


  <a data-fancybox="" href="/img/SPPNet_spatial_pyramid_pooling_layer.png" data-caption="Fig 4. The spatial pyramid pooling layer in SPPNet.">
<img data-src="/img/SPPNet_spatial_pyramid_pooling_layer.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 4. The spatial pyramid pooling layer in <a href="https://arxiv.org/pdf/1406.4729.pdf">SPPNet</a>.
  </figcaption>


</figure>

<p>By proposing spatial pyramid pooling layer, SPPNet is able to reuse the feature maps extracted from CNN by passing the image once through because all information that region proposals need is shared in these feature maps. The only thing we could do next is project the region proposals selected by Selective Search onto these feature maps (<strong>How to project Region Proposals to feature maps? Please go to basic_understanding post for ROI pooling.</strong>). This operation extremely saves time consumption compared to extract feature maps per proposal per forward (like RCNN does). The total speedup of SPPNet is about 100 times compared to RCNN.</p>
<h3 id="23-fast-rcnn">2.3 Fast RCNN</h3>



  











<figure>


  <a data-fancybox="" href="/img/fast_rcnn2.png" data-caption="Fig 6. The pipeline of Fast RCNN in this blog.">
<img data-src="/img/fast_rcnn2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 6. The pipeline of Fast RCNN in <a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">this blog</a>.
  </figcaption>


</figure>

<p><a href="https://arxiv.org/pdf/1504.08083.pdf">Fast RCNN</a> attempts to overcome three notable <strong>drawbacks</strong> of RCNN:</p>
<ol>
<li><strong>Training a multi-stage pipeline</strong>: fine-tune a ConvNet based on Region Proposals; train SVM classifiers with Conv Features; train bounding-box regressors.</li>
<li><strong>Training is expensive in space and time</strong>: 2.5 GPU-days for 5k images and hundreds of gigabytes of storage.</li>
<li><strong>Speed is slow</strong>: ~47 second per image even on GPU.</li>
</ol>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Combine both classification (replace SVM with softmax) and bounding-box regression into one network with multi-task loss.</strong></li>
<li><strong>Introduce ROI pooling for: 1. reuse Conv feature maps of one image; 2. speedup both training and testing.</strong> Using VGG16 as backbone network, ROI (Region of Interest) pooling converts all different sizes of region proposals into 7x7x512 feature vector fed into Fc layers. Please go to post <strong>basic_understanding_dl</strong> for more details about ROI pooling.</li>
</ol>



  











<figure>


  <a data-fancybox="" href="/img/speed_rcnn_fastrcnn.png" data-caption="Fig 6. Speed comparison between RCNN and Fast RCNN in this blog.">
<img data-src="/img/speed_rcnn_fastrcnn.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 6. Speed comparison between RCNN and Fast RCNN in <a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">this blog</a>.
  </figcaption>


</figure>

<h4 id="multi-task-loss-for-classification-and-bounding-box-regression">Multi-task Loss for Classification and Bounding-box Regression</h4>
<p>$&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;$</p>
<p><strong>Symbol Explanation</strong></p>
<p>$u$       Groundtruth class label, $u \in 0,1,&hellip;,K$; To simplify, all background class has $u=0$.</p>
<p>$v$       Groundtruth bounding-box regression target, $v=(v_x,v_y,v_w,v_h)$.</p>
<p>$p$       Descrete probability distribtion (per RoI), $p=(p_0,p_1,&hellip;,p_K)$ over $K+1$ categories. $p$ is computed by a softmax over the $K+1$ outputs of a fully connected layer.</p>
<p>$t^u$     Predicted bounding-box vector, $t^u=(t_x^u,t_y^u,t_w^u,t_h^u)$.</p>
<p>$&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;$</p>
<p>The multi-task loss on each RoI is defined as:</p>
<p>$$L(p,u,t^u,v) = L_{cls}(p,u) + \lambda[u \geqslant 1]L_{loc}(t^u,v)$$
where $L_{cls}(p,u)$=-log$p_u$ is a log loss for groundtruth class $u$. The Iverson bracket indicator function $[u \geqslant 1]$ is 1 when $u \geqslant 1$ (the predicted class is not background), and is 0 otherwise. The $L_{loc}$ term is using smooth L1 loss, which is denoted as:
$$L_{loc}(t^u,v)=\sum_{i \in {x,y,w,h}}smooth_{L_1}(t_i^u-v_i)$$ and
\begin{equation}
smooth_{L_1}(x) = \begin{cases}
0.5x^2, \ |x| &lt; 1 \newline
|x|-0.5, \ otherwise
\end{cases}
\end{equation}
$smooth_{L_1}(x)$ is a robust $L_1$ loss that is less sensitive to outliers than $L_2$ loss.</p>
<h3 id="24-faster-rcnn">2.4 Faster RCNN</h3>



  











<figure>


  <a data-fancybox="" href="/img/faster_rcnn.png" data-caption="Fig 7. The pipeline of Faster RCNN in this blog.">
<img data-src="/img/faster_rcnn.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 7. The pipeline of Faster RCNN in <a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">this blog</a>.
  </figcaption>


</figure>

<p><a href="https://arxiv.org/pdf/1506.01497.pdf">Faster RCNN</a> focuses on solving the speed bottleneck of Region Proposal Selection as previous RCNN and Fast RCNN separately compute the region proposal by Selective Search on CPU which still consumes much time. To address this problem, a novel subnetwork called RPN (Region Proposal Network) is proposed to combine Region Proposal Selection into ConvNet along with Softmax classifiers and Bounding-box regressors.</p>



  











<figure>


  <a data-fancybox="" href="/img/RPN_mechanism.png" data-caption="Fig 8. The pipeline of RPN in the paper.">
<img data-src="/img/RPN_mechanism.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 8. The pipeline of RPN in <a href="https://arxiv.org/pdf/1506.01497.pdf">the paper</a>.
  </figcaption>


</figure>

<p>To adapt the multi-scale scheme of region proposals, the RPN introduces an anchor box. Specifically, RPN has a classifier and a regressor. The classifier is to predict the probability of a proposal holding an object, and the regressor is to correct the proposal coordinates. Anchor is the centre point of the sliding window. For any image, scale and aspect-ratio are two import factors, where scale is the image size and aspect-ratio is width/height. <a href="https://arxiv.org/pdf/1506.01497.pdf">Ren et al., 2015</a> introduce 9 kinds of anchors, which are scales (1,2,3) and aspect-ratio(1:1,1:2,2:1). Then for the whole image, the number of anchors is $W \times H \times 9$ where $W$ and $H$ are width and height, respectively.</p>
<p>$&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-$</p>
<p><strong>Symbol  Explanation</strong></p>
<p><strong>$i$</strong>         the index of an anchor in a mini-batch.</p>
<p><strong>$p_i$</strong>       the probability that the anchor $i$ being an object.</p>
<p><strong>$p_i^{*}$</strong>  the groundtruth label $p_i^{*}$ is 1 if the anchor is positive, and is 0 if the anchor is negative.</p>
<p><strong>$t_i$</strong>       a vector $(x,y,w,h)$ representing the coordinates of predicted bounding box.</p>
<p><strong>$t_i^{*}$</strong>  that of the groundtruth box associated with a positive anchor.</p>
<p>$&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-$</p>
<p>The RPN also has a multi-task loss just like in Fast RCNN, which is defined as:</p>
<p>$$L({p_i},{t_i}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i,p_i^{*}) + \lambda \frac{1}{N_{reg}} \sum_i p_i^{*} L_{reg}(t_i, t_i^{*})$$
where the classification $L_{cls}$ is log loss over two classes (object vs not object). The regression loss $L_{reg}(p_i, p_i^{*}) = smooth_{L1}(t_i - t_i^{*})$. The term $p_i^{*} L_{reg}(t_i, t_i^{*})$ means the regression loss is activated if $p_i^{*}=1$ and is disabled if $p_i^{*}=0$. These two loss term are normalized by $N_{cls}$ and $N_{reg}$ and weighted by a balancing parameter $\lambda$. In implementation, $N_{cls}$ is the number of images in a mini-batch (i.e., $N_{cls}=256$), and the $reg$ term is normalized by the number of anchor locations (i.e., $N_{reg} \sim 2,400$). By default, the $\lambda$ is set to 10.</p>
<p>Therefore, there are four loss functions in one neural network:</p>
<ol>
<li>one is for classifying whether an anchor contains an object or not (anchor good or bad in RPN);</li>
<li>one is for proposal bounding box regression (anchor -&gt; groundtruth proposal in RPN);</li>
<li>one is for classifying which category that the object belongs to (over all classes in main network);</li>
<li>one is for bounding box regression (proposal -&gt; groundtruth bounding-box in main network);</li>
</ol>
<p>The total speedup comparison between RCNN, Fast RCNN and Faster RCNN is shown below:



  











<figure>


  <a data-fancybox="" href="/img/comparison_speedup_rcnn_fastrcnn_fasterrcnn.png" data-caption="Fig 9. The speedup comparison between RCNN, Fast RCNN and Faster RCNN in this blog.">
<img data-src="/img/comparison_speedup_rcnn_fastrcnn_fasterrcnn.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 9. The speedup comparison between RCNN, Fast RCNN and Faster RCNN in <a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">this blog</a>.
  </figcaption>


</figure>
</p>
<h3 id="25-mask-rcnn">2.5 Mask RCNN</h3>



  











<figure>


  <a data-fancybox="" href="/img/mask_rcnn.png" data-caption="Fig 10. The pipeline of Mask RCNN, which is Faster RCNN &#43; Instance Segmentation &#43; improved RoIAlign Pooling.">
<img data-src="/img/mask_rcnn.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 10. The pipeline of Mask RCNN, which is Faster RCNN + Instance Segmentation + improved RoIAlign Pooling.
  </figcaption>


</figure>

<p><a href="https://arxiv.org/pdf/1703.06870.pdf">Mask RCNN</a> has three branches: RPN for region proposal + (a pretrained CNN + Headers for classification and bounding-box regression) + Mask Network for pixel-level instance segmentation. Mask RCNN is developed on Faster RCNN and adds RoIAlign Pooling and instance segmentation to output object masks in a pixel-to-pixel manner. The RoIAlign is proposed to improve RoI for pixel-level segmentation as it requires much more fine-grained alignment than Bounding-boxes. The accurate computation of RoIAlign is described in RoIAlign Pooling for Object Detection in Basic_understanding_dl post.</p>



  











<figure>


  <a data-fancybox="" href="/img/mask_rcnn_results.png" data-caption="Fig 11. Mask RCNN results on the COCO test set. Image source: Mask RCNN paper">
<img data-src="/img/mask_rcnn_results.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 11. Mask RCNN results on the COCO test set. Image source: <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask RCNN paper</a>
  </figcaption>


</figure>

<h4 id="mask-loss">Mask Loss</h4>
<p>During the training, a multi-task loss on each sampled RoI is defined as : $L=L_{cls} + L_{bbox}+L_{mask}$. The $L_{cls}$ and $L_{bbox}$ are identical as those defined in <a href="https://arxiv.org/pdf/1506.01497.pdf">Faster RCNN</a>.</p>
<p>The mask branch has a $K\times m^2$ dimensional output for each RoI, which is $K$ binary masks of resolution $m \times m$, one for each the $K$ classes. Since the mask branch learns a mask for every class with a per-pixel <strong>sigmoid</strong> and a <strong>binary cross-entropy loss</strong>, there is no competition among classes for generating masks. Previous semantic segmentation methods (e.g., <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">FCN for semantic segmentation</a>) use a <strong>softmax</strong> and a <strong>multinomial cross-entropy loss</strong>, which causes classification competition among classes.</p>
<p>$L_{mask}$ is defined as the **average binary mask loss**, which **only includes $k$-th class** if the region is associated with the groundtruth class $k$:
$$L_{mask} = -\frac{1}{m^2} \sum_{1 \leqslant i,j \leqslant m} (y_{ij}log\hat{y}_{ij}^k +(1-y_{ij})log(1-\hat{y}_{ij}^k))$$
where $y_{ij}$ is the label (0 or 1) for a cell $(i,j)$ in the groundtruth mask for the region of size $m \times m$, $\hat{y}_{ij}$ is the predicted value in the same cell in the predicted mask learned by the groundtruth class $k$.</p>
<h3 id="26-summary-for-r-cnn-based-object-detection-methods">2.6 Summary for R-CNN based Object Detection Methods</h3>



  











<figure>


  <a data-fancybox="" href="/img/rcnn-family-summary.png" data-caption="Fig 12. Summary for R-CNN based Object Detection Methods . Image source: this blog">
<img data-src="/img/rcnn-family-summary.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 12. Summary for R-CNN based Object Detection Methods . Image source: <a href="https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html">this blog</a>
  </figcaption>


</figure>

<h2 id="3-classic-one-stage-methods">3. Classic One-Stage Methods</h2>
<h3 id="31-yolo-you-only-look-once">3.1 YOLO (You Only Look Once)</h3>
<p><strong>Introduction.</strong> YOLO is the first approach removing region proposal and learns an object detector in an end-to-end manner. Due to no region proposal, it frames object detection as a total regression problem which spatially separates bounding boxes and associated class probabilities. The proposed YOLO performs extremely fast (around 45 FPS), but less accuracy than main approaches like Faster RCNN.</p>



  











<figure>


  <a data-fancybox="" href="/img/yolo.png" data-caption="Fig 13. YOLO pipeline. Image source: original paper.">
<img data-src="/img/yolo.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 13. YOLO pipeline. Image source: <a href="https://arxiv.org/pdf/1506.02640.pdf">original paper.</a>
  </figcaption>


</figure>

<h4 id="311-pipeline">3.1.1 Pipeline</h4>
<ol>
<li>
<p>Resize input image from 224x224 to 448x448;</p>
</li>
<li>
<p>Pre-train a single CNN (<strong>DarkNet similar to GoogLeNet: 24 conv layer + 2 fc</strong>) on ImageNet for classification.</p>
</li>
<li>
<p>Split the input image into $S \times S$ grid, for each cell in the grids:</p>
<p>3.1. predict coordinates of B boxes, for each box coordinates: $(x,y,w,h)$ where $x$ and $y$ are the centre location of box, $w$ and $h$ are the width and height of box.</p>
<p>3.2. predict a confidence score, $C = Pr(obj) \times IoU(trurh, pred)$ where $Pr(obj)$ denote whether the cell contains an object, $Pr(obj)=1$ if it contains an object, otherwise $Pr(obj)=0$. $IoU(truth, pred)$ is the interaction under union.</p>
<p>3.3. predict a probability for every class, $p(c_i)$ where $i$ $\in$ {$1,2,&hellip;,K$} if a cell contains an object. During this stage, each cell only predicts one set of class probabilities regardless of the number of predicted bounding boxes $B$.</p>
</li>
<li>
<p>Output a $S \times S \times (5B + K)$ shape tensor after the last FC layer in total, then compute the loss.</p>
</li>
<li>
<p>In inference time, the network maybe outputs multiple candidate bounding boxes for one same object, then it uses non-maximum suppression to preserve the best match box.</p>
</li>
</ol>
<h4 id="312-loss-functions">3.1.2 Loss functions</h4>
<p>$&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-$</p>
<p><strong>Symbol  Explanation</strong></p>
<p>$\mathbb{1}_{ij}^{obj}$:    an indicator function. It's 1 when there is an object contained in the $j$-th predicted box of the $i$-th cell and <strong>$j$-th predicted box has the largest overlap region with the groundtruth box</strong>, otherwise it's 0.</p>
<p>{$x_{ij}^p, y_{ij}^p, w_{ij}^p, h_{ij}^p$}:  the centre coordinates and (width, height) of the predicted $j$-th bounding box in $i$-th cell.</p>
<p>{$x_{ij}^t, y_{ij}^t, w_{ij}^t, h_{ij}^t$}:  the centre coordinates and (width, height) of the groundtruth $j$-th bounding box in $i$-th cell.</p>
<p>$C_{ij}^p$: the predicted confidence score for the $j$-th bounding box in $i$-th cell.</p>
<p>$C_{ij}^t$: the groundtruth confidence score for the $j$-th bounding box in $i$-th cell.</p>
<p>$p_i^{p}(c)$:  the predicted class probability for $i$-th class category.</p>
<p>$p_i^{t}(c)$:  the groundtruth class probability for $i$-th class category.</p>
<p>$\lambda_{coord}$: a weight parameter for coordinate loss. The default value is 5.</p>
<p>$\lambda_{noobj}$:  a weight parameter for confidence score loss. The default value is 5.</p>
<p>$&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-$</p>
<p>The loss functions basically consists of three parts: coordinates $(x,y,w,h)$, confidence score $C$ and class probabilities $p(c_i)$, $i \in$ {$1,&hellip;,K$}. The total loss is denoted as:
$$\begin{eqnarray}
L_{total} &amp;=&amp; L_{loc} + L_{cls} \\\<br>
&amp;=&amp; \lambda_{coor} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} ((x_{ij}^p - x_{ij}^t)^2 + (y_{ij}^p - y_{ij}^t)^2 + (\sqrt{w_{ij}^p} - \sqrt{w_{ij}^t})^2 + (\sqrt{h_{ij}^p} - \sqrt{h_{ij}^t})^2) \\\<br>
&amp;+&amp; \sum_{i=0}^{S^2} \sum_{ij}^{B}  (\mathbb{1}_{ij}^{obj} + \lambda_{noobj} (1 - \mathbb{1}_{ij}^{obj})) (C_{ij}^p - C_{ij}^t)^2 \\\<br>
&amp;+&amp; \sum_{i=0}^{S^2} \mathbb{1}_{ij}^{obj} \sum_{c\in classes} (p_i^p(c) - p_i^t(c))^2
\end{eqnarray}$$</p>
<h4 id="313-differences--or-insights">3.1.3 Differences ( or insights)</h4>
<ol>
<li>remove region proposal and complete the object detection task in an end-to-end manner.</li>
<li>the first approach achieves real-time speed.</li>
<li><strong>the coordinate loss uses $(x,y,w,h)$ to represent bounding box, which is different from R-CNN based methods. This is because YOLO does not pre-define bounding boxes (i.e., region proposals or anchor boxes), thus YOLO can not use offset of coordinates to compute the loss or train the neural network.</strong></li>
</ol>
<h4 id="314-limitations">3.1.4 Limitations</h4>
<ol>
<li>Less accurate prediction for irregular shapes of object due to a limited box candidates.</li>
<li>Less accurate prediction for small objects.</li>
</ol>
<h3 id="32-ssd-single-shot-multibox-detector">3.2 SSD (single shot multibox detector)</h3>
<p><strong>Introduction.</strong> <a href="https://arxiv.org/pdf/1512.02325.pdf">SSD</a> is one of early approaches attempts to detect multi-scale objects based on pyramid conv feature maps. It adopts the pre-defined anchor box idea but applies it on multiple scales of conv feature maps, which achieves real-time application via removing region proposal and high detection accuracy (even higher than Faster RCNN) via multi-scale object detection as well, e.g., it is capable of detecting both large objects and small objects in one image which increases the mAP.</p>



  











<figure>


  <a data-fancybox="" href="/img/ssd.png" data-caption="Fig 14. SSD pipeline recreated based on original paper.">
<img data-src="/img/ssd.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 14. SSD pipeline recreated based on <a href="https://arxiv.org/pdf/1512.02325.pdf">original paper</a>.
  </figcaption>


</figure>

<h4 id="321-pipeline">3.2.1 Pipeline</h4>
<ol>
<li>Modify pre-trained VGG16 with replaced conv6-7 layers and extra multi-scale conv features. To detect multiple scales of input objects, a few (4 in this case) extra sizes of conv features are added into base model (see light green color in Fig14).</li>
<li>Several default anchor boxes with various scale and ratio (width/height) are introduced for each cell in all feature maps. <strong>For each of $m$ level conv feature maps, we compute the scale $s_k$, aspect ratio $a_r$, width $w_k^a$, height $h_k^a$ and centre location ($x_k^a, y_k^a$) of default boxes</strong> as:</li>
</ol>
<ul>
<li>scale: $$s_k = s_{min} + \frac{s_{max} -s_{min}}{m-1}(k-1), k \in [1,m], s_{min}=0.2, s_{max}=0.9$$</li>
<li>aspect ratio: $a_r \in$ {1,2,3,$\frac{1}{2}, \frac{1}{3}$}, additional ratio $s_k^{'}=\sqrt{s_k s_{k+1}}$,  6 default boxes in total.</li>
<li>width: $w_k^a=s_k \sqrt{a_r}$</li>
<li>height: $h_k^a= s_k / \sqrt{a_r}$</li>
<li>centre location ($x_k^a, y_k^a$):$(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$ where $|f_k|$ is the size of the $k$-th square feture map, $i,j \in [0, |f_k|]$ .</li>
</ul>
<p>where $s_{min}$ is 0.2 and $s_{max}$ is 0.9, which means the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9. Therefore, for each input object, there are $\sum_{i=0}^m C_i \times 6$ anchor boxes where $C_i$ is the channel of $i$-th level feature maps. And for all the multiple level feature maps, there are $\sum_{i=0}^{m} C_i H_i W_i \times 6$ anchor boxes in total where $H_i$ and $W_i$ are the height and width of $i$-th level feature maps.</p>



  











<figure>


  <a data-fancybox="" href="/img/ssd_2.png" data-caption="Fig 15. Matching strategy of anchor boxes during training. Image source: original paper.">
<img data-src="/img/ssd_2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 15. Matching strategy of anchor boxes during training. Image source: <a href="https://arxiv.org/pdf/1512.02325.pdf">original paper</a>.
  </figcaption>


</figure>

<p><strong>Advantage of pre-defined anchor boxes in SSD (matching strategy).</strong> During training, we first match each groundtruth box to the default box with highest jaccard overlap, then match default boxes to any groundtruth boxes with jaccard overlap higher than a threshold (0.5). <strong>This enables SSD to predict multiple high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap</strong>. Thus the network learns match suitable scale of default boxes to groundtruth box. For example, in Fig 15, the network learns from training that anchor boxes of dog on higher layer $4 \times 4$ are matched to groundtruth as the scale of anchor boxes on one $8 \times 8$ feature map are too small to cover the large size of dog.</p>
<ol start="3">
<li>
<p>Hard negative mining. During training, the number of input objects (or labeled anchor boxes) is quite smaller compared to the total number of default anchor boxes, thus most of them are negative samples. This introduces a significant imbalance between negative and positive training examples. The authors of SSD narrow down negative samples by choosing default boxes of top confidence loss, which makes sure the ratio between negative and positive samples at most 3:1.</p>
</li>
<li>
<p><strong>Data augmentation (this contributes most improvement).</strong> To make the detector more robust to various input object sizes, SSD introduces a data augmentation which choose training samples by three following options:</p>
</li>
</ol>
<ul>
<li>use the original input image</li>
<li>sample a patch of original input image, whose IoU with its corresponding groundtruth box is 0.1,0.3,0.5,0.7 or 0.9.</li>
<li>randomly sample a patch of the original input image.</li>
</ul>
<p>The size of sampled patch is [0.1,1] of the original input image and its aspect ratio is between $\frac{1}{2}$ and 2. The overlapped region of the groundtruth box is kept if the centre of it is in the sampled patch. After the sampling step above, all the sampled patches are resized to fixed size and is horizontally flipped with probabilitiy of 0.5.</p>
<ol start="5">
<li>Compute the loss functions.</li>
<li>Non-maximum suppression to find the best match predicted boxes.</li>
</ol>
<h4 id="322-loss-functions">3.2.2 Loss functions</h4>
<p>The training objective is the weighted combination of a <em>localization loss</em> and a <em>classification loss</em>:
$$L = \frac{1}{N}(L_{loc} + \alpha L_{cls})$$
where $N$ is the number of matched boxes and $\alpha$ is picked by cross validation.</p>
<p>The localization loss is the smooth L1 between the predict offset of default boxes and those of matched groundtruth boxes, which is as same as the bounding box regression in RCNN:</p>
<p>$$L_{loc} = \sum_{i=0}^N \sum_{j\in(cx,cy,w,h)} \mathbb{1}_{ij}^k smooth_{L1}(\Delta t_{j}^i - \Delta p_{j}^i)$$
$$\Delta t_{cx}^i = (g_{cx}^i - p_{cx}^i) / p_w^i, \Delta t_{cy}^i = (g_{cy}^i - p_{cy}^i) / p_h^i,$$
$$\Delta t_{w}^i = log(\frac{g_{w}^i}{p_{w}^i}), \Delta t_{h}^i = log(\frac{g_{h}^i}{p_{h}^i}),$$
where $\Delta t_{j}^i$ is the offset of groundtruth boxes, and $\Delta p_{j}^i$ is the offset of predicted boxes. $\mathbb{1}_{ij}^k$ is an indicator for matching $i$-th default box to the $j$-th ground truth box of category $k$.</p>
<p>The classification loss is the softmax loss over multiple classes confidences ($c$) using cross entropy loss:
$$L_{cls} = - \sum_{i=0}^N \mathbb{1}_{ij}^k log(\hat{c}_i^k) - \sum_{j=0}^M log(\hat{c}_j^0), \hat{c}_i^k = softmax(c_i^k) $$
where $N$ and $M$ indicates the positive and negative samples, $c_{i}^k$ is the predicted class probability for $k$-th object class, and $c_i^0$ is the predicted negative probability for non-object class (or background class).</p>
<h4 id="323-differences-or-insights">3.2.3 Differences (or insights)</h4>
<ol>
<li>Multi-scale object detection via extra multiple scales of conv feature maps and matching strategy between default anchor boxes and ground truth boxes.</li>
<li>Training tricks: hard negative mining and data augmentation which increases the mAP most.</li>
</ol>
<h4 id="324-limitations">3.2.4 Limitations</h4>
<p>Some posts (e.g., <a href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06">this blog</a> and <a href="https://medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad">this blog</a>) point out that matching strategy may not help to improve the prediction accuracy for smaller object as it basically only depends on lower layers with high resolution feature maps. These lower layers contain less information for classification. (well, I have not done further experiments to prove this).</p>
<h3 id="33-yolov2yolo9000">3.3 YOLOv2/YOLO9000</h3>
<p><a href="https://arxiv.org/pdf/1612.08242.pdf">YOLOv2</a> is an improvement version of YOLOv1 with several fine-tuning tricks (including adding anchor boxes, multi-scale training etc. see next section), and YOLO9000 is built on top of YOLOv2 but with a joint training strategy of COCO detection dataset and 9000 classes from ImageNet. The enhanced YOLOv2 achieves higher detection accuracy (including bounding box prediction and classification) and
even more faster speed (480x480,59FPS) than SSD.</p>
<h4 id="331-tricks-in-yolov2">3.3.1 Tricks in YOLOv2</h4>
<p><strong>Batch Normalization.</strong> YOLOv2 adds BN after each convolutional layer and it helps to fast convergence, and <strong>increases the mAP about 2.4%</strong>.</p>
<p><strong>High Resolution Classifier.</strong> YOLOv1 fine-tunes a pre-trained model with 448x448 resolution image from detection dataset (e.g., COCO). However, the pre-trained model is trained with 224x224 resolution images, which means directly fine-tuning this pre-trained model with higher resolution will not extract features with powerful expression of images. To address this problem, YOLOv2 first trains the pre-trained model with 448x448 resolution images for classification task, then trains the model with high resolution images for detection task. The high resolution is multiple of 32 as its network has 32 stride.</p>
<p><strong>Convolutional Anchor Boxes.</strong> Instead of using 2 fc layers to regress the location of bounding boxes, inspired by RPN with anchor boxes, YOLOv2 uses convolutional layers and anchor boxes to predict bounding boxes and confidence scores. Each anchor box has a predicted $K$ class probability, thus <strong>the spatial location of anchor boxes and classification is decoupled.</strong> By adding anchor boxes, the mAP of YOLOv2 decreases a bit but it increases recall from 81% to 88%.</p>
<p><strong>Dimension Clusters.</strong> Unlike the sizes of anchor box in Faster RCNN are hand-made, YOLOv2 chooses sizes of anchor box better suit to groundtruth bounding boxes. To find more suitable sizes, YOLOv2 uses k-means to cluster groundtruth bounding boxes and choose the sizes of anchor boxes more close to the centroid of each cluster by the following distance metric:
$$d(box, centroid) = 1 - IoU(box, centroid)$$
and the best number of centroid $k$ can be chosen by the <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">elbow method</a>.</p>
<p><strong>Direct location prediction.</strong> In Faster RCNN, the offset of an anchor box is predicted by the detector, and it is presented by ($\Delta x,\Delta y, \Delta w,\Delta h$). Then the predicted centre location of a bounding box is:
$$x_p=x_a+(\Delta x \times w_a), y_p=y_a+(\Delta y \times h_a)$$
where $x_a$ and $y_a$ are centre location of an anchor box, $w_a$ and $h_a$ are the width and height. The centre location of a predict bounding box can be anywhere in a feature map, for example, if $\Delta x=1$, then the predicted $x_p$ will more a width distance horizontally from $x_a$. This is not good to locate the bounding boxes and could make training unstable. Therefore, YOLOv2 decides to predict the offset to the top-left corner ($c_x,c_y$) of a grid which the anchor box locates at. The scale of a grid is default 1. Then the location ($b_x,b_y,b_w,b_h$) of predicted bounding box is formulated as:
$$b_x = (\sigma(\Delta x) \times 1) + c_x, b_y = (\sigma(\Delta y) \times 1) + c_y$$
$$b_w=a_w e^{\Delta w}, b_h=a_w e^{\Delta h}$$
where $\sigma(\cdot)=sigmoid(\cdot)$, $a_w$ and $a_h$ are width and height of an anchor box, and the width and height of the grid is set default 1. In this way, the movement of $b_x$ and $b_y$ is constrained in the grid as their maximum move distance is $\sigma(\cdot) \times 1 = 1$. <strong>Combining dimension clustering and direct location prediction increases mAP by 5%.</strong> The below figure illustrates the process:</p>



  











<figure>


  <a data-fancybox="" href="/img/yolov2.png" data-caption="Fig 16. Illustration of direct location prediction. Image source recreated on original paper.">
<img data-src="/img/yolov2.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 16. Illustration of direct location prediction. Image source recreated on <a href="https://arxiv.org/pdf/1612.08242.pdf">original paper</a>.
  </figcaption>


</figure>

<p><strong>Add fine-grained feature via passthrough layer.</strong> Inspired by <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>, YOLOv2 also designs a passthrough layer to bring the fine-grained features from an earlier layer to the last output layer. This <strong>increases the mAP about 1%.</strong></p>
<p><strong>Multi-scale Training.</strong> To be robust to various input sizes, YOLOv2 inserts a new size of randomly sampled input images every 10 batches. The new sizes are multiple of 32 as its stride is 32.</p>
<p><strong>Light-weighted base model.</strong> YOLOv2 use DarkNet-19 as base model which has 19 conv layers and 5 maxpooling layers. The key point is to add global avg pooling and 1x1 conv layers between 3x3 conv layers. <strong>This does not increases significant mAP but decreases the computation by about 33%.</strong></p>
<h4 id="332-yolo9000-joint-training-of-detection-and-classification">3.3.2 YOLO9000: Joint Training of Detection and Classification</h4>
<p>Since drawing bounding boxes in images for detection is much more expensive than tagging image for classification, the paper proposes a joint training strategy which combines small detection dataset and large classification dataset, and extand the detection from around 100 categories in YOLOv1 to 9000 categories. The name of YOLO9000 comes from the top 9000 classes of ImageNet. If one input image is from classification dataset, then the network only back propagates the classification loss during training.</p>
<p>The small detection dataset basically has the coarse labels (e.g., cat, person), while the large classification dataset may contain much more detailed labels (e.g., persian cat). Without mutual exclusiveness, this does not make sense to apply softmax to predict all over the classes. Thus YOLO9000 proposes a WordTree to combine the class labels into one hierarchical tree structure with reference to <a href="https://wordnet.princeton.edu/">WordNet</a>. For example, the root node of the tree is a physical object, then next level is coarse-grained labels like animal and artifact, then next level is more detailed labels like cat, dog, vehicle and equipment. Thus, physical object is the parant node of animal and artifact, and animal is the parent node of cat and dog. The labels on the same level should be classified by softmax as they are mutual exlusive.</p>



  











<figure>


  <a data-fancybox="" href="/img/yolo2_wordtree.png" data-caption="Fig 17. Word tree in YOLO9000. Image source: original paper.">
<img data-src="/img/yolo2_wordtree.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 17. Word tree in YOLO9000. Image source: <a href="https://arxiv.org/pdf/1612.08242.pdf">original paper</a>.
  </figcaption>


</figure>

<p>To predict the probability of a class node, we follow the path one node to the root, the searching stops when the probability is over a threshold. For example, the probability of a class label persian cat is:
\begin{eqnarray}
Pr(perisan \ cat \ | \ contain \ a \ physical \ object)
&amp;=&amp; Pr(persian \ cat \ | \ cat) \\\<br>
&amp;\times&amp; Pr(cat \ | \ animal) \\\<br>
&amp;\times&amp; Pr(animal \ | \ physical \ object)
\end{eqnarray}
where $Pr(animal \ | \ physical \ object)$ is the confidence score, predicted separately from the bounding box detection.</p>
<h4 id="333-differences-or-insights">3.3.3 Differences (or insights)</h4>
<ol>
<li>Dimension clustering and direct location prediction gives the most contribution of increasing mAP.</li>
<li>Word Tree is a creative thing in YOLO9000.</li>
</ol>
<h4 id="334-limitations--or-unsolved-problems">3.3.4 Limitations ( or unsolved problems)</h4>
<p>During training, the significant imbalance number between positive anchor boxes containing objects and negative boxes containing background still hinders further improvement of detection accuracy.</p>
<h4 id="insight-questions-for-object-detection">Insight Questions for Object Detection</h4>
<p><strong>Why does the measurement of object detection only use mAP but no classification measurement?</strong> Honestly, I don't know.</p>
<p><strong>Why does improving the classification (binary classification: contains objects of interests or not) increase the detection accuracy?</strong> Well, I think that better classification will reduce negative samples (including easy and hard negative examples), then the network focuses on learning more positive samples to predict bounding boxes, which increases the mAP.</p>
<h3 id="4-retinanet">4. RetinaNet</h3>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">RetinaNet</a> is an one-stage object detector, which proposes two critical contributions: 1. focal loss for addressing class imbalance between foreground containing objects of interest and background containing no object; 2. FPN + ResNet as backbone network for detecting objects at different scales.</p>
<h4 id="41-focal-loss">4.1 Focal Loss</h4>
<p>The extreme class imbalance between training examples is one critical issue for object detection. To address the problem, a focal loss is designed to increase weights for hard yet easily misclassified examples (e.g., background with noisy texture or partial object) and down-weight easy classified examples (e.g., background obviously contains no objects and foreground obviously contains object of interests).</p>
<p>Starting with normal Cross Entropy loss for binary classification:</p>
<p>$$CE(p,y) = -ylog(p) - (1-y)log(1-p)$$
where $y \in$ {0,1} is a groundtruth binary label which indicates a bounding box contains an object or not. $p \in$ [0,1] is the predicted probability of a bounding box containing an object (aka confidence score).</p>
<p>For notational convenience, let $p_t$:</p>
<p>\begin{equation}
p_t = \begin{cases}
p, \ &amp;if \ y=1 \newline
(1-p), \ &amp;otherwise,
\end{cases}
\end{equation}<br>
then
$$CE(p,y) \ = \ CE(p_t) \ = \ -log(p_t)$$</p>
<p>To down-weigh the $CE(p_t)$ when $p \gg 0.5$ (e.g., easily classified examples) and increase weight of loss when $p$ approaching 0 (e.g., hard classified examples), a focal loss is designed by adding a weight $(1-p_t)^\gamma$ into CE loss, which comes to the form:</p>
<p>$$FL(p_t) = -(1-p_t)^\gamma log(p_t)$$
here is the illustration of focal loss, as can be seen, easily classified examples with $p \gg 0.5$ is decreased and the loss of hard examples increases rapidly when $p$ is more closer to 0.</p>



  











<figure>


  <a data-fancybox="" href="/img/retinanet_focal_loss.png" data-caption="Fig 18. The Focal Loss decreases along with predicted probability with a factor of $(1-p_t)^\gamma$. Image source: original paper.">
<img data-src="/img/retinanet_focal_loss.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 18. The Focal Loss decreases along with predicted probability with a factor of $(1-p_t)^\gamma$. Image source: <a href="https://arxiv.org/pdf/1708.02002.pdf">original paper</a>.
  </figcaption>


</figure>

<p>In practise, RetinaNet uses an $\alpha$-balanced variant of the focal loss:
$$FL(p_t) = - \alpha (1-p_t)^\gamma log(p_t)$$
and <strong>there are experiments prove this form slightly improves accuracy over the non-$\alpha$-balanced form. In addition, implementing the loss layer with sigmoid operation for computing $p$ results in greater numerical stability</strong>.</p>
<p>To better illustrate the $\alpha$-balanced FL form, here are a few weighted Focal Loss with various combinations of $\alpha$ and $\gamma$:</p>



  











<figure>


  <a data-fancybox="" href="/img/focal-loss-weights.png" data-caption="Fig 19. The illustration of various combinations of $\alpha$ and $\gamma$ in $\alpha$-balanced Focal Loss. Image source: LilianWeng&#39;s blog.">
<img data-src="/img/focal-loss-weights.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 19. The illustration of various combinations of $\alpha$ and $\gamma$ in $\alpha$-balanced Focal Loss. Image source: <a href="https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html">LilianWeng's blog</a>.
  </figcaption>


</figure>

<h4 id="42-fpn--resnet-as-backbone-network">4.2 FPN + ResNet as Backbone Network</h4>
<p><a href="https://arxiv.org/pdf/1612.03144.pdf">Feature Pyramid Network</a> (FPN) proposes that the hierarchic feature pyramids boost the detection accuracy. Thus, RetinaNet exploits this feature pyramid into their backbone network. Here is a brief introduction about the FPN.</p>
<p>The key point of feature pyramid network is that multi-scale features in different stages are combined together via bottom-up and top-down pathways. For example, Fig 20, the basic pathways in FPN:</p>
<ul>
<li><strong>bottom-up pathway:</strong> a forward pass via ResNet and features from different residual blocks (<strong>downscale by 2</strong>) form the scaled pyramid.</li>
<li><strong>top-down pathway:</strong> merges the strong semantic features from later coarse layer back to front fine layer by <strong>x2 upscale and 1x1 lateral connection and element-wise addition</strong>. The upscale operation is using nearest neighbour upsample in RetinaNet. While other upscale methods like deconv may also be suitable. The conv 1x1 lateral connection is to reduce the feature channel. The prediction happens after each top-down stage by a conv 3x3.</li>
</ul>



  











<figure>


  <a data-fancybox="" href="/img/featurized-image-pyramid.png" data-caption="Fig 20. The illustration of feature pyramid network. Image source: LilianWeng&#39;s blog.">
<img data-src="/img/featurized-image-pyramid.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 20. The illustration of feature pyramid network. Image source: <a href="https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html">LilianWeng's blog</a>.
  </figcaption>


</figure>

<p>The improvement rank of these introduced components is as follows: <strong>1x1 lateral connnect</strong> &gt; detect object across multiple layers &gt; top-down enrichment &gt; pyramid representation (compared to only use single scale image like the finest layer).</p>
<h4 id="43-model-architecture">4.3 Model Architecture</h4>



  











<figure>


  <a data-fancybox="" href="/img/retinanet_architecture.png" data-caption="Fig 21. The model architecture in RetinaNet: ResNet &#43; FPN &#43; Class subnet (focal loss) &#43; Box subnet. Modified on fig 3 in original paper.">
<img data-src="/img/retinanet_architecture.png" class="lazyload" alt="" ></a>


  
  
  <figcaption>
    Fig 21. The model architecture in RetinaNet: ResNet + FPN + Class subnet (focal loss) + Box subnet. Modified on fig 3 in <a href="https://arxiv.org/pdf/1708.02002.pdf">original paper</a>.
  </figcaption>


</figure>

<p>The basic model architecture of RetinaNet is built on top of ResNet by adding FPN and two subnets for classification and box regression. The ResNet has 5 residual blocks which are used to extract feature pyramid. Let $C_i$ denote the output of last conv layer of the $i$-th pyramid level (residual block) and $P_i$ denote the prediction based on $C_i$. As the downscale is 2 used between every two residual blocks,  then $C_i$ is $2^i$ downscale lower than the original input image resolution.</p>
<p>RetinaNet uses prediction $P_s$ to $P_7$ for prediction, where:</p>
<ul>
<li>$P_3$ to $P_5$ are computed on features obtained from the element-wise addition of features after 1x1 conv $C_i$ (bottom-up pathway) and x2 upscaled $C_{i+1}$ (top-down pathway).</li>
<li>$P_6$ is computed on features after 3x3 stride-2 conv on $C_5$.</li>
<li>$P_7$ is computed on features after 3x3 stride-2 conv plus relu on $C_6$.</li>
</ul>
<p>Prediction on higher level features leads to better detect large objects. In addition,  all pyramid features are fixed to channel=256 as they share the same class subnet and box subnet.</p>
<p>The anchor boxes are also applied in RetinaNet, and they are set default to <strong>3 scales {$2^0, 2^{\frac{1}{3}}, 2^{\frac{1}{2}}$}</strong> and <strong>3 aspect ratios {$\frac{1}{2},1,2$}</strong>, thus 9 pre-defined anchor boxes in total.  For each anchor box, the model predicts a classification probability for $K$ classes after passing through the class subnet (applying the proposed <em>focal loss</em>), and regresses the offset of the anchor box to the nearest groundtruth box via the box subnet.</p>
<h4 id="43-insights">4.3 Insights</h4>
<ol>
<li><strong>Focal loss</strong> does a good job to address the imbalance between foreground containing objects of interests and background containing no object.</li>
<li><strong>FPN+ResNet</strong> will be backbone for further one-stage object detectors.</li>
</ol>
<h3 id="yolov3">YOLOv3</h3>
<p><a href="https://arxiv.org/pdf/1804.02767.pdf">YOLOv3</a> proposes a series of incremental tricks for YOLOv2, and these tricks are inspired by recent researches:</p>
<ol>
<li>
<p><strong>logistic regression for objectiveness score (aka confidence score).</strong> Unlike using squared error for location loss in YOLOv1-2, YOLOv3 change this loss to a logistic loss to predict the offset of bounding boxes. The yolov3 paper tells that linear regression for predicting offset decreases in mAP.</p>
</li>
<li>
<p><strong>independent logistic classifier instead of softmax for class prediction.</strong></p>
</li>
<li>
<p><strong>FPN works well in YOLOv3.</strong> YOLOv3 adopts the FPN to predict boxes at 3 different scales of features.</p>
</li>
<li>
<p><strong>Features extracted from a new net called DarkNet-53.</strong> DarkNet-53 adopts the shortcut connections (residual blocks) from ResNet, and it performs similar to ResNet-152 but 2x faster.</p>
</li>
</ol>
<p>Well, the authors of YOLO series tried focal loss, but it dropped mAP about 2 points. It maybe has something with no subnets for class and box prediction in YOLOv3, or the parameters $\lambda_{coord}$ and $\lambda_{noobj}$ in YOLO have done the job to balance the loss. Not sure yet.</p>
<p><strong>Overall, YOLOv3 is still a good real-time object detector, it performs less accruacy than RetinaNet-101 (ResNet-101-FPN) but around 3.5~4.0x faster.</strong></p>
<p>TO BE CONTINUED&hellip;</p>
<h2 id="reference">Reference</h2>
<ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">https://blog.csdn.net/v_JULY_v/article/details/80170182</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html">https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html</a></li>
<li><a href="https://medium.com/egen/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9">https://medium.com/egen/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9</a></li>
<li><a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-">https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37998710">https://zhuanlan.zhihu.com/p/37998710</a></li>
<li><a href="https://blog.csdn.net/heiheiya/article/details/81169758">https://blog.csdn.net/heiheiya/article/details/81169758</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html">https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/35325884">https://zhuanlan.zhihu.com/p/35325884</a></li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/academic/">Academic</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://Alibabade.github.io/post/object_detection/&amp;text=Object_detection" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://Alibabade.github.io/post/object_detection/&amp;t=Object_detection" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Object_detection&amp;body=https://Alibabade.github.io/post/object_detection/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://Alibabade.github.io/post/object_detection/&amp;title=Object_detection" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Object_detection%20https://Alibabade.github.io/post/object_detection/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://Alibabade.github.io/post/object_detection/&amp;title=Object_detection" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu8c1967adf340de7ae400ce95dd873c2d_1186948_250x250_fill_lanczos_center_2.png" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://Alibabade.github.io/">Li Wang</a></h5>
      <h6 class="card-subtitle">PhD candidate</h6>
      <p class="card-text">My research focuses on image/video based neural style transfer.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/lwang@bournemouth.ac.uk" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.facebook.com/profile.php?id=100013393752495" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=KKNBDWQAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Alibabade" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/training_tricks_dl/">Training_tricks_dl</a></li>
      
      <li><a href="/post/graph_nn/">Graph_NN</a></li>
      
      <li><a href="/post/archive_papers/">Archive_papers</a></li>
      
      <li><a href="/post/attention/">Attention</a></li>
      
      <li><a href="/post/meta_learning_in_dl/">Meta_learning_in_dl</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.3/mermaid.min.js" integrity="" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
