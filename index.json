[{"authors":["admin"],"categories":null,"content":"Li Wang is a PhD candidate of Computer Vision and Computer Graphics at National Centre for Computer Animation, Bournemouth University, UK. His research focuses on image/video based neural style transfer. I received my M.Eng and B.Sci degrees from Department of Computer Science at Jilin University in 2016 and 2013 repectively. Then I got a 4-year scholarship to support my PhD study from Bournemouth University and China Scholarship Council.\n","date":1578572488,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1578941543,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://lwang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Li Wang is a PhD candidate of Computer Vision and Computer Graphics at National Centre for Computer Animation, Bournemouth University, UK. His research focuses on image/video based neural style transfer. I received my M.Eng and B.Sci degrees from Department of Computer Science at Jilin University in 2016 and 2013 repectively. Then I got a 4-year scholarship to support my PhD study from Bournemouth University and China Scholarship Council.","tags":null,"title":"Li Wang","type":"authors"},{"authors":["Li Wang"],"categories":["Academic"],"content":"This blog contains four parts:\n Introduction: What is Object Detection? and general thoughts/ideas to deal with Object Detection; Classic Deep Learning based Methods: multi-stage :RCNN and SPP Net , two-stage: Fast RCNN, Faster RCNN, Mask RCNN; Classic One-Stage Methods: YOLOv1-v3, SSD, RetinaNet; More Recent Anchor-Free Object Detection Methods (2018-2019);  1. Introduction What is Object Detection? Given an image, object detection aims to find the categories of objects contained and their corresponding locations (presented as bounding-boxes) in the image. Thus Object Detection contains two tasks: classification and localization.\nGeneral thoughts/ideas to detect objects. The classification has been done by CNNs like AlexNet, VGG and ResNet. Then only localization still needs to be done. There are two intuitive ways: 1. Regression: the location of an object is presented by a vector $(x,y,w,h)$ which are the centre coordinates and width/height of an object bounding-box in the given image. For example, given an image, it only contains one object\u0026mdash;cat. To locate the bounding-box, we apply a CNN to predict the vector $(x_p,y_p,w_p,h_p)$ and learn to regress the predicted vector to be close to groundtruth $(x_t,y_t,w_t,h_t)$ by calculating the L2 loss (see Fig 1.).\n   Fig 1. Regression for localization shown in this blog.   If the initial bounding-box is randomly chosen or there are many objects, then the entire regression will be much difficult and take lots of training time to correct the predicted vector to groundtruth. Sometime, it may not achieve a good convergence. However, if we select approximately initial box coordinates which probably contains the objects, then the regression of these boxes should be much easier and faster as these initial boxes already have closer coordinates to groundtruth than random ones. Thus, we could divide the problem into Box Selection and Regression, which are called Region Proposal Selection and Bounding-box Regression(please go post Basic_understanding_dl if you do not know Bounding-box regression) in Object Detection, respectively. Based on this, the early Object Detection methods contain multi-stage tasks like: Region Proposal Selection, Classification and Bounding-box Regression. In this blog, we only focus on the DL-based techniques, thus We do not review any pre-DL methods here.\nThere are a few candidate Region Proposal Selection methods (shown in below), and some of them are able to select fewer proposals (nearly hundreds or thousands) and keep high recall.    Fig 2. Comparisons between different Region Proposal Selection methods shown in this blog.   2. Classic Deep Learning based Methods Since using Region Proposal Selection can reduce bounding-box candidates from almost infinite to ~2k for one image with multiple objects, Ross et al. 2014 propose the first CNN-based Object Detection method, which uses CNN to extract features of images, classifies the categories and regress bounding-box based on the CNN features.\n2.1 R-CNN (Region CNN) The basic procedure of R-CNN model:\n Use Selective Search to select ~2k Region Proposals for one image. Warp all the Region Proposals into a same size as the fully connection layers in their backbone neural network (i.e., AlexNet) has image size limitation. For example, the FC layers only take 21x21xC feature vector as input, then all the input image size has to be 227x227 if all the Conv + BN + relu layers of a pre-trained AlexNet are preserved. Feed the Region Proposals into the pre-trained AlexNet at each proposal per time rate, and extract the CNN features from FC7 layer for further classification (i.e., SVM). The extracted CNN features will also be used for Bounding-box Regression.  Based on the procedure above, there are twice fine-tuning:\n Fine-tune the pre-trained CNN for classification. For example, the pre-trained CNN (i.e., AlexNet) may have 1000 categories, but we may only need it to classify ~20 categories, thus we need to fine-tune the neural network. Fine-tune the pre-trained CNN for bounding-box regression. For example, we add a regression head behind the FC7 layer, and we need to fine-tune the network for bounding-box regression task.  2.1.1 Some common tricks used 1. Non-Maximum Suppression\nCommonly, sometimes the RCNN model outputs multiple bounding-boxes to localize the same object in the given image. To choose the best matching one, we use non-maximum suppression technique to avoid repeated detection of the same instance. For example, we sort all the boxes with confidence score, and remove those with low confidence score. Then we choose the box with highest IOU.\n   Fig 3. Non-maximum suppression used in RCNN this blog.   2. Hard Negative Mining\nBounding-box containing no objects (i.e., cat or dog) are considered as negative samples. However, not all of them are equally hard to be identified. For example, some samples purely holding background are \u0026ldquo;easily negative\u0026rdquo; as they are easily distinguished. However, some negative samples may hold other textures or part of objects which makes it more difficult to identify. These samples are likely \u0026ldquo;Hard Negative\u0026rdquo;.\nThese \u0026ldquo;Hard Negative\u0026rdquo; are difficult to be correctly classified. What we can do about it is to find explicitly those false positive samples during training loops and add them into the training data in order to improve the classifier.\n2.1.2 Problems of RCNN RCNN extracts CNN features for each region proposal by feeding each of them into CNN once at a time, and the proposals selected by Selective Search are approximately 2k for each image, thus this process consumes much time. Adding pre-processing Selective Search, RCNN needs ~47 second per image.\n2.2 SPP Net (Spatial Pyramid Pooling Network) To speedup RCNN, SPPNet focuses on how to fix the problem that each proposal is fed into the CNN once a time. The reason behind the problem is the fully connected layers need fixed feature size (i.e., 1 x 21 x 256 in He et al.,2014) for further classification and regression. Thus SPPNet comes up with an idea that an additional pooling layer called spatial pyramid pooling is inserted right after the last Conv layer and before the Fc layers. The operation of this pooling first projects the region proposals to the Conv features, then divides each feature map (i.e., 60 x 40 x 256 filters) from the last Conv layer into 3 patch scales (i.e., 1,4 and 16 patches, see Fig 4. For example, the patch size is: 60x40 for 1 patch, 30x20 for 4 patches and 15x10 for 16 patches, next operates max pooling on each scaled patch to obtain a 1 x 21(1+4+16) for each feature map, thus we get 1x21x256 fiexd vector for Fc layers.\n   Fig 4. The spatial pyramid pooling layer in SPPNet.   By proposing spatial pyramid pooling layer, SPPNet is able to reuse the feature maps extracted from CNN by passing the image once through because all information that region proposals need is shared in these feature maps. The only thing we could do next is project the region proposals selected by Selective Search onto these feature maps (How to project Region Proposals to feature maps? Please go to basic_understanding post for ROI pooling.). This operation extremely saves time consumption compared to extract feature maps per proposal per forward (like RCNN does). The total speedup of SPPNet is about 100 times compared to RCNN.\nFast RCNN    Fig 6. The pipeline of Fast RCNN in this blog.   Fast RCNN attempts to overcome three notable drawbacks of RCNN:\n Training a multi-stage pipeline: fine-tune a ConvNet based on Region Proposals; train SVM classifiers with Conv Features; train bounding-box regressors. Training is expensive in space and time: 2.5 GPU-days for 5k images and hundreds of gigabytes of storage. Speed is slow: ~47 second per image even on GPU.  Solutions:\n Combine both classification (replace SVM with softmax) and bounding-box regression into one network with multi-task loss. Introduce ROI pooling for: 1. reuse Conv feature maps of one image; 2. speedup both training and testing. Using VGG16 as backbone network, ROI (Region of Interest) pooling converts all different sizes of region proposals into 7x7x512 feature vector fed into Fc layers. Please go to post basic_understanding_dl for more details about ROI pooling.     Fig 6. Speed comparison between RCNN and Fast RCNN in this blog.   Multi-task Loss for Classification and Bounding-box Regression $\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;$\nSymbol Explanation\n$u$ Groundtruth class label, $u \\in 0,1,\u0026hellip;,K$; To simplify, all background class has $u=0$.\n$v$ Groundtruth bounding-box regression target, $v=(v_x,v_y,v_w,v_h)$.\n$p$ Descrete probability distribtion (per RoI), $p=(p_0,p_1,\u0026hellip;,p_K)$ over $K+1$ categories. $p$ is computed by a softmax over the $K+1$ outputs of a fully connected layer.\n$t^u$ Predicted bounding-box vector, $t^u=(t_x^u,t_y^u,t_w^u,t_h^u)$.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;$\nThe multi-task loss on each RoI is defined as:\n$$L(p,u,t^u,v) = L_{cls}(p,u) + \\lambda[u \\geqslant 1]L_{loc}(t^u,v)$$ where $L_{cls}(p,u)$=-log$p_u$ is a log loss for groundtruth class $u$. The Iverson bracket indicator function $[u \\geqslant 1]$ is 1 when $u \\geqslant 1$ (the predicted class is not background), and is 0 otherwise. The $L_{loc}$ term is using smooth L1 loss, which is denoted as: $$L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}}smooth_{L_1}(t_i^u-v_i)$$ and \\begin{equation} smooth_{L_1}(x) = \\begin{cases} 0.5x^2, \u0026amp;|x| \u0026lt; 1 \\newline |x|-0.5,\u0026amp;otherwise \\end{cases} \\end{equation} $smooth_{L_1}(x)$ is a robust $L_1$ loss that is less sensitive to outliers than $L_2$ loss.\nFaster RCNN    Fig 7. The pipeline of Faster RCNN in this blog.   Faster RCNN focuses on solving the speed bottleneck of Region Proposal Selection as previous RCNN and Fast RCNN separately compute the region proposal by Selective Search on CPU which still consumes much time. To address this problem, a novel subnetwork called RPN (Region Proposal Network) is proposed to combine Region Proposal Selection into ConvNet along with Softmax classifiers and Bounding-box regressors.\n   Fig 8. The pipeline of RPN in the paper.   To adapt the multi-scale scheme of region proposals, the RPN introduces an anchor box. Specifically, RPN has a classifier and a regressor. The classifier is to predict the probability of a proposal holding an object, and the regressor is to correct the proposal coordinates. Anchor is the centre point of the sliding window. For any image, scale and aspect-ratio are two import factors, where scale is the image size and aspect-ratio is width/height. Ren et al., 2015 introduce 9 kinds of anchors, which are scales (1,2,3) and aspect-ratio(1:1,1:2,2:1). Then for the whole image, the number of anchors is $W \\times H \\times 9$ where $W$ and $H$ are width and height, respectively.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-$\nSymbol Explanation\n$i$ the index of an anchor in a mini-batch.\n$p_i$ the probability that the anchor $i$ being an object.\n$p_i^{*}$ the groundtruth label $p_i^{*}$ is 1 if the anchor is positive, and is 0 if the anchor is negative.\n$t_i$ a vector $(x,y,w,h)$ representing the coordinates of predicted bounding box.\n$t_i^{*}$ that of the groundtruth box associated with a positive anchor.\n$\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-$\nThe RPN also has a multi-task loss just like in Fast RCNN, which is defined as:\n$$L({p_i},{t_i}) = \\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{*}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{*} L_{reg}(t_i, t_i^{*})$$ where the classification $L_{cls}$ is log loss over two classes (object vs not object). The regression loss $L_{reg}(p_i, p_i^{*}) = smooth_{L1}(t_i - t_i^{*})$. The term $p_i^{*} L_{reg}(t_i, t_i^{*})$ means the regression loss is activated if $p_i^{*}=1$ and is disabled if $p_i^{*}=0$. These two loss term are normalized by $N_{cls}$ and $N_{reg}$ and weighted by a balancing parameter $\\lambda$. In implementation, $N_{cls}$ is the number of images in a mini-batch (i.e., $N_{cls}=256$), and the $reg$ term is normalized by the number of anchor locations (i.e., $N_{reg} \\sim 2,400$). By default, the $\\lambda$ is set to 10.\nTherefore, there are four loss functions in one neural network:\n one is for classifying whether an anchor contains an object or not (anchor good or bad in RPN); one is for proposal bounding box regression (anchor -\u0026gt; groundtruth proposal in RPN); one is for classifying which category that the object belongs to (over all classes in main network); one is for bounding box regression (proposal -\u0026gt; groundtruth bounding-box in main network);  The total speedup comparison between RCNN, Fast RCNN and Faster RCNN is shown below:    Fig 9. The speedup comparison between RCNN, Fast RCNN and Faster RCNN in this blog.   Mask RCNN    Fig 10. The pipeline of Mask RCNN, which is Faster RCNN + Instance Segmentation + improved RoIAlign Pooling.   Mask RCNN has three branches: RPN for region proposal + a pretrained CNN + Headers for classification and bounding-box regression + Mask Network for pixel-level instance segmentation. Mask RCNN is developed on Faster RCNN and adds RoIAlign Pooling and instance segmentation to output object masks in a pixel-to-pixel manner. The RoIAlign is proposed to improve RoI for pixel-level segmentation as it requires much more fine-grained alignment than Bounding-boxes. The accurate computation of RoIAlign is described in RoIAlign Pooling for Object Detection in Basic_understanding_dl post.\n   Fig 11. Mask RCNN results on the COCO test set. Image source: Mask RCNN paper   Mask Loss During the training, a multi-task loss on each sampled RoI is defined as : $L=L_{cls} + L_{bbox}+L_{mask}$. The $L_{cls}$ and $L_{bbox}$ are identical as those defined in Faster RCNN.\nThe mask branch has a $K\\times m^2$ dimensional output for each RoI, which is $K$ binary masks of resolution $m \\times m$, one for each the $K$ classes. Since the mask branch learns a mask for every class with a per-pixel sigmoid and a binary cross-entropy loss, there is no competition among classes for generating masks. Previous semantic segmentation methods (e.g., FCN for semantic segmentation) use a softmax and a multinomial cross-entropy loss, which causes classification competition among classes.\n$L_{mask}$ is defined as the **average binary mask loss**, which **only includes $k$-th class** if the region is associated with the groundtruth class $k$: $$L_{mask} = -\\frac{1}{m^2} \\sum_{1 \\leqslant i,j \\leqslant m} (y_{ij}log\\hat{y}_{ij}^k +(1-y_{ij})log(1-\\hat{y}_{ij}^k))$$ where $y_{ij}$ is the label (0 or 1) for a cell $(i,j)$ in the groundtruth mask for the region of size $m \\times m$, $\\hat{y}_{ij}$ is the predicted value in the same cell in the predicted mask learned by the groundtruth class $k$.\nSummary for R-CNN based Object Detection Methods    Fig 12. Summary for R-CNN based Object Detection Methods . Image source: this blog   Reference  https://blog.csdn.net/v_JULY_v/article/details/80170182 https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html https://medium.com/egen/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9 https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review- https://zhuanlan.zhihu.com/p/37998710  ","date":1578572488,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578941543,"objectID":"092317b203c8d1e2a34b394edbc30e63","permalink":"https://lwang.github.io/post/object_detection/","publishdate":"2020-01-09T12:21:28Z","relpermalink":"/post/object_detection/","section":"post","summary":"Summary of object detection in DL","tags":["Computer Vision","DL"],"title":"Object_detection","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"Fine-tuning neural networks In practise, researchers tend to use pre-trained neural networks on datasets like ImageNet to train their own neural network for new tasks due to their dataset perhaps not big enough (compared to millions of images in ImageNet). Thus this type of operation is called fine-tuning the neural network.\nThere are two typical scenarios:\n use the pre-trained CNNs as feature extractors. For example, we remove the fully connected layers from a pre-trained image classification CNN, then add a classification operator (i.e., softmax and SVM) at the end of left fully convolutional networks to classify images. fine-tune the pre-trained CNNs. For example, we preserve part/all of the layers in a pre-trained CNNs, and retrain it on our own dataset. In this case, the front layers extract low-level features which can be used for many tasks (i.e., object recognition/detection and image segmentation), and the rear layers extract high-level features related to specific classification task, thus we only need fine-tune the rear layers.  How to fine-tune There are normally four different situations:\n New dataset is small and similar to pre-trained dataset. Since the dataset is small, then retrain the CNN may cause overfitting. And the new dataset is similar to pre-trained dataset, thus we hope the high-level features are similar as well. In this case, we could just use the features extracted from pre-trained CNN and train a classification operator like softmax. New dataset is small but not similar to pre-trained datasets. Since the dataset is small then we can not retrain the CNN. And the new dataset is not similar to pre-trained datasets, then we do not use high-level features which means we do not use rear layers. Thus we can just use front layers as feature extractor and training a classification operator like softmax or SVM. New dataset is big and similar to pre-trained datasets. We can fine-tune the entire pre-trained CNN. dataset is big but not similar to pre-trained datasets. We can fine-tune the entire pre-trained CNN.  In practise, a smaller learning-rate is suggested as the weights in network is already smooth and a larger learning-rate may distort the weights of pre-trained CNN.\nCoding in experiments In Pytorch, you can set \u0026ldquo;param.requires_grad = False\u0026rdquo; to freeze any pre-trained CNN part. For example, to freeze some layers in BERT model, you could do something like this:\nif freeze_embeddings: for param in list(model.bert.embeddings.parameters()): param.requires_grad = False print (\u0026quot;Froze Embedding Layer\u0026quot;) # freeze_layers is a string \u0026quot;1,2,3\u0026quot; representing layer number if freeze_layers is not \u0026quot;\u0026quot;: layer_indexes = [int(x) for x in freeze_layers.split(\u0026quot;,\u0026quot;)] for layer_idx in layer_indexes: for param in list(model.bert.encoder.layer[layer_idx].parameters()): param.requires_grad = False print (\u0026quot;Froze Layer: \u0026quot;, layer_idx)  ","date":1578517390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578696038,"objectID":"2cd8a3ec49cf5b20aa026a962a81fc8c","permalink":"https://lwang.github.io/post/training_tricks_dl/","publishdate":"2020-01-08T21:03:10Z","relpermalink":"/post/training_tricks_dl/","section":"post","summary":"Summary neural network training tricks.","tags":["Academic"],"title":"Training_tricks_dl","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","DL"],"content":"This blog simply clarifies the concepts of graph embedding, graph neural networks and graph convolutional networks.\nGraph Embedding Graph Embedding (GE) is in representation learning of neural network, which often contains two types:\n embed each node in a graph into a low-dimension, scalar and dense vector, which can be represented and inferenced for learning tasks. embed the whole graph into a low-dimension, scalar and dense vector, which can be used for graph structure classification.  There are three types of method to complete graph embedding:\n Matrix Factorization. A representable matrix of a graph is factorized into vectors, which can be used for learning tasks. The representable matrix of a graph are often adjacency matrix, laplacian matrix etc. Deepwalk. Inspired by word2vec, the deepwalk considers the reached node list by random walk as a word, which is fed into word2vec network to obtain a embeded vector for learning tasks. Graph Neural Network. It is basically a series of neural networks operating on graphs. The graph information like representable matrix is fed into neural networks in order to get embedded vectors for learning tasks.  Graph Neural Networks Graph Neural Networks (GNN) are deep neural networks with graph information as input. In general, the GNN can be divided into different types: 1. Graph Convolutional Networks (GCN); 2. Graph Attention Networks (GAT); 3. Graph Adversarial Networks; 4. Graph LSTM. The basic relationship between GE, GNN,GCN is shown as following picture:\n   Fig 3. Relation between GE, GNN and GCN in this blog   Graph Convolutional Networks Graph Convolutional Networks (GCN) operate convolution on graph information like adjacency matrix, which is similar to convolution on pixels in CNN. To better clarify this concept, we will use equations and pictures in the following paragraph.\nConcepts There are two concepts should be understood first before GCN. Degree Matrix (D): this matrix ($N \\times N$, N is the node number) is a diag matrix in which values in diag line means the degree of each node; Adjacency Matrix (A): this matrix is also a $N \\times N$ matrix in which value $A_{i,j}=1$ means there is an edge between node $i$ and $j$, otherwise $A_{i,j}=0$;\nSimple GCN example Let's consider one simple GCN example, which has one GCN layer and one activation layer, the formulation is as following: $$f(H^{l}, A) = \\sigma(AH^{l}W^{l})$$ where $W^l$ denotes the weight matrix in the $l$th layer and $\\sigma(\\dot)$ denotes the activation function like ReLU. This is the simplest expression of GCN example, but it's already much powerful (we will show example below). However, there are two basic limitations of this simple formulation:\n there is no node self information as adjacency matrix $A$ does not contain any information of nodeself. there is no normalization of adjacency matrix. The formulation $AH^{l}$ is actually a linear transformation which scales node feature vectors $H^l$ by summing the features of all neighbour nodes. The nodes having more neighbour nodes has more impact, which should be normalized.  Fix limitation 1. We introduce the identity matrix $I$ into adjacency matrix $A$ to add nodeself information. For example, $\\hat{A} = A + I_n$ where $I_n$ is the identity matrix with $n \\times n$ size.\nFix limiation 2. Normalizing $A$ means that all rows of $A$ should sum to be one, and we realize this by $D^{-1}A$ where $D$ is the diag degree matrix. In practise, we surprisingly find that using a symmetric normalization, e.g., $D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ is more dynamically more interesting (I still do not get it why use symmetric normalization $D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$). Combining these two tricks, we get the propagation rule introduced in Kipf et al. 2017: $$f(H^l, A) = \\sigma(\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^lW^l)$$ where $\\hat{A} = I_n + A$ and $\\hat{D}$ is the diagonal node degree matrix of $\\hat{A}$. In general, whatever matrix multiplies $H^lW^l$ (i.e. $A$, $D^{-1}A$ or $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$) is called Laplacian Matrix, which can be denoted as $L_{i,j}$ the value for $i$th and $j$th node. Taking the laplacian Matrix introduced in Kipf et al. 2017 as an example, the $L_{i,j}$ is as following: \\begin{equation} L_{i,j}^{sym} = \\begin{cases} 1, \u0026amp;i=j, deg(j) \\neq 0 \\newline -\\frac{1}{\\sqrt{deg(i)deg(j)}}, \u0026amp;i \\neq j, j \\in \\Omega_i \\newline 0, \u0026amp;otherwise \\end{cases} \\end{equation} where deg($\\cdot$) denotes the degree matrix and $\\Omega_i$ denotes all the neighbour nodes of node $i$. This symmetric Laplacian matrix not only considers the degree of node $i$ but also takes the degree of its neighbour node $j$ into account, which refers to symmetric normalization. This propagation rule weighs neighbour in the weighted sum higher if the node $i$ has a low-degree and lower if the node $i$ has a high-degree. This may be useful when low-degree neighbours have bigger impact than high-degree neighbours.\nCode example for simple GCN Considering the following simple directed graph:\n   Fig 2. Graph example in this blog   Then the Adjacency Matrix $A$ is:\nA = np.matrix([ [0.,1.,0.,1.], [0.,0.,1.,1.], [0.,1.,0.,0.], [1.,0.,1.,0.]], dtype=float)  the identity matrix $I_n$ of $A$ is:\nI_n = np.matrix([ [1.,0.,0.,0.], [0.,1.,0.,0.], [0.,0.,1.,0.], [0.,0.,0.,1.]], dtype=float)  We assign random weight matrix of one GCN layer:\nW = np.matrix([[1,-1],[-1,1]])  Next we randomly give 2 integer features for each node in the graph.\nH = np.matrix([[i,-i] for i in range(A.shape[0])], dtype=float) H matrix([[ 0., 0.], [ 1., -1.], [ 2., -2.], [ 3., -3.]])  Now the unnormalized features $\\hat{A}H$ are:\nA_hat * H matrix([[ 1., -1.], [ 6., -6.], [ 3., -3.], [ 5., -5.]])  the output of this GCN layer:\nA_hat * H * W matrix([[ 2., -2.], [ 12., -12.], [ 6., -6.], [ 10., -10.]]) # f(H,A)=relu(A_hat * H * W) relu(A_hat * H * W) matrix([ [2., 0.], [12.,0.] [6., 0.], [10., 0]])  Next we apply the propagation rule introduced in Kipf et al. 2017. First, we add self-loop information $\\hat{A} = A + I$ is :\nA_hat = A + I_n A_hat matrix([ [1.,1.,0.,0.], [0.,1.,1.,1.], [0.,1.,1.,0.], [1.,0.,1.,1.]])  Second, we add normalization by computing $\\hat{D}$ and $\\hat{D}^{-\\frac{1}{2}}$ (inverse matrix of square root of $\\hat{D}$):\nD_hat = np.array(np.sum(A_hat, axis=0))[0] D_hat = np.matrix(np.diag(D_hat)) D_hat matrix([[ 2., 0., 0., 0.], [ 0., 3., 0., 0.], [ 0., 0., 3., 0.], [ 0., 0., 0., 2.]]) inv_D_hat_sqrtroot = np.linalg.inv(np.sqrt(D_hat)) inv_D_hat_sqrtroot matrix([[ 0.70710678, 0. , 0. , 0. ], [ 0. , 0.57735027, 0. , 0. ], [ 0. , 0. , 0.57735027, 0. ], [ 0. , 0. , 0. , 0.70710678]])  Next, we compute the Laplacian matrix of $L = \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ and the nomalized features $L * H$ :\nLaplacian_matrix = inv_D_hat_sqrtroot * A_hat * inv_D_hat_sqrtroot Laplacian_matrix matrix([[ 0.5 , 0.40824829, 0. , 0. ], [ 0. , 0.33333333, 0.33333333, 0.40824829], [ 0. , 0.33333333, 0.33333333, 0. ], [ 0.5 , 0. , 0.40824829, 0.5 ]])  # normalized feature vectors Laplacian_matrix * H matrix([[ 0.40824829, -0.40824829], [ 2.22474487, -2.22474487], [ 1. , -1. ], [ 2.31649658, -2.31649658]]) #non-normalized feature vectors A_hat * H matrix([[ 1., -1.], [ 6., -6.], [ 3., -3.], [ 5., -5.]])  As can be seen, all the values of feature vectors are scaled to smaller absolute values than Non-normalized feature vectors.\nFinally, the output of GCN layer with applying the propagation rule:\n# f(H,A) = relu(L*H*W) relu(Laplacian_matrix*H*W) matrix([[ 0.81649658, 0.], [ 4.44948974, 0.], [ 2. , 0.], [ 4.63299316, 0.]]) # compared to f(H,A) = relu(A_hat*H*W) relu(A_hat*H*W) matrix([[ 2., 0.], [ 12., 0.], [ 6., 0.], [ 10., 0.]])  I suggest to verify this operation by yourself.\nReal example: Semi-Supervised Classification with GCNs Kipf \u0026amp; Welling ICLR 2017 demonstrates that the propgation rule in GCNs can predict semi-supervised classification for social networks. In this semi-supervised learning example, we assume that we know all the graph information including nodes and their neighbours, but not all the node labels, which means some nodes are labeled but others are not labeled.\nWe train the GCNs on labeled nodes and propagate the node label information to unlabedled nodes by updating weight matrices shared arcoss all nodes. This is done by following steps:\n perform forward propagation through the GCN layers. apply sigmoid function row-wise at the last layer of GCN. compute the cross entropy loss on known node labels. backpropagate the loss and update the weight matrices $W$ in each layer.  Zachary's Karate Club Zachary's Karate Club is a typical small social network where there are a few main class labels. The task is to predict which class each member belongs to.    Fig 3. Graph structure of Karate Club in this blog   We run a 3-layer GCN with randomly initialized weights. Now before training the weights, we simply insert Adjacency matrix $A$ and feature $H=I$ (i.e., $I$ is the identity matrix) into the model, then perform three propagation steps during the forward pass and effectively convolves the 3rd-order neighbourhood of each node. The model already produces predict results like picture below without any training updates:\n   Fig 4. Predicted nodes for Karate Club in this blog   Now we rewrite the propagation rule in layer-wise GCN (in vector form): $$h_{i}^{l+1} = \\sigma (\\sum_{j} \\frac{1}{c_{i,j}} h_{j}^{l} W^l)$$ where $\\frac{1}{c_{i,j}}$ originates from $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ and $h_j^l$ denotes the feature vector of neighbour node $j$. Now the propagation rule is interpreted as a differentiable and parameterized (with $W^l$) variant, if we choose an appropriate non-linear activation and initialize the random weight matrix such that is orthogonal (or using the initialization from Glorot \u0026amp; Bengio, AISTATS 2010), this update rule becomes stable in practice (also thanks to the normalization with $c_{i,j}$).\nNext we simply label one node per class and use the semi-supervised learning algorithm for GCNs introduced in Kipf \u0026amp; Welling, ICLR 2017, and we start to train for a couple of iterations:  The video above shows Semi-supervised classification with GCNs in this blog. And the model directly produces a 2-dimensional laten space which we can visualize.\nNote that we just use random feature vectors (e.g., identity matrix we used here) and random weight matrices, only after a couple of iteration, the model used in Kipf \u0026amp; Welling is already able to achieve remarkable results. If we choose more serious initial node feature vectors, then the model can achieve state-of-the-art classification results on a various number of graph datasets.\nFurther Reading on GCN Well, the GCN is developing rapidly, here are a few papers for further reading:\n Inductive Representation Learning on Large graph FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification  Reference  https://zhuanlan.zhihu.com/p/89503068 https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780 https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0 https://tkipf.github.io/graph-convolutional-networks/  ","date":1578245173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578868338,"objectID":"99a069dafe5d68a4745b2e6222d70ae8","permalink":"https://lwang.github.io/post/graph_nn/","publishdate":"2020-01-05T17:26:13Z","relpermalink":"/post/graph_nn/","section":"post","summary":"Basic summary of Graph Convolutional Networks","tags":["Academic"],"title":"Graph_NN","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vison","Computer Graphics"],"content":"Deep Correlations for Texture Synthesis (TOG2017) Motivation The texture synthesis using deep feature maps has difficulty to synthesize structure textures, which is a challenge to preserve the non-local structures.\nCore Ideas  Introduce a structure matrix to represent the deep correlation among deep features. In another word, Gatys et al.,2015 consider the Gram loss of deep features between channels, while acutally the structure information is included inside each feature channel. Thus Sendik et al., 2017 propose an intra-feature based Gram loss, which is fomulated as:  \\begin{equation} R_{i,j}^{l,n} = \\sum_{q,m} w_{i,j} f_{q,n}^{l,n} f_{q-i,m-j}^{l,n} \\end{equation} where $R_{i,j}^{l,n}$ denotes the intra-feature Gram loss of $n$th channel in $l$th layer, $i \\in [-Q/2, Q/2]$ and $ j \\in [-M/2,M/2]$. This means that $f_{q,m}^{l,n}$ is shifted by $i$ pixels vertically and $j$ pixels horizontally, and applying a point-wise multiplication across the overlapping region, weighted by the inverse of the total amount of overlapping regions, That is $$w_{i,j} = [(Q-|i|)(M-|j|)]^{-1}$ Then the structure loss based on intra-feature Gram loss is denoted as:\n$$E_{DCor}^l = \\frac{1}{4}\\sum_{i,j,n}(R_{i,j}^{l,n} - \\widetilde{R}_{i,j}^{l,n})^2$$\n Introduce a diversity loss to make sure the method can synthesize a larger texture than input exemplar. The idea is to shift the input deep correlation matrix $f_{i,j}^{l,n}$ to the size of desired output texture.\n  Introduce an edge-preserving smooth loss, which only penalizes the pixel difference when none of neighbouring pixels are smiliar to the pixel under consideration. The authors claim that this smooth loss is useful to reduce checker artefacts.\n  The total loss function is weighted $E_{DCor}$, Gram loss, diversity loss and edge-preserving smooth loss.\n  ","date":1578229042,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578259946,"objectID":"7f34bbe5927931de207d4c2ed722ba90","permalink":"https://lwang.github.io/post/archive_papers/","publishdate":"2020-01-05T12:57:22Z","relpermalink":"/post/archive_papers/","section":"post","summary":"Archive for reording papers","tags":["Academic"],"title":"Archive_papers","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","Deep Learning"],"content":"Attention The attention mechanism was created to simulate the human visual attention on images or understanding attention on texts. It was firstly born for solving a problem that widely exists in natural language processing (NLP) models like seq2seq, which NLP models often tend to forget the first part of processed sentences.\nSeq2seq model The encoder-decoder architecture commonly used in Seq2seq model:\n encoder, compress the input sentence into a context vector in a fixed length way which is regarded as a representation of the meaning of input sentence. decoder, fed by the context vector and translate the vector to output. In some early works, the last state of encoder is usually used as the initial state of decoder.  Both of the encoder and decoder are recurrent neural networks, using LSTM or GRU units.\nThe critical problem of seq2seq model. The seq2seq model often forgets the first part of a long sentence once it completes translation from the entire sentence to a context vector. To address this problem, the attention mechanism is proposed.\nThe attention mechanism The new architecture for encoder-decoder machine translaion is as following:    Fig 1. The encoder-decoder architecture in Bahdanau et al. 2015   The encoder is composed by a bidirection RNN, a context vector is the sum of weighted hidden states and the decoder translates the context vector to a output target based on previous output targets.\nFomula Let x=$(x_1, x_2,\u0026hellip;,x_n)$ denote the source sequence of length $n$ and y=$(y_1, y_2,\u0026hellip;,y_m)$ denote the output sequence of length $m$, $\\overrightarrow{h_i}$ denotes the forward direction state and $\\overleftarrow{h_i}$ presents the backward direction state, then the hidden state for $i$th input word is fomulated as: $$h_i = [\\overrightarrow{h_i}^T; \\overleftarrow{h_i}^T], i=1,2,\u0026hellip;,n$$\nThe hidden states at position $t$ in decoder includes previous hidden states $s_{t-1}$, previous output target $y_{t-1}$ the context vector $c_t$, which is denoted as $s_{t} = f(s_{t-1}, y_{t-1}, c_{t})$, where the context vector $c_{t}$ is a sum of encoder hidden states of input sequence, weighted by alignment scores. For output target at position $t$, we have: $$c_{t} = \\sum_{i=1}^{n} \\alpha_{t,i} h_i$$\n$$ \\alpha_{t,i}= align(y_t, x_i) = \\frac{exp(score(s_{t-1},h_i))}{\\Sigma^n_{i=1} exp(score(s_{t-1},h_{i}))} $$ The score $\\alpha_{t,i}$ is assigned to the pair $(y_t, x_i)$ of input at position $i$ and output at position $t$, and the set of weights ${\\alpha_{t,i}}$ denotes how much each source hidden state matches for each output. In Bahdanau et al. 2015, the score $\\alpha$ is learnt by a feed-forward network with a single hidden layer and this network is jointly learnt with other part of the model. Since the score is modelled in a network which also has weight matrices (i.e., $v_a$ and $W_a$) and activation layer (i.e., tanh), then the learning function is fomulated as: $$score(s_t, h_i) = v_a^T tanh(W_a[s_t;h_i])$$\nSelf-attention Self-attention (or intra-attention) is such an attention mechnaism that assigns correlation in a single sequence for an effective representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarizatin or image description generation.\nIn Cheng et al., 2016, an application of self-attention mechanism is shown in machine reading. For example, the self-attention mechanism enables the model to learn a correlation between the current word and the previous part of the input sentence.    Fig 2. An example of self-attention mechanism in Cheng et al., 2016   Soft and Hard Attention In image caption generation, the attention mechanism is applied and shown to be very helpful. Xu et al.,2015 shows a series of attention visualization to demonstrate how the model learn to summarize the image by paying attention to different regions.    Fig 2. The visulation of attention mechanism for image caption generation in Xu et al., 2015   The soft attention and hard attention is telled by whether the attention has access to the entire image or only a patch region:\nSoft attention: the alignment weights are assigned to all the patches in the source image, which is the same type used in Bahdanau et al. 2015 Pro: the model is smooth and differentiable Con: computationally expensive when the source image is large\nHard attention: the alignment weights are only assigned to a patch in the source image at a time Pro: less computation at the inference time Con: the model is non-differentiable and requires more complicated techniques such as variance reduction and reinforcement learning to train.(Luong et al., 2015)\nwhy hard attention is non-differentiable? Hard attention is non-differentiable because it’s stochastic. Both hard and soft attention calculate a context vector using a probability distribution (usually over some set of annotation vectors), but soft attention works by taking the expected context vector at that time (i.e. a weighted sum of the annotation vectors using the probability distribution as weights), which is a differentiable action. Hard attention, on the other hand, stochastically chooses a context vector; when the distribution is multinomial over a set of annotation vectors (similar to how soft attention works, but we aren’t calculating the expected context vector now), you just sample from the annotation vectors using the given distribution. The advantage of hard attention is that you can also attend to context spaces that aren’t multinomial (e.g. a Gaussian-distributed context space), which is very helpful when the context space is continuous rather than discrete (soft attention can only work over discrete spaces).\nAnd since hard attention is non-differentiable, models using this method have to be trained using reinforcement learning.\nGlobal and Local Attention Luong et al. 2015 proposed a global and local attention, where the global attention calculate the entire weighted sum of hidden states for a target output (which is similar to soft attention). While the local attention is more smiliar to the blend of soft and hard attention. For example, in their paper, the model first predict a aligned position for the current target word then a window centred around the source position is used to compute a context vector.\n   Fig 1. The global and local attention architecture in Luong et al., 2015   Reference  This attention blog heavily borrowed from Lilian Weng's blog, more details refer to https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html  ","date":1577570595,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578175512,"objectID":"d7323692466e95a1bd1eca0d480be9b1","permalink":"https://lwang.github.io/post/attention/","publishdate":"2019-12-28T22:03:15Z","relpermalink":"/post/attention/","section":"post","summary":"Basic summary about attention","tags":["Academic","attention in dl"],"title":"Attention","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","Deep Learning"],"content":"","date":1577479276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577571935,"objectID":"5acf0a9265de05af60c769d7e9347a9c","permalink":"https://lwang.github.io/post/meta_learning_in_dl/","publishdate":"2019-12-27T20:41:16Z","relpermalink":"/post/meta_learning_in_dl/","section":"post","summary":"Summary about the meta learning","tags":["Academic","Meta-learning in dl"],"title":"Meta_learning_in_dl","type":"post"},{"authors":["Li Wang"],"categories":["DL/ML","CV/CG"],"content":"Basic knowledge in neural networks  Graph Neural Network Graph Convolutional Network Transform learning: mete learning, few shot learning, zero-shot learning RNN, GRU Siamese Network Reinforcement learning NAS Optimization: Adam, SGD Normalization AlexNet, VGG, Inception, ResNet (ResNet-50,ResNet-101, ResNeXt-50/101), Xecption.  SOTA research areas  Object recognition/detection: YOLO, LOGAN, Anchor, Anchor free, two-stage,one-stage Object tracking face recognition/detection NLP: BERT, seq2seq, bag of words Image segmentation/instance segmentation: deeplab ShuffleNet, MobileNet 2D Image \u0026mdash;\u0026gt; 3D model Human pose estimation: 2D skeleton, 3D skeleton, 3D mesh Optical flow Attention FPN, Mask-rcnn,faster-rcnn, RPN, RetinaNet, ROI pooling,  Machine Learning  KNN,SVM, GBDT, XGBOOST  ","date":1577269663,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578868338,"objectID":"dfd8aa50d70e25a70f0dc59876316d5a","permalink":"https://lwang.github.io/post/record_discrete_knowledge/","publishdate":"2019-12-25T10:27:43Z","relpermalink":"/post/record_discrete_knowledge/","section":"post","summary":"a simple record of discrete knowledge","tags":["Academic"],"title":"Record_discrete_knowledge","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","Deep Learning"],"content":"Activation functions Description Activation functions works like an on-off button that determines whether the output of a neuron or what information should be passed to next layer. In biology, it works like synaptic in brain which decides what information it passes from one neuron cell to next one. There are several activation functions widely used in neural networks.\nBinary function (step function) In one word, the output of binary function is 1 or 0 which is based on whether the input is greater or lower than a threshold. In math, it looks like this: f(x) = {1, if x \u0026gt; T; 0, otherwise}.\nCons: it does not allow multiple outputs, and it can not support to classify inputs to one of categories.\nLinear function f(x) = $cx$. Cons: 1. the deviation of linear function is a constant, which does not help for backpropagation as the deviation is not correlated to its inputs, in another word, it can not distinguih what weights or parameters help to learn the task; 2. linear function makes the entire multiple neural network into one linear layer (as the combination of linear functions is still a linear function), which becomes a simple regression model. It can not handle complex tasks by varying parameters of inputs.\nNon-linear functions Non-linear functions address the problems by two aspects:\n The deviation of non-liear function is a function correlated to its inputs, which contributes the backpropagation to learn how to update weights for high accurancy. Non-linear functions form the layers with hidden neurons into a deep neural network which is capable of predicting for complicated tasks by learning from complex datasets.  There are several popular activation functions used in modern deep neural networks.\nSigmoid/Logistic Regression    Fig 1. Sigmoid Visualization   Equation: $$Sigmoid(x) = \\frac{1}{1+e^{-x}}$$ Derivative (with respect to $x$): $$Sigmoid^{'}(x) = Sigmoid(x)(1-Sigmoid(x))$$ Pros:\n smooth gradient, no jumping output values compared to binary function. output value lies between 0 and 1, normalizing output of each neuron. right choice for probability prediction, the probability of anything exists only between 0 and 1.  Cons:\n vanishing gradient, the gradient barely changes when $x\u0026gt;2$ or $x\u0026lt;-2$. computationally expensive. non zero centered outputs. The outputs after applying sigmoid are always positive, during gradient descent, the gradients on weights in backpropagation will always be positive or negative, which means the gradient updates go too far in different directions, and makes the optimization harder.  Softmax $$Softmax(x_i)= \\frac{x_i}{\\Sigma_{j=1}^{n}{x_j}}$$\nPros: capable of handling multiple classification and the sum of predicted probabilities is 1. Cons: only used for output layer.\nSoftmax is more suitable for multiple classification case when the predicted class must and only be one of categories. k-sigmoid/LR can be used to classify such multi-class problem that the predicted class could be multiple.\nTanh Equation: $$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$ Derivative (with respect to $x$): $$tanh^{'}(x) = 1 -tanh(x)^2$$\nPros:\n zero centered. make it easier to model inputs that have strongly positive, strongly negative, and natural values. similar to sigmoid  Cons:\n vanishing gradient computationally expensive as it includes division and exponential operation.  Vanishing gradient Vanishing gradient means that the values of weights and biases are barely change along with the training.\nExploding gradient Gradient explosion means that the values of weights and biases are increasing rapidly along with the training.\nReLU     Fig 2. ReLU Visualization   Equation: $$ReLU(x) = max(0, x)$$ Derivative (with respect to $x$): \\begin{equation} ReLU^{'}(x) = \\begin{cases} 0, \u0026amp;x \\leqslant 0; \\newline 1, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation} Pros:\n computationally efficient non-linear  Why ReLU performs better in modern NNs? The answer is not so sure right now, but its propeties like non-saturation gradient and computionally efficient indeed lead to fast convergence. Additionally, its property sparsing the network also improves the modeling preformance. The non-zero centered issue can be tackled by other regularization techniques like Batch Normalization which produces a stable distribution for ReLU.\nCons:\n Dying ReLU problem. The backpropagation won't work when inputs approach zero or negative. However, to some extent, dying ReLU problem makes input values sparse which is helpful for neural network to learn more important values and perform better. Non differentiable at zero. Non zero centered. Don't avoid gradient explode  ELU     Fig 3. ELU Visualization   Equation: \\begin{equation} ELU(x) = \\begin{cases} \\alpha (e^x-1), \u0026amp; x \\leqslant 0 \\newline x, \u0026amp;x \u0026gt; 0 \\end{cases} \\end{equation}\nDerivative: \\begin{equation} ELU^{'}(x) = \\begin{cases} ELU(x) + \\alpha, \u0026amp; x \\leqslant 0 \\newline 1, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation}\nPros:\n prevent dying ReLU problem. gradient works when input values are negative. non-linear, gradient is not zero.  Cons:\n don't avoid gradient explode. not computationally efficient. $\\alpha$ is not learnt by neural networks.  Leaky ReLU     Fig 4. LReLU Visualization ($\\alpha=0.1$)   Equation: \\begin{equation} LReLU(x) = \\begin{cases} \\alpha x, \u0026amp;x \\leqslant 0 \\newline x, \u0026amp;x \u0026gt; 0 \\end{cases} \\end{equation}\nDerviative: \\begin{equation} LReLU^{'}(x) = \\begin{cases} \\alpha, \u0026amp;x \\leqslant 0 \\newline 1, \u0026amp;x \u0026gt; 0 \\end{cases} \\end{equation}\nPros:\n prevent Dying ReLU problem computationally efficient non-linear  Cons:\n don't avoid gradient explode Non consistent results for negative input values. non-zero centered non differentiable at Zeros  SELU     Fig 5. SELU Visualization   Equation: \\begin{equation} SELU(x) = \\lambda \\begin{cases} \\alpha e^x-\\alpha, \u0026amp; x \\leqslant 0 \\newline x, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation}\nDerivative: \\begin{equation} SELU^{'}(x) = \\lambda \\begin{cases} \\alpha e^x, \u0026amp; x \\leqslant 0 \\newline 1, \u0026amp; x \u0026gt; 0 \\end{cases} \\end{equation} where $\\alpha \\approx 1.6732632423543772848170429916717$ and $\\lambda \\approx 1.0507009873554804934193349852946$.\nPros:\n Internal normalization, which means faster convergence. Preventing vanishing gradient and exploding gradient.  Cons: Need more applications to prove its performance on CNNs and RNNs.\nGELU     Fig 6. GELU Visualization   Equation: \\begin{equation} GELU(x) = 0.5x(1 + tanh(\\sqrt{\\frac{2}{\\pi}} (x + 0.044715x^3))) \\end{equation}\nPros:\n Best performance in NLP, especially BERT and GPT-2 Avoid vanishing gradient  Cons: Need more applications to prove its performance.\nReference  https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/ https://www.jianshu.com/p/6db999961393 https://towardsdatascience.com/activation-functions-b63185778794 https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions  ","date":1577222141,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577571935,"objectID":"a5a05febc73f1d219093a9c23e6b6b66","permalink":"https://lwang.github.io/post/activation_functions_in_dl/","publishdate":"2019-12-24T21:15:41Z","relpermalink":"/post/activation_functions_in_dl/","section":"post","summary":"Basic summary for understanding activation functions in NN","tags":["Academic","Activation functions in DL"],"title":"Activation_functions_in_dl","type":"post"},{"authors":["Li Wang"],"categories":["ML/DL"],"content":"Bag of words (BOW) BOW is a method to extract features from text documents, which is usually used in NLP, Information retrieve (IR) from documents and document classification. In general, BOW summarizes words in documents into a vocabulary (like dict type in python) that collects all the words in the documents along with word counts but disregarding the order they appear.\nFor examples, two sentences:\nLei Li would like to have a lunch before he goes to watch a movie.  James enjoyed the movie of Star War and would like to watch it again.  BOW will collect all the words together to form a vocabulary like:\n{\u0026quot;Lei\u0026quot;:1, \u0026quot;Li\u0026quot;:1, \u0026quot;would\u0026quot;:2, \u0026quot;like\u0026quot;:2, \u0026quot;to\u0026quot;:3, \u0026quot;have\u0026quot;:1, \u0026quot;a\u0026quot;:2, \u0026quot;lunch\u0026quot;:1, \u0026quot;before\u0026quot;:1, \u0026quot;he\u0026quot;:1, \u0026quot;goes\u0026quot;:1, \u0026quot;watch\u0026quot;:2, \u0026quot;movie\u0026quot;:2, \u0026quot;James\u0026quot;:1, \u0026quot;enjoyed\u0026quot;:1, \u0026quot;the\u0026quot;:1, \u0026quot;of\u0026quot;:1, \u0026quot;Star\u0026quot;:1, \u0026quot;War\u0026quot;:1, \u0026quot;and\u0026quot;:1, \u0026quot;it\u0026quot;:1, \u0026quot;again\u0026quot;:1 }  The length of vector represents each sentence is equal to the word number, which is 22 in our case. Then first sentence is presented in vector (in the order of vocabulary) as: {1,1,1,1,2,1,2,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0}, and the second sentence is presented in vector as: {0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1}.\nReference https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/\nPrincipal Component Analysis (PCA)  mean: $\\mu_i = \\frac{1}{n} \\Sigma_{i=1}^n{x_i}$ variance: $\\sigma^2 = \\frac{1}{n} \\Sigma_{i=1}^n{(x_i - \\mu_i)^2}$ standard deviation: $\\sigma^2 = \\frac{1}{n-1} \\Sigma_{i=1}^n{(x_i - \\mu_i)^2}$ covariance: $cov(x,y) = \\frac{1}{n-1} \\Sigma_{i=1}^n{(x_i-\\mu_x)*(y_i -\\mu_y)}$  Cross Entropy Amount of information that an event gives In general, the amount of information should be greater when an event with low probability happens. For example, event A: China won the table-tennis world champion; event B: Eygpt won the table-tennis world champion. Obviously, event B will give people more information if it happens. The reason behind this is that event A has great probability to happen while event B is rather rare, so people will get more information if event B happens.\nThe amount of information that an event gives is denoted as following equation:\n$$f(x) = -log(p(x))$$ where $p(x)$ denotes the probability that event $x$ happens.\nEntropy For a given event $X$, there may be several possible situations/results, and each situation/result has its own probability, then the amount of information that this event gives is denoted as: $$f(X)= -\\Sigma_{i=1}^{n}p(x_i)log(p(x_i))$$\nwhere $n$ denotes the number of situations/results and $p(x_i)$ is the probability of situation/result $x_i$ happens.\nKullback-Leibler (KL) divergence The KL divergence aims to describe the difference between two probability distributions. For instance, for a given event $X$ consisting of a series events ${x_1,x_2,\u0026hellip;,x_n}$, if there are two probability distributions of possible situations/results: $P={p(x_1),p(x_2),\u0026hellip;,p(x_n)}$ and $Q={q(x_1),q(x_2),\u0026hellip;,q(x_n)}$, then the KL divergence distance between $P$ and $Q$ is formulated as:\n$$D_{KL}(P||Q) = \\Sigma_{i=1}^n p(x_i)log(\\frac{p(x_i)}{q(x_i)})$$ further, $$D_{KL}(P||Q) = \\Sigma_{i=1}^n p(x_i)log(p(x_i)) - \\Sigma_{i=1}^n p(x_i)log(q(x_i))$$ where $Q$ is closer to $P$ when $D_{KL}(P||Q)$ is smaller.\nCross Entropy In machine learning or deep learning, let $y={p(x_1),p(x_2),\u0026hellip;,p(x_n)}$ denote the groundturth probability distribution, and $\\widetilde{y}={q(x_1),q(x_2),\u0026hellip;,q(x_n)}$ present the predicted probability distribution, then KL divergence is just a good way to compute the distance between predicted distribution and groudtruth. Thus, the loss could be just formulated as: $$Loss(y,\\widetilde{y}) = \\Sigma_{i=1}^n p(x_i)log(p(x_i)) - \\Sigma_{i=1}^n p(x_i)log(q(x_i))$$ where the first term $\\Sigma_{i=1}^n p(x_i)log(p(x_i))$ is a constant, then the $Loss(y,\\widetilde{y})$ is only related to the second term $- \\Sigma_{i=1}^n p(x_i)log(q(x_i))$ which is called **Cross Entropy** as a training loss.\nReference https://blog.csdn.net/tsyccnh/article/details/79163834\nConv 1x1 Conv 1x1 in Google Inception is quite useful when the filter dimension of input featues needs to be increased or decreased meanwhile keeping the spaital dimension, and reduces the convolution computation. For example, the input feautre dimension is $(B, C, H, W)$ where $B$ is batch size, $C$ is channel number, $H$ and $W$ are height and width. Using $M$ filters of Conv 1x1, then the output of Conv 1x1 is $(B,M,H,W)$, only channel number changes but spatial dimension ($H \\times W$) is still the same as input. To demonstrate the computation efficiency using conv 1x1, take a look at next example. For instance, the output feature we want is $(C, H, W)$, using M filters of conv 3x3, then the compuation is $3^2C \\times MHW$. Using conv 1x1, the computation is $1^2C \\times MHW$, which is $\\frac{1}{9}$ of using conv 3x3.\nWhy do we need to decrease filter dimension or the number of feature maps? The filters or the number of feature maps often increases along with the depth of the network, it is a common network design pattern. For example, the number of feature maps in VGG19, is 64,128,512 along with the depth of network. Further, some networks like Inception architecture may also concatenate the output feature maps from multiple front convolution layers, which also rapidly increases the number of feature maps to subsequent convolutional layers.\nReference https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network\nIOU (Intersection of Union) in Object Detection    Fig 1. IOU Visualization in this blog.   Bounding-box Regression in Object Detection Why do we need Bounding-box Regression? In general, our object detection method predicts bounding-box for an object like blue box in below image. But the groundturth box is shown in green colour, thus we can see the bounding-box of the plane is not accurate compared to the groundtruth box as IOU is lower than 0.5 (intersection of union). If we want to get box location more close to groundtruth, then Bounding-box Regression will help us do this.\n   Fig 2. Predicted box for airplane and its corresponding groudtruth in this blog.   What is Bounding-box Regression? We use $P = (P_x,P_y, P_w, P_y)$ presents the centre coordinates and width/height for the Region Proposals, which is shown as Blue window in the Fig 3. The groundtruth box is represented by $G=(G_x,G_y,G_w,G_h)$. Our aim is to find a projection function $F$ which finds a box $\\hat{G}=(\\hat{G}_x,\\hat{G}_y,\\hat{G}_w,\\hat{G}_h)$ closer to $G$. In math, we need to find a $F$ which makes sure that $F(P) = \\hat{G}$ and $\\hat{G} \\approx G$.\n   Fig 3. Bounding box regression in this blog.   How to do Bounding-box Regression in R-CNN? We want to transform $P$ to $\\hat{G}$, then we need a transformation $(\\Delta x, \\Delta y, \\Delta w, \\Delta h)$ which makes the following happen: $$\\hat{G}_x = P_x + \\Delta x * P_w \\Rightarrow \\Delta x = (\\hat{G}_x - P_x)/P_w$$ $$\\hat{G}_y = P_y + \\Delta y * P_h \\Rightarrow \\Delta y = (\\hat{G}_y - P_y)/P_h$$ $$\\hat{G}_w = P_w e^{\\Delta w} \\Rightarrow \\Delta w = log(\\hat{G}_w/P_w)$$ $$\\hat{G}_h = P_h e^{\\Delta h} \\Rightarrow \\Delta h = log(\\hat{G}_h/P_h)$$\nWhile the groundtruth $(\\Delta t_x, \\Delta t_y, \\Delta t_w, \\Delta t_h)$ is defined as: $$\\Delta t_x = (G_x - P_x)/P_w$$ $$\\Delta t_y = (G_y - P_y)/P_h$$ $$\\Delta t_w = log(G_w/P_w)$$ $$\\Delta t_h = log(G_h/P_h)$$ Next, we denote $W_i \\Phi(P_i)$ where $i \\in {x,y,w,h}$ as learned transformation through the neural network, then the loss function is to minimize the L2 distance between $\\Delta t_i$ and $W_i \\Phi(P_i)$ where $i \\in {x,y,w,h}$ by SGD: $$L_{reg} = \\sum_{i}^{N} (\\Delta t_i - W_i \\Phi(P_i))^2 + \\lambda ||W||^2$$\nUpsampling, Deconvolution and Unpooling Upsampling: upsample any image to higher resolution. It uses upsample and interpolation.\nDeconvolution: also called transpose convolution. For example, your input for deconvolution layer is 4x4, deconvolution layer multiplies one point in the input with a 3x3 weighted kernel and place the 3x3 results in the output image. Where the outputs overlap you sum them. Often you would use a stride larger than 1 to increase the overlap points where you sum them up, which adds upsampling effect (see blue points). The upsampling kernels can be learned just like normal convolutional kernels.    Fig 4. Visualization of Deconvolution in this Quora answer.   Unpooling: We use an unpooling layer to approximately simulate the inverse of max pooling since max pooling is non-invertible. The unpooling operates on following steps: 1. record the maxima positions of each pooling region as a set of switch variables; 2. place the maxima back to the their original positions according to switch variables; 3. reset all values on non-maxima positions to $0$. This may cause some information loss.\nReference:s  https://www.quora.com/What-is-the-difference-between-Deconvolution-Upsampling-Unpooling-and-Convolutional-Sparse-Coding  RoI Pooling in Object Detection (Fast RCNN) RoI pooling is simple version of Spatial Pyramid Pooling (multiple division scales, i.e., divide the entire feature maps into (1,4,16) patches/grids), which has only one scale division. For example, the original input image size is (1056x640) and one region proposal ($x_1=0, y_1=80,x_2=245, y_2=498$), after convolutional layers and pooling layers, the feature map size is (66x40), then we should rescale the proposal from ($x_1=0, y_1=80,x_2=245, y_2=498$) to ($x_1=0, y_1=5,x_2=15, y_2=31$) as the scale is 16 (1056/66=16 and 640/40=16). Then we divide the proposal on feature map into 7x7 sections/grids (the proposal size is no need of 7 times) if the output size is 7x7. Next we operate max pooling on each grid, and place the maxima into output 7x7. Here is another simple example below, the input feature size is 8x8, proposal is (0,3,7,8), and the output size is 2x2 thus divide the proposal region into 2x2 sections:\n   Fig 5. Visualization of ROI pooling in this blog.   Reference  https://zhuanlan.zhihu.com/p/73654026ß  RoIAlign Pooling in Object Detection RoI align Pooling is proposed in Mask RCNN to address the problem that RoI pooling causes misalignments by rounding quantization.\nProblem of RoI pooling. There are twice misalignments for each RoI pooling operation. For example, the size of original input image is 800x800, and one region proposal is 515x482 and its corresponding coordinates are ($x_{tl}=20, y_{tl}=267, x_{br}=535, y_{br}=749$) where \u0026lsquo;tl\u0026rsquo; means top left and \u0026lsquo;br\u0026rsquo; means bottom right. And the stride of last conv layer is **16** which means each feature map extracted from the last conv layer is **50x50** (800/16). If the output size is fixed to 7x7, then RoI pooling would quantize (by floor operation) the region proposal to **32x30** and its corresponding coordinates to ($x_{tl}=1,y_{tl}=16, x_{br}=33, y_{br}=46$). The twice misalignments in each feature map are visualized in the below figure (acutal coordinates in blue colour).\n   Fig 7. Visualization of RoI quantization/misaligments and RoIAlign corrections.   Solution. RoI Align removes all the quantizations of RoI and keeps the float number. Taking the example above, RoI Align Pooling keeps the projected region proposal size 32.1875x30.125 and its corresponding coordinates are ($x_{tl}=1.25,y_{tl}=16.6875, x_{br}=33.4375, y_{br}=46.8125$). Then its corresponding grid size is **4.598x4.303**. We assume the **sample rate is 2**, then 4 points will be sampled. For each grid, we compute the coordinates of 4 sampled points. The coordinates of top left sampled point is (1.25+(4.598/2)/2=2.3995, (16.6875+(4.303/2)/2)=17.76325), the top right sampled point is (1.25+(4.598/2)x1.5=4.6985, 17.76325), the bottom left sampled point is (1.25+(4.598/2)/2=2.3995, 16.6875+(4.303/2)x1.5=19.91475), and the bottom right samples point is (1.25+(4.598/2)x1.5=4.6985, 16.6875+(4.303/2)x1.5=19.91475). For the first sampled point, we compute the value at (2.3995,17.76325) by interpolating values at four nearest points ((2,17),(3,17),(2,18) and (3,18)) in each feature map. The computation of one sampled point can be visualized by the below figure.\n    Fig 6. Visualization of one sampled value computation in RoI Align.   where area0=(2.3995-2)x(17.76325-17)=0.304918375, area1=(3-2.3995)x(17.76325-17)=0.458331625, area2=(2.3995-2)x(18-17.76325)=0.094581625, area3=(3-2.3995)x(18-17.76325)=0.142168375.\nReference  https://zhuanlan.zhihu.com/p/61317964  ","date":1577202754,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578941543,"objectID":"096c7d801f98983face3501dd500a162","permalink":"https://lwang.github.io/post/basic_understanding_dl/","publishdate":"2019-12-24T15:52:34Z","relpermalink":"/post/basic_understanding_dl/","section":"post","summary":"Some discrete knowledge across research areas like NLP, IR, image/video and geometry","tags":["Academic","Discrete knowledge in ML/DL"],"title":"Basic_understanding_dl","type":"post"},{"authors":["Li Wang"],"categories":["Computer Vision","Deep Learning"],"content":"Batch Normalization (BN) Short Description Batch Normalization is a basic method to initialize inputs to neural networks. In the early years, the neural network is sensitive to the hyperparameters, which makes it difficult to train and stabilize. To address this problem, Loffe et al. proposed a novel normalization method to accelerate the neural network trianing process.\nTraining Problems 1. Slow learning speed. The $W$ weights and $b$ bias (parameters) in each layer are updated along with each SGD iterative optimization (back propagation), which makes the distribution of inputs to the next layer changes all the time. Thus the learning speed becomes slow as each layer has to adapt to input changes.\n2. Slow convergence speed. For saturating nonlinearities like Sigmoid and Tanh, more and more inputs to activation functions may lay in the saturation regions along with the $W$ and $b$ increase. This further causes the gradient becomes close to 0 and weights update in a slow rate.\nThese problems are described as Internal Covariate Shift in Loffe et al. 2015.\nSolutions to Internal Covariate Shift 1. Whitening (machine learning). Whitening aims to linearly transform the inputs to zero mean and unit variances, which decorrelates the inputs. There are normally two whitening methods: PCA and ZCA, where PCA whitening transforms inputs to zero mean and unit variance, and ZCA whitening transforms inputs to zero mean and same variance as original inputs.\n2. Batch Normalization. Motivation: 1. whitening costs too much computation if it is put before each layer; 2. the distribution of transformed inputs does not have expressive power as original inputs.\nSolutions:\n  simplify the linearly transformation by following equations: $$ \\mu_j = \\frac{1}{m}\\Sigma_{i=1}^{m}{x_j^i}$$ $$\\sigma^2_j = \\frac{1}{m}\\Sigma_{i=1}^{m}{(x_j^i - \\mu_j)^2}$$ $$x_j^{i'} = \\frac{x_j^i - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}$$ where $j$ denotes the $j$th layer, $\\mu_j$ and $\\sigma_j^2$ denote the mean and variance of inputs $x_j$. $x_j^{i'}$ denotes the transformation output which has zero mean and unit variance, $m$ denotes the sample number in $x_i$ and $\\varepsilon$ ($\\varepsilon=10^{-8}$) prevents the zero in variance.\n  learn a linear transformation with parameter $\\gamma$ and $\\beta$ to restore original input distribution by following equation: $$x_j^{i\u0026quot;} = \\gamma_j x_j^{i'} + \\beta_j$$\nwhere the transformed output will have the same distribution of original inputs when $\\gamma_j^2$ and $\\beta_i$ equal to $\\sigma_j^2$ and $\\mu_j$.\n  Normalization in testing stage. Generally, there may be just one or a few examples to be predicted in testing stage, the mean and variance computed from such examples could be baised. To address this problem, the $\\mu_{batch}$ and $\\sigma^2_{batch}$ of each layer are stored to compute the mean and variance for testing stage. For example, $\\mu_{test} = \\frac{1}{n} \\Sigma \\mu_{batch}$ and $\\sigma^2_{batch}=\\frac{m}{m-1} \\frac{1}{n} \\Sigma \\sigma^2_{batch}$, then $BN(x_{test})=\\gamma \\frac{x_{test} - \\mu_{test}}{\\sqrt{\\sigma^2_{test} + \\varepsilon}} + \\beta$.\nAdvantages of BN  Faster learning speed due to stable input distribution. Saturating nonlinearities like Sigmoid and Tanh can still be used since gradients are prevented from disappearing. Neural network is not sensitive to parameters, simplfy tuning process and stabilize the learning process. BN partially works as regularization, increases generalization ability. The mean and variance of each mini -batch is different from each other, which may work as some noise input for the nerual network to learn. This has same function as Dropout shutdowns some neurons to produce some noise input to the neural networks.  Disadvantages of BN  NOT well for small mini-batch as the mean ans variance of small mini-batch differ great from other mini-batches which may introduce too much noise into the NN training. NOT well for recurrent neural network as one hidden state may deal with a series of inputs, and each input has different mean and variance. To remember these mean and variance, it may need more BN to store them for each input. NOT well for noise-sensitive applications such as generative models and deep reinforcement learning.  Rethink the resaon behind the effectiveness of BN Why does the BN works so well in CNNs? This paper revisits the BN and proposes that the success of BN has little to do with reducing Internal Covariate Shift. Well, the ICS does exist in the deeper neural layers, but adding artifiical ICS after BN into a deep neural network, the added ICS does not affect the good performance of BN, which indicates that the performance of BN has little to do with ICS. Then what does the BN do to improve the training performance? The work mentioned above points out that BN smoothes the optimization landscape and makes the optimizer more easier to find the global minimic solution (c.f. Figure 4 in the paper). For more details, please refer to the paper.\nWeight Normalization (WN) Short description Weight normalization is designed to address the disadvantages of BN which are that BN usually introduces too much noise when the mini-batch is small and the mean and variance of each mini-batch is correlated to inputs. It eliminates the correlations by normalizing weight parameters directly instead of mini-batch inputs.\nMotivation and methodology The core limitaion of BN is that the mean and variance is correlated to each mini-batch inputs, thus a better way to deal with that is design a normalization without the correlation. To achieve this, Salimans et al. proposed a Weight Normalization which is denoted as: $$w = \\frac{g}{||v||} v$$ where $v$ denotes the parameter vector, $||v||$ is the Euclidean norm of $v$, and $g$ is scalar. This reparameterization has the effect of fixing the Euclidean norm of weight vector $w$, and we now have $||w||=g$ which is totally independent from parameter vector $v$. This operation is similar to divide inputs by standard deviation in batch normalization.\nThe mean of neurons still depends on $v$, thus the authors proposed a \u0026lsquo;mean-only batch normalization\u0026rsquo; which only allows the inputs to subtract their mean but not divided by variance. Compared to variance divide, the mean subtraction seems to introduce less noise.\nLayer Normalization Short description Layer normalization is inspired by batch normalization but designed to small mini-batch cases and extend such technique to recurrent neural networks.\nMotivation and methodology As mentioned in short description. To achieve the goal, Hinton et al. 2016 alter the sum statistic of inputs from batch dimension to feature dimension (multiple channels). For example, in a mini-batch (containing multiple input features), the computation can be described as following picture:\n   Fig 1. Layer Normalization Visualization   As can be seen, batch normalization computes the sum statistic of inputs across the batch dimension while layer normalization does across the feature dimension. The computation is almost the same in both normalization cases, but the mean and variance of layer normalization is independent of other examples in the same mini-batch. Experiments show that layer normalization works well in RNNs.\nInstance Normalization (IN) Short description Instance normalization is like layer normalization mentioned above but it goes one step further that it computes mean and variance of each channel in each input feature. In this way, the statistic like mean and variance is independent to each channel. The IN is originally designed for neural style transfer which discovers that stylization network should be agnostic to the contrast of the style image. Thus it is usually specific to image.\nGroup Normalization (GN) Short description Group normalization computes the mean and variance across a group of channels in each training example, which makes it sound like a combination of layer normalization and instance normalization. For example, group normalization becomes layer normalization when all the channels are put into one single group, and becomes instance normalization when each channel is put into one single group. The picture below shows the visual comparisons of batch normalization, layer normalization, instance normalization and group normalization.\n   Fig 2. Normalization Visualization   Motivation Training small batch size with BN introduces noise into network which decreases the accuary. However, for larger models like object detection, segmentation and video, they have to require small batches considering the memory consumption. In addition, dependence bewteen channels widely exists but it is not extremely like all channels have dependences (layer normalization) or totally no dependence between channels (instance normalization). Based on this oberservation, He et al. 2018 proposed a group-wise normalization which divides the channels into groups and makes it flexiable for different applications.\nBatch-Instance Normalization (BIN) Short description Batch-instance normalization is actually a interpolation of BN and IN, and lets the gradient descent to learn a parameter to interploates the weight of BN and IN. The equation below shows the defination of BIN:\n$$BIN(x) = \\gamma (\\rho BN(x) + (1-\\rho) IN(x)) + \\beta$$ To some extend, this BIN inspires readers that models can learn to adaptively use different normalization methods using gradient descent. Would the network be capable of learning to use even wider range of normalization methods in one single model?\nMotivation Rethinking the instance normalization, Nam et al. 2019 regard instance normalization as an effective method to earse unnecessary style information from image and perserve useful styles for tasks like object classification, multi-domain learning and domain adaptation.\nSwitchable Normalization (SN) Luo et al. 2018 investigated into whether different layers in a CNN needs different normalization methods. Thus they proposed a Switchable Normalization which learns parameters to switch normalizers between BN, LN and IN. As for results, their experiments suggest that (1) using distinct normalizations in different layer indeed improves both learning and generation of a CNN;(2) the normalization choices are more related to depth and batch size but less relevant to parameter initialization, learning rate decay and solver;(3) different tasks and datasets influence tha normalization choices. Additionally, the experiments in general also suggest that IN works well in early layers, LN works better in later layers while BN is preferred in middle layers.\nSpectral Normalization Spectral normalization (another form of weight normalization) is designed to improve the training of GANs by tuning the Lipschitz constant of the discriminator. The Lipschitz constant is a constant $L$ used in the following equation: $$||f(x) - f(y)|| \\leqslant L ||x-y||$$\nThe Lipschitz constant is tuned by normalizing the weight matrices where is by their largest eigenvalue. And experiments show that spectral normalization stabilize the training by minimal tuning.\nConclusion BN is a millstone research on training deep neural network which makes it much easier and more robust. However, the limitations like small batch size, noise-sensitie applications and distributed training still need to be fixed in further researches. And different applications/tasks may prefer different normalizations respect to accurancy. New dimensions of normalization still need to be discovered.\nReference:  BN, https://arxiv.org/pdf/1502.03167.pdf WN, https://arxiv.org/pdf/1602.07868.pdf LN, https://arxiv.org/pdf/1607.06450.pdf IN, https://arxiv.org/pdf/1607.08022.pdf GN, https://arxiv.org/pdf/1803.08494.pdf BIN, https://arxiv.org/pdf/1805.07925.pdf SN, https://arxiv.org/pdf/1811.07727v1.pdf https://arxiv.org/pdf/1805.11604.pdf Spectral Normalization, https://arxiv.org/pdf/1802.05957.pdf https://zhuanlan.zhihu.com/p/34879333 https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/  ","date":1577100366,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578868338,"objectID":"55127274bb980c1bfb10f617e7ec7c18","permalink":"https://lwang.github.io/post/normalizaion_in_dl/","publishdate":"2019-12-23T11:26:06Z","relpermalink":"/post/normalizaion_in_dl/","section":"post","summary":"Some basic summary to understand normalizaion in DL","tags":["Academic","Normalization--Neual Network Optimization"],"title":"Normalization_in_DL","type":"post"},{"authors":[],"categories":[],"content":"This is the first test blog post, which aims to see how the post creation goes and how to modify the post properties like font, math, diagram and codes.\nExamples: Code Try code examples:\nimport pandas as pd import torch print(\u0026quot;hello world!\u0026quot;)  Math Try math latex block: $$L_{total} = \\alpha L_c(x, c) + \\beta L_s(x, s) + \\gamma L_{tv}$$\nDiagram Try diagram example:\ngraph TD; A--\u0026gt;B; B--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;E; D--\u0026gt;E;  ","date":1577096110,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577138488,"objectID":"89b8b7c95c9e5efc362d50816cdedfdb","permalink":"https://lwang.github.io/post/test_post1/","publishdate":"2019-12-23T10:15:10Z","relpermalink":"/post/test_post1/","section":"post","summary":"This is the first test blog post, which aims to see how the post creation goes and how to modify the post properties like font, math, diagram and codes.\nExamples: Code Try code examples:\nimport pandas as pd import torch print(\u0026quot;hello world!\u0026quot;)  Math Try math latex block: $$L_{total} = \\alpha L_c(x, c) + \\beta L_s(x, s) + \\gamma L_{tv}$$\nDiagram Try diagram example:\ngraph TD; A--\u0026gt;B; B--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;E; D--\u0026gt;E;  ","tags":[],"title":"Test_post1","type":"post"},{"authors":["Nan Xiang","Li Wang","Tao Jiang","Yanran Li","Xiaosong Yang","Jianjun Zhang"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"12ae3ed59759a4cf57633844292fa027","permalink":"https://lwang.github.io/publication/xiang-2019-single/","publishdate":"2019-12-22T22:19:13.536821Z","relpermalink":"/publication/xiang-2019-single/","section":"publication","summary":"","tags":null,"title":"Single-image Mesh Reconstruction and Pose Estimation via Generative Normal Map","type":"publication"},{"authors":["Li Wang","Nan Xiang","Xiaosong Yang","Jianjun Zhang"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"1c7bdeb1aca5067221f5d12ef0122f65","permalink":"https://lwang.github.io/publication/wang-2018-fast/","publishdate":"2019-12-22T22:19:13.534953Z","relpermalink":"/publication/wang-2018-fast/","section":"publication","summary":"","tags":null,"title":"Fast photographic style transfer based on convolutional neural networks","type":"publication"},{"authors":["Li Wang","Zhao Wang","Xiaosong Yang","Shi-Min Hu","Jianjun Zhang"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"6e4f26e1a5306d394de9b992d9eb244f","permalink":"https://lwang.github.io/publication/wang-2018-photographic/","publishdate":"2019-12-22T22:19:13.533202Z","relpermalink":"/publication/wang-2018-photographic/","section":"publication","summary":"","tags":null,"title":"Photographic style transfer","type":"publication"},{"authors":["Li Wang","Xiang-Jiu Che","Ning Wang","Jie Li","Ming-Hui Zhu"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577053434,"objectID":"a3a4964cce2a06b97559077e24d02480","permalink":"https://lwang.github.io/publication/wang-2014-regulatory/","publishdate":"2019-12-22T22:19:13.537892Z","relpermalink":"/publication/wang-2014-regulatory/","section":"publication","summary":"","tags":null,"title":"Regulatory network analysis of microRNAs and genes in neuroblastoma","type":"publication"}]