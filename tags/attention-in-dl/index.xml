<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>attention in dl | Li Wang</title>
    <link>https://lwang.github.io/tags/attention-in-dl/</link>
      <atom:link href="https://lwang.github.io/tags/attention-in-dl/index.xml" rel="self" type="application/rss+xml" />
    <description>attention in dl</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 28 Dec 2019 22:03:15 +0000</lastBuildDate>
    <image>
      <url>https://lwang.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>attention in dl</title>
      <link>https://lwang.github.io/tags/attention-in-dl/</link>
    </image>
    
    <item>
      <title>Attention</title>
      <link>https://lwang.github.io/post/attention/</link>
      <pubDate>Sat, 28 Dec 2019 22:03:15 +0000</pubDate>
      <guid>https://lwang.github.io/post/attention/</guid>
      <description>&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;
&lt;p&gt;The attention mechanism was created to simulate the human visual attention on images or understanding attention on texts. It was firstly born for solving a problem that widely exists in natural language processing (NLP) models like seq2seq, which NLP models often tend to forget the first part of processed sentences.&lt;/p&gt;
&lt;h3 id=&#34;seq2seq-model&#34;&gt;Seq2seq model&lt;/h3&gt;
&lt;p&gt;The encoder-decoder architecture commonly used in Seq2seq model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;encoder, compress the input sentence into a context vector in a fixed length way which is regarded as a representation of the meaning of input sentence.&lt;/li&gt;
&lt;li&gt;decoder, fed by the context vector and translate the vector to output. In some early works, the last state of encoder is usually used as the initial state of decoder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of the encoder and decoder are recurrent neural networks, using LSTM or GRU units.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The critical problem of seq2seq model.&lt;/strong&gt; The seq2seq model often forgets the first part of a long sentence once it completes translation from the entire sentence to a context vector. To address this problem, the &lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34;&gt;attention mechanism&lt;/a&gt; is proposed.&lt;/p&gt;
&lt;h2 id=&#34;the-attention-mechanism&#34;&gt;The attention mechanism&lt;/h2&gt;
&lt;p&gt;The new architecture for encoder-decoder machine translaion is as following:



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://lwang.github.io/img/attention_encoder-decoder-attention.png&#34; data-caption=&#34;Fig 1. The encoder-decoder architecture in Bahdanau et al. 2015&#34;&gt;
&lt;img data-src=&#34;https://lwang.github.io/img/attention_encoder-decoder-attention.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. The encoder-decoder architecture in Bahdanau et al. 2015
  &lt;/figcaption&gt;


&lt;/figure&gt;

The encoder is composed by a bidirection RNN, a context vector is the sum of weighted hidden states and the decoder translates the context vector to a output target based on previous output targets.&lt;/p&gt;
&lt;h3 id=&#34;fomula&#34;&gt;Fomula&lt;/h3&gt;
&lt;p&gt;Let &lt;strong&gt;x&lt;/strong&gt;=$(x_1, x_2,&amp;hellip;,x_n)$ denote the source sequence of length $n$ and &lt;strong&gt;y&lt;/strong&gt;=$(y_1, y_2,&amp;hellip;,y_m)$ denote the output sequence of length $m$, $\overrightarrow{h_i}$ denotes the forward direction state and $\overleftarrow{h_i}$ presents the backward direction state, then the hidden state for $i$th input word is fomulated as:
$$h_i = [\overrightarrow{h_i}^T; \overleftarrow{h_i}^T], i=1,2,&amp;hellip;,n$$&lt;/p&gt;
&lt;p&gt;The hidden states at position $t$ in decoder includes previous hidden states $s_{t-1}$, previous output target $y_{t-1}$ the context vector $c_t$, which is denoted as $s_{t} = f(s_{t-1}, y_{t-1}, c_{t})$, where the context vector $c_{t}$ is a sum of encoder hidden states of input sequence, weighted by alignment scores. For output target at position $t$, we have:
$$c_{t} = \sum_{i=1}^{n} \alpha_{t,i} h_i$$&lt;/p&gt;
&lt;p&gt;$$ \alpha_{t,i}= align(y_t, x_i) = \frac{exp(score(s_{t-1},h_i))}{\Sigma^n_{i=1} exp(score(s_{t-1},h_{i}))} $$
The score $\alpha_{t,i}$ is assigned to the pair $(y_t, x_i)$ of input at position $i$ and output at position $t$, and the set of weights ${\alpha_{t,i}}$ denotes how much each source hidden state matches for each output. In &lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34;&gt;Bahdanau et al. 2015&lt;/a&gt;, the score $\alpha$ is learnt by a feed-forward network with a single hidden layer and this network is jointly learnt with other part of the model. Since the score is modelled in a network which also has weight matrices (i.e., $v_a$ and $W_a$) and activation layer (i.e., tanh), then the learning function is fomulated as:
$$score(s_t, h_i) = v_a^T tanh(W_a[s_t;h_i])$$&lt;/p&gt;
&lt;h2 id=&#34;self-attention&#34;&gt;Self-attention&lt;/h2&gt;
&lt;p&gt;Self-attention (or intra-attention) is such an attention mechnaism that assigns correlation in a single sequence for an effective representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarizatin or image description generation.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/pdf/1601.06733.pdf&#34;&gt;Cheng et al., 2016&lt;/a&gt;, an application of self-attention mechanism is shown in machine reading. For example, the self-attention mechanism enables the model to learn a correlation between the current word and the previous part of the input sentence.



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://lwang.github.io/img/self_attention.png&#34; data-caption=&#34;Fig 2. An example of self-attention mechanism in Cheng et al., 2016&#34;&gt;
&lt;img data-src=&#34;https://lwang.github.io/img/self_attention.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2. An example of self-attention mechanism in Cheng et al., 2016
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;soft-and-hard-attention&#34;&gt;Soft and Hard Attention&lt;/h2&gt;
&lt;p&gt;In image caption generation, the attention mechanism is applied and shown to be very helpful. &lt;a href=&#34;http://proceedings.mlr.press/v37/xuc15.pdf&#34;&gt;Xu et al.,2015&lt;/a&gt; shows a series of attention visualization to demonstrate how the model learn to summarize the image by paying attention to different regions.



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://lwang.github.io/img/soft_attention.png&#34; data-caption=&#34;Fig 2. The visulation of attention mechanism for image caption generation in Xu et al., 2015&#34;&gt;
&lt;img data-src=&#34;https://lwang.github.io/img/soft_attention.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2. The visulation of attention mechanism for image caption generation in Xu et al., 2015
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The soft attention and hard attention is telled by whether the attention has access to the entire image or only a patch region:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soft&lt;/strong&gt; attention: the alignment weights are assigned to all the patches in the source image, which is the same type used in &lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34;&gt;Bahdanau et al. 2015&lt;/a&gt;
Pro: the model is smooth and differentiable
Con: computationally expensive when the source image is large&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hard&lt;/strong&gt; attention: the alignment weights are only assigned to a patch in the source image at a time
Pro: less computation at the inference time
Con: the model is &lt;strong&gt;non-differentiable&lt;/strong&gt; and requires more complicated techniques such as variance reduction and reinforcement learning to train.(&lt;a href=&#34;https://arxiv.org/pdf/1508.04025.pdf&#34;&gt;Luong et al., 2015&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quora.com/Why-is-hard-attention-non-differentiable-in-NNs&#34;&gt;why hard attention is non-differentiable?&lt;/a&gt; Hard attention is non-differentiable because it’s stochastic. Both hard and soft attention calculate a context vector using a probability distribution (usually over some set of annotation vectors), but soft attention works by taking the expected context vector at that time (i.e. a weighted sum of the annotation vectors using the probability distribution as weights), which is a differentiable action. Hard attention, on the other hand, stochastically chooses a context vector; when the distribution is multinomial over a set of annotation vectors (similar to how soft attention works, but we aren’t calculating the expected context vector now), you just sample from the annotation vectors using the given distribution. The advantage of hard attention is that you can also attend to context spaces that aren’t multinomial (e.g. a Gaussian-distributed context space), which is very helpful when the context space is continuous rather than discrete (soft attention can only work over discrete spaces).&lt;/p&gt;
&lt;p&gt;And since hard attention is non-differentiable, models using this method have to be trained using reinforcement learning.&lt;/p&gt;
&lt;h2 id=&#34;global-and-local-attention&#34;&gt;Global and Local Attention&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.04025.pdf&#34;&gt;Luong et al. 2015&lt;/a&gt; proposed a global and local attention, where the global attention calculate the entire weighted sum of hidden states for a target output (which is similar to soft attention). While the local attention is more smiliar to the blend of soft and hard attention. For example, in their paper, the model first predict a aligned position for the current target word then a window centred around the source position is used to compute a context vector.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://lwang.github.io/img/global_local_attention.png&#34; data-caption=&#34;Fig 1. The global and local attention architecture in Luong et al., 2015&#34;&gt;
&lt;img data-src=&#34;https://lwang.github.io/img/global_local_attention.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1. The global and local attention architecture in Luong et al., 2015
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;This attention blog heavily borrowed from Lilian Weng&#39;s blog, more details refer to &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&#34;&gt;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
